{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Falaremos sobre Cont\u00eaineres?","text":"<p>Para iniciar essa conversa acho importante ter uma vis\u00e3o geral sobre o conte\u00fado \u00e9 importante que voc\u00ea assista o video do C\u00f3digo Fonte TV que explica sobre cont\u00eaineres.</p> <p>E tamb\u00e9m esse Containers, Docker e Kubernetes com Giovanni Bassi que fala um pouco sobre essas mudan\u00e7as que os cont\u00eaineres fizeram.</p>"},{"location":"#como-conteineres-funcionam","title":"Como cont\u00eaineres funcionam?","text":"<p>Temos agora uma base te\u00f3rica para trabalhar, refor\u00e7aremos o conceito de M\u00e1quina Virtual e Cont\u00eaineres:</p> <ul> <li>Cont\u00eaineres s\u00e3o medidos em megabyte. Eles cont\u00eam, no m\u00e1ximo, a aplica\u00e7\u00e3o e os arquivos necess\u00e1rios para execut\u00e1-la. Al\u00e9m disso, eles costumam ser usados para empacotar fun\u00e7\u00f5es individuais que realizam tarefas espec\u00edficas, os famosos microsservi\u00e7os. Como s\u00e3o leves e t\u00eam um sistema operacional compartilhado, os cont\u00eaineres s\u00e3o muito f\u00e1ceis de migrar entre v\u00e1rios ambientes.</li> <li>As m\u00e1quinas virtuais(VM) s\u00e3o medidas em gigabyte. Eles costumam ter seu pr\u00f3prio sistema operacional, possibilitando a execu\u00e7\u00e3o simult\u00e2nea de v\u00e1rias fun\u00e7\u00f5es com uso intenso de recursos. Por terem um n\u00famero maior de recursos \u00e0 disposi\u00e7\u00e3o, as m\u00e1quinas virtuais conseguem abstrair, dividir, duplicar e emular por inteiro servidores, sistemas operacionais, desktops, bancos de dados e redes.</li> </ul>  <p>Temos essa imagem para ilustra melhor esse conceito: </p> <ul> <li>Virtualiza\u00e7\u00e3o: O hipervisor \u00e9 um software que separa os recursos das respectivas m\u00e1quinas f\u00edsicas para eles poderem ser particionados e dedicados \u00e0s m\u00e1quinas virtuais. Quando o usu\u00e1rio emite uma instru\u00e7\u00e3o de m\u00e1quina virtual que exige mais recursos do ambiente f\u00edsico, o hipervisor retransmite a solicita\u00e7\u00e3o ao sistema f\u00edsico e armazena as mudan\u00e7as em mem\u00f3ria transit\u00f3ria. As m\u00e1quinas virtuais s\u00e3o similares aos servidores f\u00edsicos e agem como eles, o que pode multiplicar as desvantagens de grandes infraestruturas de sistema operacional e das depend\u00eancias da aplica\u00e7\u00e3o. Na maioria das vezes, essas infraestruturas n\u00e3o s\u00e3o necess\u00e1rias para executar uma aplica\u00e7\u00e3o ou microsservi\u00e7os.</li> <li>Cont\u00eaineres: Os cont\u00eaineres armazenam um microsservi\u00e7o ou aplica\u00e7\u00e3o, al\u00e9m de todos os elementos necess\u00e1rios para execut\u00e1-los. Tudo que eles cont\u00eam \u00e9 mantido em um recurso chamado imagem: um arquivo baseado em c\u00f3digo que inclui todas as bibliotecas e depend\u00eancias. Pense nesses arquivos como uma instala\u00e7\u00e3o da distribui\u00e7\u00e3o Linux, j\u00e1 que a imagem inclui pacotes RPM e arquivos de configura\u00e7\u00e3o. Como os cont\u00eaineres s\u00e3o muito pequenos, geralmente h\u00e1 centenas deles levemente acoplados.</li> </ul>"},{"location":"#beneficios-de-usar-conteineres","title":"Beneficios de usar Cont\u00eaineres","text":"<p>Cont\u00eaineres se tornaram populares porque eles fornecem benef\u00edcios extra, tais como:</p> <ul> <li>Cria\u00e7\u00e3o e implanta\u00e7\u00e3o \u00e1gil de aplica\u00e7\u00f5es: aumento da facilidade e efici\u00eancia na cria\u00e7\u00e3o de imagem de cont\u00eainer comparado ao uso de imagem de VM.</li> <li>Desenvolvimento, integra\u00e7\u00e3o e implanta\u00e7\u00e3o cont\u00ednuos: fornece capacidade de cria\u00e7\u00e3o e de implanta\u00e7\u00e3o de imagens de cont\u00eainer de forma confi\u00e1vel e frequente, com a funcionalidade de efetuar revers\u00f5es r\u00e1pidas e eficientes (devido \u00e0 imutabilidade da imagem).</li> <li>Separa\u00e7\u00e3o de interesses entre Desenvolvimento e Opera\u00e7\u00f5es: crie imagens de cont\u00eaineres de aplica\u00e7\u00f5es no momento de constru\u00e7\u00e3o/libera\u00e7\u00e3o em vez de no momento de implanta\u00e7\u00e3o, desacoplando as aplica\u00e7\u00f5es da infraestrutura.</li> <li>A capacidade de observa\u00e7\u00e3o (Observabilidade) n\u00e3o apenas apresenta informa\u00e7\u00f5es e m\u00e9tricas no n\u00edvel do sistema operacional, mas tamb\u00e9m a integridade da aplica\u00e7\u00e3o e outros sinais.</li> <li>Consist\u00eancia ambiental entre desenvolvimento, teste e produ\u00e7\u00e3o: funciona da mesma forma em um laptop e na nuvem.</li> <li>Portabilidade de distribui\u00e7\u00e3o de nuvem e sistema operacional: executa no Ubuntu, RHEL, CoreOS, localmente, nas principais nuvens p\u00fablicas e em qualquer outro lugar.</li> <li>Gerenciamento centrado em aplica\u00e7\u00f5es: eleva o n\u00edvel de abstra\u00e7\u00e3o da execu\u00e7\u00e3o em um sistema operacional em hardware virtualizado \u00e0 execu\u00e7\u00e3o de uma aplica\u00e7\u00e3o em um sistema operacional usando recursos l\u00f3gicos.</li> <li>Microservi\u00e7os fracamente acoplados, distribu\u00eddos, el\u00e1sticos e livres: as aplica\u00e7\u00f5es s\u00e3o divididas em partes menores e independentes e podem ser implantados e gerenciados dinamicamente - n\u00e3o uma pilha monol\u00edtica em execu\u00e7\u00e3o em uma grande m\u00e1quina de prop\u00f3sito \u00fanico.</li> <li>Isolamento de recursos: desempenho previs\u00edvel de aplica\u00e7\u00f5es.</li> <li>Utiliza\u00e7\u00e3o de recursos: alta efici\u00eancia e densidade</li> </ul>  <p>J\u00e1 que sabemos a diferen\u00e7a entre VM e Cont\u00eaineres, precisamos entender aonde essa solu\u00e7\u00e3o se encaixa, pois, com as mudan\u00e7as que a tecnologia traz no decorrer da sua hist\u00f3ria, \u00e9 preciso entender as novas pr\u00e1ticas de TI com as tradicionais para chegamos a um ponto-chave dessa pergunta:</p> <ul> <li>As novas pr\u00e1ticas de TI (desenvolvimento nativo em nuvem, CI/CD e DevOps) existem gra\u00e7as \u00e0 divis\u00e3o das cargas de trabalho nas menores unidades \u00fateis poss\u00edveis, que geralmente s\u00e3o uma fun\u00e7\u00e3o ou um microsservi\u00e7o. Essas unidades s\u00e3o melhor empacotadas em cont\u00eaineres. Assim, v\u00e1rias equipes podem trabalhar em partes separadas de uma aplica\u00e7\u00e3o ou servi\u00e7o sem interromper, ou p\u00f4r em risco o c\u00f3digo empacotado em outros cont\u00eaineres.</li> <li>Nas arquiteturas de TI tradicionais (monol\u00edticas e legadas), todos os elementos de uma carga de trabalho s\u00e3o mantidos em um arquivo grande que n\u00e3o pode ser dividido. Por isso, ele precisa ser empacotado como uma unidade completa em um ambiente maior, frequentemente uma m\u00e1quina virtual. Era comum criar e executar uma aplica\u00e7\u00e3o inteira dentro de uma m\u00e1quina virtual, mesmo sabendo que ao armazenar todo o c\u00f3digo e depend\u00eancias, ela ficava grande demais, e isso poderia gerar falhas em cascata e downtime durante as atualiza\u00e7\u00f5es.</li> </ul>"},{"location":"#se-aprofundando-um-pouco-mais-nos-conteineres","title":"Se aprofundando um pouco mais nos Cont\u00eaineres","text":"<p>Se em algum momento voc\u00ea j\u00e1 usou uma VM, imagino que tenha passado pelo processo de baixar uma SO, seja Linux ou Windows e alocando os recursos conforme a capacidade do hardware dispon\u00edvel, ent\u00e3o esse processo dos Cont\u00eaineres pode parecer meio confuso, para isso deixei duas perguntas que precisamos responder antes de continuar.</p>"},{"location":"#se-nao-e-uma-virtualizacao-como-ocorre-o-isolamento","title":"Se n\u00e3o \u00e9 uma virtualiza\u00e7\u00e3o como ocorre o isolamento?","text":"<p>Existe uma palavra importante, que resolve essa quest\u00e3o Namespaces, permitindo que os cont\u00eaineres consigam isolamento em determinados n\u00edveis que s\u00e3o:</p> <ul> <li>PID: Isola os processos rodando dentro do cont\u00eainer</li> <li>NET: Isola as interfaces de redes</li> <li>IPC: Isola a comunica\u00e7\u00e3o entre processos e memoria</li> <li>MNT: Isola o ponto de montagem</li> <li>UTS: Isola o Kernel, ou seja, simula um novo host(usu\u00e1rio)</li> <li>USER: Isola os arquivos do sistema</li> </ul> <p>Dentre esse processo o mais importante, \u00e9 o UTS, o respons\u00e1vel pelo isolamento dos cont\u00eaineres, ao simular um novo host, ele usar o Kernel da m\u00e1quina f\u00edsica para o processamento, por\u00e9m isoladamente.</p> <p>Os namespaces fazem parte do kernel do Linux desde 2002, os namespaces s\u00e3o respons\u00e1veis por gerar o isolamento de grupos de processos em seu n\u00edvel l\u00f3gico.</p>"},{"location":"#como-ocorre-a-divisao-de-recursos","title":"Como ocorre a divis\u00e3o de recursos?","text":"<p>O gerenciamento de recursos, tem a palavra Cgroups para resolver essa demanda de gerenciar, seja ela automaticamente ou manualmente. Os cgroups fornecem os recursos:</p> <ul> <li>Limites de recursos: voc\u00ea pode configurar um cgroup para limitar quanto de um determinado recurso (mem\u00f3ria ou CPU, por exemplo) um processo pode usar.</li> <li>Prioriza\u00e7\u00e3o: Voc\u00ea pode controlar quanto de um recurso (CPU, disco ou rede) um processo pode usar em compara\u00e7\u00e3o com processos em outro cgroup quando h\u00e1 conten\u00e7\u00e3o de recursos.</li> <li>Contabilidade: os limites de recursos s\u00e3o monitorados e relatados no n\u00edvel do cgroup.</li> <li>Controle: Voc\u00ea pode alterar o status (congelado, interrompido ou reiniciado) de todos os processos em um cgroup com um \u00fanico comando.</li> </ul> <p>Os cgroups s\u00e3o um componente-chave dos cont\u00eaineres porque geralmente h\u00e1 v\u00e1rios processos em execu\u00e7\u00e3o em um cont\u00eainer que precisam ser controlados juntos.</p>  <p>Agora que entendemos o significado de cont\u00eaineres, temos mais duas palavras importantes para aprender:</p> <ul> <li>Kubernetes</li> <li>Swarm</li> </ul> <p>Essas duas palavras possuem uma frase em comum:</p>  <p>orquestra\u00e7\u00e3o de containers</p>"},{"location":"#para-que-serve-a-orquestracao-de-conteineres","title":"Para que serve a orquestra\u00e7\u00e3o de cont\u00eaineres?","text":"<p>Basicamente, a orquestra\u00e7\u00e3o de cont\u00eaineres automatiza a implanta\u00e7\u00e3o, o gerenciamento, a escala e a rede dos cont\u00eaineres. Use a orquestra\u00e7\u00e3o de cont\u00eaineres para automatizar e gerenciar tarefas como:</p>  <ul> <li>Provisionamento e implanta\u00e7\u00e3o</li> <li>Configura\u00e7\u00e3o e programa\u00e7\u00e3o</li> <li>Aloca\u00e7\u00e3o de recursos</li> <li>Disponibilidade dos containers</li> <li>Escala ou remo\u00e7\u00e3o de containers com base no balanceamento de cargas de trabalho na infraestrutura</li> <li>Balanceamento de carga e roteamento de tr\u00e1fego</li> <li>Monitoramento da integridade do container</li> <li>Configura\u00e7\u00e3o da aplica\u00e7\u00e3o com base no container em que ela ser\u00e1 executada</li> <li>Prote\u00e7\u00e3o das intera\u00e7\u00f5es entre os containers</li> </ul>  <p>As ferramentas de orquestra\u00e7\u00e3o de cont\u00eaineres fornecem um framework para gerenciar arquiteturas de microsservi\u00e7os e cont\u00eaineres em escala. Muitas delas podem ser usadas no gerenciamento do ciclo de vida dos cont\u00eaineres. Algumas op\u00e7\u00f5es s\u00e3o o Kubernetes, Docker Swarm e Apache Mesos, DC/SO e N\u00f4made.</p>  <p>Nesse tema vamos aborda sobre e praticar:</p> <ul> <li> Docker 100%<ul> <li> Docker Swarm: Cluster do Docker 100%</li> <li> Usar o Vagrant para cria\u00e7\u00e3o da infraestrutura do Swarm 0%</li> </ul> </li> <li> Kubernetes 80%<ul> <li> Persistir Dados</li> <li> Liveness, Readiness e Probes</li> <li> Escalar a aplica\u00e7\u00e3o</li> </ul> </li> <li> Amazon Kubernetes 0%</li> <li> Azure Kubernetes 0%</li> <li> GCP Kubernetes 0%<ul> <li> API do Registry</li> <li> Cluster AutoPilot e Standard</li> <li> Log e Monitoramento</li> </ul> </li> </ul>"},{"location":"#fontes-usadas-para-elaboracao-desse-conteudo","title":"Fontes usadas para elabora\u00e7\u00e3o desse conte\u00fado","text":"<ul> <li>Red Hat: Cont\u00eaineres vs Vms</li> <li>Red Hat: Orquestra\u00e7\u00e3o de containers</li> <li>etcd: O que s\u00e3o namespaces e cgroups</li> </ul>"},{"location":"GCP%20-%20Kubernetes/usando_gcp/","title":"Preste aten\u00e7\u00e3o","text":"<p>Conhecimento minimo que vai lhe ajudar a entender o processo, mas, BUT, n\u00e3o significa que voc\u00ea n\u00e3o possa tentar.</p> <ul> <li>Docker, conhecimento sobre container e imagens.</li> <li>Kubernetes, entender o funcionamento dos arquivos.YAML nessa atividade t\u00e3o satisfatoria</li> <li>Conta no Google - Se \u00e9 a primeira vez que vai acessar o google cloud console verificar se est\u00e1 no periodo de gratuidade, se n\u00e3o, basta criar uma nova e seguir adiante.</li> </ul> <p>Com isso tudo em m\u00e3os, vamos iniciar.</p>"},{"location":"GCP%20-%20Kubernetes/usando_gcp/#criando-o-projeto-no-gcp","title":"Criando o projeto no GCP","text":"<p>Para quem j\u00e1 se aventurou nessas bandas, basta criar um projeto novo para iniciar, para quem n\u00e3o vou deixar um passo a passo simples:</p> <ol> <li>Com sua conta no free tier, super importante para n\u00e3o temos custo na nossa fatura de cart\u00e3o de credito</li> <li>Vai no google cloud console e verificar se est\u00e1 com o free tier</li> <li>Acesse com sua conta, COM FUCK FREE TIER, e crie um projeto novo.</li> </ol>  <p>Certo?</p> <p>Projeto fresquinho e free tier, t\u00e1 na hora da maldade.</p>"},{"location":"GCP%20-%20Kubernetes/usando_gcp/#api","title":"API","text":"<p>Vamos acessar a API necessaria para continuamos com o nosso projeto, para isso ai na barra superior AZUL, BEM NO MEIO TEM UMA LUGAR PARA PESQUISA, e digite ou copie:  apis &amp; services e pronto.</p> <p>Com isso, basta</p>  <p>EEMMMMMMMMMMMM</p> <p>Enable APIs and Services vai est\u00e1 um pouco a baixo da barra azul a esquerda.</p> <p>Voc\u00ea vai ser redirecionado para uma nova pagina bem fofinha, pesquise ISSO PESQUISE AI NA PAGINA FOFINHA o Google Container Registry API ele vai ter retorna o resultado e selecione o Google Container Registry API e voc\u00ea vai chegar numa pagina dessa AQUI:</p>  <p>Vai em Enable com o ponteiro do mouse e no mouse aperta o bot\u00e3o do lado esquerdo que fica geralmente no dedo indicador, se voc\u1ebd for destro.</p>  <p>Depois de alguns anos para habilitar, vai ser redirecionado l\u00e1 para aquela tela de apis &amp; services.</p>"},{"location":"GCP%20-%20Kubernetes/usando_gcp/#e-agora-o-que-eu-faco-emmmmmm","title":"E AGORA O QUE EU FA\u00c7O EMMMMMM","text":"<p>Para ser uma pessoa democratica e levar em considera\u00e7\u00e3o os diferentes OS que podem passar por esse guia simpatico, vamos manter tudo no shell do GCP, BUUUUUUUUT, N\u00c3O RECOMENDO QUE NO DIA A DIA QUE O SHELL DO GCP SEJA USADO. Naturalmente, SSH \u00e9 a palavra de 3 letras mais importante para quem gosta de fu\u00e7a a nuvem. Dito isso, vamos retorna.</p> <p>V\u00e1 l\u00e1 na BARRA AZUL no canto direito vai ter alguns ICONES, SIM ICONES, e dentre eles, vai ter ESSE AQUI OHHH:</p>  <p>T\u00e1 vendo?</p> <p>Bem, com isso vai ser aberto um terminal ai mesmo na tela na parte inferior da tela para ser mais preciso. Ele vai carregar um pouquinho e pronto, tem uma telinha no canto inferior da sua tela, por\u00e9m para melhorar nossa experiencia com essa ferramenta CLASSUDA do Google, vamos aperta em Open new window \u00e9 um icone bem ao lado do X desse terminal embutido ai.</p> <p>Feito isso, ele vai abrir em uma nova aba ai do navegador que voc\u1ebd estiver usando, agora com uma tela de respeito para trabalhar com um pouco mais de liberdade de pixel, precisamos de um projetinho para subir.</p>"},{"location":"GCP%20-%20Kubernetes/usando_gcp/#projetinho","title":"PROJETINHO","text":"<p>AHHHH QUE DELICIA CARA, um projetinho fresquinho para quem gosta de um projetinho, temos aqui um projetinho feito por pessoas que gostam de um projetinho.</p> <p>Kubedoom</p> <p>Perfeito, l\u00e1 no projetinho o autor deixou instru\u00e7\u00f5es para rodar localmente, mas nosso foco \u00e9 fazer a parada rodar na NUVEM.</p> <p>L\u00e1 na sua PAGINA maravilhosa do Shell da Google, basta digitar:</p> <pre><code>git clone https://github.com/storax/kubedoom.git\n</code></pre> <p>E magicamente, ele vai baixar para esse local na NUVEM em seguida de um ls para verificar as pastas ali e TARAMMMM magicamente, HEHEHE, o nosso arquivo est\u00e1 ai no SHELL.</p> <p>Agora, fa\u00e7a o SEGUINTE, precisamos entrar nas pasta pelo SHELL da NUVEM, para isso basta usa o cd e o nome da pasta que no NOSSO caso \u00e9 kubedoom:</p> <pre><code>cd kubedoom\n</code></pre> <p>Agora, quero d\u00e1 uma editada no arquivo ai MESMO, ISSO AI, NESSE MESMO LOCAL DA NUVEM, como fa\u00e7o isso?</p> <p>Primeiro, veja a tela, isso a TELA, l\u00e1 no canto superior direito, vai ter uma fotinha sua, ou n\u00e3o e olhando atentamente para esquerda vai ter alguns icones, ICONES, e um deles \u00e9 uma CANETA meio deitada, que na verdade \u00e9 um folk mal feito do VScode.</p>  <p>RESUMO super elaborado para lhe ajudar a compreender de forma eficiente e interativa, imaginativa, perspectativa e maravilhosativa. Agora basta ir na op\u00e7\u00e3o file &gt; open &gt; seleciona nossa pasta kubedoom para ele abrir o editor na pasta.</p> <p>Certo?</p> <p>Com nosso projetinho, disponivel para os proximos passos, iremos consumar o usar o docker para um teste simples com um comando simples, LEMBRANDO QUE VOC\u00ca PRECISA EST\u00c1 DENTRO DA PASTA, e com isso basta digitar no seu terminal:</p>"},{"location":"Kubernetes/deployment/","title":"Deployment","text":"<p>Voc\u00ea descreve um estado desejado em um Deployment e controla e altera o estado real para o estado desejado a uma taxa controlada. Voc\u00ea pode definir Deployment para criar novos ReplicaSets ou remover Deployment existentes e adotar todos os seus recursos com novas Deployment.</p>  <p>Bem, para simplificar iremos criar um .yaml e passar no kind o Deployment, que vai criar um versionamento para a aplica\u00e7\u00e3o, por\u00e9m ele engloba o ReplicaSets, passando a quantidade de r\u00e9plicas.</p>  <p>Para simplificar ainda mais \u00e9 praticamente um ReplicaSet com versionamento.</p> <p>Crie o arquivo .yaml e vamos executar:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre> <p>Execute em seguida:</p> <pre><code>kubectl apply -f deployment.yaml\n</code></pre> <p>Para verificar:</p> <pre><code>kubectl get deployment\n</code></pre>"},{"location":"Kubernetes/deployment/#versionamento","title":"Versionamento","text":"<p>Com o versionamento podemos, criar vers\u00f5es e depois podemos retornar se houve necessidade</p> ExecuteSaida   <pre><code>kubectl rollout history deployment\n</code></pre> <p>Se houver mais deployment basta passa o nome no final para especificar.</p>   <pre><code>REVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n</code></pre> <p>Aqui no nosso hist\u00f3rico est\u00e1 em none na vers\u00e3o 1</p>    <p>Vou alterar a vers\u00e3o do nginx:1.14.2 para nginx:1 com o set.</p> <pre><code>kubectl set image deployment/nginx-deployment nginx=nginx:1\n</code></pre> <p>Em seguida repetimos o processo para verificar a revis\u00e3o.</p> ExecuteSaida   <pre><code>kubectl rollout history deployment\n</code></pre> <p>Se houver mais deployment basta passa o nome no final para especificar.</p>   <pre><code>REVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         &lt;none&gt;\n</code></pre> <p>Aqui no nosso historico agora com a vers\u00e3o 2</p>    <p>Que tal mudamos o nome das altera\u00e7\u00f5es que fizemos? Usando o annotate</p> <pre><code>kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"altera\u00e7\u00e3o da imagem para a vers\u00e3o 1\"\n</code></pre> <p>E a saida do kubectl rollout history deployment agora est\u00e1 com a altera\u00e7\u00e3o:</p> <pre><code>REVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         altera\u00e7\u00e3o da imagem para a vers\u00e3o 1\n</code></pre> <p>Lembrando \u00e9 importante voc\u00ea anotar sempre que mudar de vers\u00e3o.</p> <p>E para retorna para alguma vers\u00e3o espec\u00edfica:</p> <pre><code>kubectl rollout undo deployment/nginx-deployment --to-revision=1\n</code></pre>"},{"location":"Kubernetes/deployment/#escalando-a-quantidade-de-replicas","title":"Escalando a quantidade de r\u00e9plicas","text":"<p>Para escalar \u00e9 bem simples:</p> <pre><code>kubectl scale deployment/nginx-deployment --replicas=10\n</code></pre> <p>Usando o scale e com a flag --replicas definimos a quantidade de r\u00e9plicas.</p> <p>Leia a Documenta\u00e7\u00e3o para ter mais informa\u00e7\u00f5es.</p>"},{"location":"Kubernetes/instala%C3%A7%C3%A3o/","title":"Instala\u00e7\u00e3o","text":"<p>Para instalar o kubernete:</p> WindowsLinuxMac   <p>Siga o passo a passo da documenta\u00e7\u00e3o</p>   <p>Siga o passo a passo da documenta\u00e7\u00e3o</p>   <p>Siga o passo a passo da documenta\u00e7\u00e3o</p>     <p>Precisamos instalar uma ferramenta para ajudar a criar essa infraestrutura localmente, temos algumas op\u00e7\u00f5es que a documenta\u00e7\u00e3o sugere que s\u00e3o o kind, minikube e kubeadm, vamos instalar somente o minikube para realizar nossos estudos.</p> <p>Na documenta\u00e7\u00e3o basta verificar o passo a passo para sua OS e fazer a instala\u00e7\u00e3o.</p> <p>Para iniciar o minikube:</p> <pre><code>minikube start\n</code></pre> <p>Ele vai dizer que est\u00e1 faltando um driver, se voc\u00ea j\u00e1 tiver o docker instalado facilita o processo, mas se n\u00e3o basta instalar uns dos drives que ele recomenda para sua OS.</p> <p>Vou usar o docker e setar ele para usar como default:</p> <pre><code>minikube config set driver docker\n</code></pre> <p>Ele solicitou para d\u00e1 um start no docker daemon:</p> <pre><code>sudo systemctl start docker\n</code></pre> <p>Como isso basta, iniciar novamente o minikube, vamos testar o kubectl get nodes:</p> <pre><code>NAME       STATUS   ROLES                  AGE     VERSION\nminikube   Ready    control-plane,master   3m54s   v1.23.3\n</code></pre> <p>Lembrando que, futuramente vou criar cluster em servi\u00e7os de nuvem, como o AWS, GCP, Azure e talvez na Oracle. Mas nesse momento, vamos focar em kubernetes e como funciona toda essa estrutura, pratica e teoricamente.</p>"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/","title":"O que \u00e9 Kubernetes?","text":"<p>Para acessar o repositorio Github Kubernetes e o Kubernetes.io</p> <p>O Google tornou Kubernetes um projeto de c\u00f3digo-aberto em 2014. O Kubernetes combina mais de 15 anos de experi\u00eancia do Google executando cargas de trabalho produtivas em escala, com as melhores id\u00e9ias e pr\u00e1ticas da comunidade.</p> <p>Kubernetes \u00e9 um plataforma de c\u00f3digo aberto, port\u00e1vel e extensiva para o gerenciamento de cargas de trabalho e servi\u00e7os distribu\u00eddos em cont\u00eaineres, que facilita tanto a configura\u00e7\u00e3o declarativa quanto a automa\u00e7\u00e3o. Ele possui um ecossistema grande, e de r\u00e1pido crescimento. Servi\u00e7os, suporte, e ferramentas para Kubernetes est\u00e3o amplamente dispon\u00edveis.</p>  <p>O Kubernetes veio com grande experi\u00eancia da Google e n\u00e3o para de crescer nas funcionalidades e usu\u00e1rios. \u00c9 ele quem gerencia os containers em execu\u00e7\u00e3o e por isso ele tamb\u00e9m \u00e9 chamado de Orquestrador de Containers. Atrav\u00e9s dele podemos definir o estado de um sistema completo, por exemplo baseado em Microservices, seguindo boas pr\u00e1ticas de infraestrutura como c\u00f3digo, permitindo balanceamento de carga, alta disponibilidade, atualiza\u00e7\u00f5es em lote e rollbacks e muito muito mais.</p> <p>Hoje em dia os grandes provedores de nuvem como Azure, AWS, IBM, Red Hat, Oracle ou Google d\u00e3o suporte ao Kubernetes. Al\u00e9m disso, existe uma implementa\u00e7\u00e3o local chamada de Minikube que simula um cluster Kubernetes, ideal para testes e estudos. O mais legal \u00e9 que as configura\u00e7\u00f5es locais, que definem o estado da aplica\u00e7\u00e3o, tamb\u00e9m rodam no Kubernetes na nuvem. Ou seja, podemos testar o orquestrador local usando Minikube e depois publicar o sistema no AWS ou Azure, apenas com poucas altera\u00e7\u00f5es.</p>"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#por-que-voce-precisa-do-kubernetes-e-o-que-ele-pode-fazer","title":"Por que voc\u00ea precisa do Kubernetes e o que ele pode fazer","text":"<p>Os cont\u00eaineres s\u00e3o uma boa maneira de agrupar e executar suas aplica\u00e7\u00f5es. Em um ambiente de produ\u00e7\u00e3o, voc\u00ea precisa gerenciar os cont\u00eaineres que executam as aplica\u00e7\u00f5es e garantir que n\u00e3o haja tempo de inatividade. Por exemplo, se um cont\u00eainer cair, outro cont\u00eainer precisa ser iniciado. N\u00e3o seria mais f\u00e1cil se esse comportamento fosse controlado por um sistema?</p> <p>\u00c9 assim que o Kubernetes vem ao resgate! O Kubernetes oferece uma estrutura para executar sistemas distribu\u00eddos de forma resiliente. Ele cuida do escalonamento e do recupera\u00e7\u00e3o \u00e0 falha de sua aplica\u00e7\u00e3o, fornece padr\u00f5es de implanta\u00e7\u00e3o e muito mais. Por exemplo, o Kubernetes pode gerenciar facilmente uma implanta\u00e7\u00e3o no m\u00e9todo can\u00e1rio para seu sistema.</p> <p>O Kubernetes oferece a voc\u00ea:</p> <ul> <li>Descoberta de servi\u00e7o e balanceamento de carga O Kubernetes pode expor um cont\u00eainer usando o nome DNS ou seu pr\u00f3prio endere\u00e7o IP. Se o tr\u00e1fego para um cont\u00eainer for alto, o Kubernetes pode balancear a carga e distribuir o tr\u00e1fego de rede para que a implanta\u00e7\u00e3o seja est\u00e1vel.</li> <li>Orquestra\u00e7\u00e3o de armazenamento O Kubernetes permite que voc\u00ea monte automaticamente um sistema de armazenamento de sua escolha, como armazenamentos locais, provedores de nuvem p\u00fablica e muito mais.</li> <li>Lan\u00e7amentos e revers\u00f5es automatizadas Voc\u00ea pode descrever o estado desejado para seus cont\u00eaineres implantados usando o Kubernetes, e ele pode alterar o estado real para o estado desejado em um ritmo controlada. Por exemplo, voc\u00ea pode automatizar o Kubernetes para criar novos cont\u00eaineres para sua implanta\u00e7\u00e3o, remover os cont\u00eaineres existentes e adotar todos os seus recursos para o novo cont\u00eainer.</li> <li>Empacotamento bin\u00e1rio autom\u00e1tico Voc\u00ea fornece ao Kubernetes um cluster de n\u00f3s que pode ser usado para executar tarefas nos cont\u00eaineres. Voc\u00ea informa ao Kubernetes de quanta CPU e mem\u00f3ria (RAM) cada cont\u00eainer precisa. O Kubernetes pode encaixar cont\u00eaineres em seus n\u00f3s para fazer o melhor uso de seus recursos.</li> <li>Autocorre\u00e7\u00e3o O Kubernetes reinicia os cont\u00eaineres que falham, substitui os cont\u00eaineres, elimina os cont\u00eaineres que n\u00e3o respondem \u00e0 verifica\u00e7\u00e3o de integridade definida pelo usu\u00e1rio e n\u00e3o os anuncia aos clientes at\u00e9 que estejam prontos para servir.</li> <li>Gerenciamento de configura\u00e7\u00e3o e de segredos O Kubernetes permite armazenar e gerenciar informa\u00e7\u00f5es confidenciais, como senhas, tokens OAuth e chaves SSH. Voc\u00ea pode implantar e atualizar segredos e configura\u00e7\u00e3o de aplica\u00e7\u00f5es sem reconstruir suas imagens de cont\u00eainer e sem expor segredos em sua pilha de configura\u00e7\u00e3o.</li> </ul>"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#o-que-o-kubernetes-nao-e","title":"O que o Kubernetes n\u00e3o \u00e9","text":"<p>O Kubernetes n\u00e3o \u00e9 um sistema PaaS (plataforma como servi\u00e7o) tradicional e completo. Como o Kubernetes opera no n\u00edvel do cont\u00eainer, e n\u00e3o no n\u00edvel do hardware, ele fornece alguns recursos geralmente aplic\u00e1veis comuns \u00e0s ofertas de PaaS, como implanta\u00e7\u00e3o, escalonamento, balanceamento de carga, e permite que os usu\u00e1rios integrem suas solu\u00e7\u00f5es de logging, monitoramento e alerta. No entanto, o Kubernetes n\u00e3o \u00e9 monol\u00edtico, e essas solu\u00e7\u00f5es padr\u00e3o s\u00e3o opcionais e conect\u00e1veis. O Kubernetes fornece os blocos de constru\u00e7\u00e3o para a constru\u00e7\u00e3o de plataformas de desenvolvimento, mas preserva a escolha e flexibilidade do usu\u00e1rio onde \u00e9 importante.</p> <p>Kubernetes:</p> <ul> <li>N\u00e3o limita os tipos de aplica\u00e7\u00f5es suportadas. O Kubernetes visa oferecer suporte a uma variedade extremamente diversa de cargas de trabalho, incluindo cargas de trabalho sem estado, com estado e de processamento de dados. Se uma aplica\u00e7\u00e3o puder ser executada em um cont\u00eainer, ele deve ser executado perfeitamente no Kubernetes.</li> <li>N\u00e3o implanta c\u00f3digo-fonte e n\u00e3o constr\u00f3i sua aplica\u00e7\u00e3o. Os fluxos de trabalho de integra\u00e7\u00e3o cont\u00ednua, entrega e implanta\u00e7\u00e3o (CI/CD) s\u00e3o determinados pelas culturas e prefer\u00eancias da organiza\u00e7\u00e3o, bem como pelos requisitos t\u00e9cnicos.</li> <li>N\u00e3o fornece servi\u00e7os em n\u00edvel de aplica\u00e7\u00e3o, tais como middleware (por exemplo, barramentos de mensagem), estruturas de processamento de dados (por exemplo, Spark), bancos de dados (por exemplo, MySQL), caches, nem sistemas de armazenamento em cluster (por exemplo, Ceph), como servi\u00e7os integrados. Esses componentes podem ser executados no Kubernetes e/ou podem ser acessados por aplica\u00e7\u00f5es executadas no Kubernetes por meio de mecanismos port\u00e1teis, como o Open Service Broker.</li> <li>N\u00e3o dita solu\u00e7\u00f5es de logging, monitoramento ou alerta. Ele fornece algumas integra\u00e7\u00f5es como prova de conceito e mecanismos para coletar e exportar m\u00e9tricas.</li> <li>N\u00e3o fornece nem exige um sistema/idioma de configura\u00e7\u00e3o (por exemplo, Jsonnet). Ele fornece uma API declarativa que pode ser direcionada por formas arbitr\u00e1rias de especifica\u00e7\u00f5es declarativas.</li> <li>N\u00e3o fornece nem adota sistemas abrangentes de configura\u00e7\u00e3o de m\u00e1quinas, manuten\u00e7\u00e3o, gerenciamento ou autocorre\u00e7\u00e3o.</li> <li>Adicionalmente, o Kubernetes n\u00e3o \u00e9 um mero sistema de orquestra\u00e7\u00e3o. Na verdade, ele elimina a necessidade de orquestra\u00e7\u00e3o. A defini\u00e7\u00e3o t\u00e9cnica de orquestra\u00e7\u00e3o \u00e9 a execu\u00e7\u00e3o de um fluxo de trabalho definido: primeiro fa\u00e7a A, depois B e depois C. Em contraste, o Kubernetes compreende um conjunto de processos de controle independentes e combin\u00e1veis que conduzem continuamente o estado atual em dire\u00e7\u00e3o ao estado desejado fornecido. N\u00e3o importa como voc\u00ea vai de A para C. O controle centralizado tamb\u00e9m n\u00e3o \u00e9 necess\u00e1rio. Isso resulta em um sistema que \u00e9 mais f\u00e1cil de usar e mais poderoso, robusto, resiliente e extens\u00edvel.</li> </ul>"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#kubernetes-como-ele-funciona","title":"Kubernetes: como ele funciona?","text":"<p>A principal vantagem que as empresas garantem ao usar o Kubernetes, especialmente se estiverem otimizando o desenvolvimento de aplica\u00e7\u00f5es para a cloud, \u00e9 que elas ter\u00e3o uma plataforma para programar e executar containers em clusters de m\u00e1quinas f\u00edsicas ou virtuais. Em termos mais abrangentes, com o Kubernetes, \u00e9 mais f\u00e1cil implementar e confiar totalmente em uma infraestrutura baseada em containers para os ambientes de produ\u00e7\u00e3o. Como o prop\u00f3sito do Kubernetes \u00e9 automatizar completamente as tarefas operacionais, ele permite que os containers realizem muitas das tarefas possibilitadas por outros sistemas de gerenciamento ou plataformas de aplica\u00e7\u00f5es.</p> <p>O Kubernetes possibilita:</p> <ul> <li>Orquestrar containers em v\u00e1rios hosts.</li> <li>Aproveitar melhor o hardware para maximizar os recursos necess\u00e1rios na execu\u00e7\u00e3o das aplica\u00e7\u00f5es corporativas.</li> <li>Controlar e automatizar as implanta\u00e7\u00f5es e atualiza\u00e7\u00f5es de aplica\u00e7\u00f5es.</li> <li>Montar e adicionar armazenamento para executar aplica\u00e7\u00f5es com monitora\u00e7\u00e3o de estado.</li> <li>Escalar rapidamente as aplica\u00e7\u00f5es em containers e recursos relacionados.</li> <li>Gerenciar servi\u00e7os de forma declarativa, garantindo que as aplica\u00e7\u00f5es sejam executadas sempre da mesma maneira como foram implantadas.</li> <li>Verificar a integridade e autorrecupera\u00e7\u00e3o das aplica\u00e7\u00f5es com posicionamento, rein\u00edcio, replica\u00e7\u00e3o e escalonamento autom\u00e1ticos.</li> </ul> <p>No entanto, o Kubernetes depende de outros projetos para oferecer plenamente esses servi\u00e7os orquestrados. Com a inclus\u00e3o de outros projetos open source, \u00e9 poss\u00edvel atingir a capacidade total do Kubernetes. Dentre esses projetos necess\u00e1rios, incluem-se:</p> <ul> <li>Registro, como o Atomic Registry ou o Docker Registry.</li> <li>Rede, como o OpenvSwitch e roteamento de borda inteligente.</li> <li>Telemetria, como o heapster, o kibana, o hawkular e o elastic.</li> <li>Seguran\u00e7a, como o LDAP, o SELinux, o RBAC e o OAUTH com camadas de multiloca\u00e7\u00e3o.</li> <li>Automa\u00e7\u00e3o, com a adi\u00e7\u00e3o de playbooks do Ansible para a instala\u00e7\u00e3o e o gerenciamento do ciclo de vida do cluster.</li> <li>Servi\u00e7os, oferecidos em um cat\u00e1logo variado de conte\u00fados previamente criados de padr\u00f5es de aplica\u00e7\u00f5es populares.</li> </ul>"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#como-ele-se-encaixa-na-infraestrutura","title":"Como ele se encaixa na infraestrutura?","text":"<p>O Kubernetes \u00e9 executado em um sistema operacional e interage com pods de containers executados em n\u00f3s. A m\u00e1quina mestre do Kubernetes aceita os comandos de um administrador (ou equipe de DevOps) e retransmite essas instru\u00e7\u00f5es aos n\u00f3s subservientes. Essa retransmiss\u00e3o \u00e9 realizada em conjunto com v\u00e1rios servi\u00e7os para automaticamente decidir qual n\u00f3 \u00e9 o mais adequado para a tarefa. Depois, s\u00e3o alocados os recursos e atribu\u00eddos os pods do n\u00f3 para cumprir a tarefa solicitada.</p> <p>Portanto, do ponto de vista da infraestrutura, s\u00e3o poucas as mudan\u00e7as em compara\u00e7\u00e3o com a forma como voc\u00ea j\u00e1 gerencia os containers. O controle sobre os containers acontece em um n\u00edvel superior, tornando-o mais refinado, sem a necessidade de microgerenciar cada container ou n\u00f3 separadamente. Ser\u00e1 necess\u00e1rio realizar algum trabalho, mas em sua maioria trata-se somente de uma quest\u00e3o de atribuir um master do Kubernetes e definir os n\u00f3s e pods.</p>"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#fontes-usadas-para-elaboracao-desse-conteudo","title":"Fontes usadas para elabora\u00e7\u00e3o desse conte\u00fado","text":"<p>Documenta\u00e7\u00e3o Kubernete</p> <p>Red Hat</p>"},{"location":"Kubernetes/pods/","title":"O que \u00e9 um POD?","text":"<p>Os pods s\u00e3o as menores unidades de computa\u00e7\u00e3o implant\u00e1veis \u200b\u200bque voc\u00ea pode criar e gerenciar no Kubernetes.</p> <p>Um Pod \u00e9 um grupo de um ou mais cont\u00eaineres, com armazenamento compartilhado e recursos de rede e uma especifica\u00e7\u00e3o de como executar os cont\u00eaineres. O conte\u00fado de um pod \u00e9 sempre co-localizado e co-agendado e executado em um contexto compartilhado. Um Pod modela um \"host l\u00f3gico\" espec\u00edfico do aplicativo: ele cont\u00e9m um ou mais cont\u00eaineres de aplicativos que s\u00e3o relativamente fortemente acoplados. Em contextos fora da nuvem, os aplicativos executados na mesma m\u00e1quina f\u00edsica ou virtual s\u00e3o an\u00e1logos aos aplicativos em nuvem executados no mesmo host l\u00f3gico.</p> <p>Al\u00e9m dos cont\u00eaineres de aplicativos, um Pod pode conter cont\u00eaineres init que s\u00e3o executados durante a inicializa\u00e7\u00e3o do Pod. Voc\u00ea tamb\u00e9m pode injetar cont\u00eaineres ef\u00eameros para depura\u00e7\u00e3o se seu cluster oferecer isso.</p> <p>O contexto compartilhado de um Pod \u00e9 um conjunto de namespaces do Linux, cgroups e potencialmente outras facetas de isolamento - as mesmas coisas que isolam um cont\u00eainer do Docker. Dentro do contexto de um Pod, os aplicativos individuais podem ter mais subisolamentos aplicados.</p> <p>Em termos de conceitos do Docker, um Pod \u00e9 semelhante a um grupo de cont\u00eaineres do Docker com namespaces compartilhados e volumes de sistema de arquivos compartilhados, ou seja, n\u00e3o vamos manipular diretamente os containers e sim os PODs, que vai se encarregar de criar os containers.</p> <p>Os PODs possuem endere\u00e7os IPs, isso significa que independente de quantos containers estejam no POD, existe somente um endere\u00e7o IP que \u00e9 o do POD, e as portas que s\u00e3o liberadas pelos containers, s\u00e3o sempre acessadas a partir do mesmo endere\u00e7o, mesmo que seja de containers diferentes. Lembrando que os essa estrutura de endere\u00e7amento de IP n\u00e3o permite que dois containers possuam a mesma porta. Os PODs podem se comunicar entre s\u00ed, assim como os containers que agoram compartilha o mesmo IP do POD que est\u00e3o alocado, possuem o mesmo IP. possibilitando a comunica\u00e7\u00e3o via localhost.</p>"},{"location":"Kubernetes/pods/#primeiro-pod","title":"Primeiro POD","text":"<p>Para inicialziar um POD, precisamos ir na documenta\u00e7\u00e3o da API e l\u00e1 vamos encontrar esse comando kubectl run, com isso j\u00e1 podemos ir no terminal para iniciar nosso primeiro POD.</p> <pre><code>kubectl run testando --image=nginx\n</code></pre> <p>Para verificar os PODs que est\u00e3o ativos kubectl get pods, na documenta\u00e7\u00e3o temos as flags que podemos aplicar, por exemplo o --watch que visualizar os pods, da cria\u00e7\u00e3o at\u00e9 efetivar a ativa\u00e7\u00e3o.</p> <p>Se precisamos de mais informa\u00e7\u00f5es do POD basta usar kubectl describe pod e passa o nome do POD, que no nosso caso \u00e9 testando:</p> <pre><code>kubectl describe pod testando\n</code></pre> <p>Com isso, temos informa\u00e7\u00e3o do IP do POD, como ele est\u00e1 e os eventos que sucederam ap\u00f3s sua cria\u00e7\u00e3o. Que tal deletamos esse POD com kubectl delete pod.</p> <p>Essa \u00e9 a cria\u00e7\u00e3o simples de um POD, por\u00e9m n\u00e3o \u00e9 a mais recomendada, a documenta\u00e7\u00e3o do kubernetes enfatiza o uso de arquivos com as informa\u00e7\u00f5es sobre a imagem para cria\u00e7\u00e3o do POD, podemos usar tanto o JSON quanto o yaml, nesse caso, por recomenda\u00e7\u00e3o da kubernetes devemos usar conforme as melhores praticas o yaml.</p> <p>Abra uma pasta e crie um arquivo o_primeiro_pod.yaml, pode nomea\u013ao como quiser, mas tem que ter o final .yaml.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: o-primeiro-pod\nspec:\n    containers:\n        - name: container-nginx\n          image: nginx\n</code></pre> <p>Com o arquivo j\u00e1 com as informa\u00e7\u00f5es abra o terminal na pasta que voc\u00ea inseriu o arquivo, e execute kubectl apply com a flag -f para passa o nome do arquivo que queremos executar.</p> <pre><code>kubectl apply -f ./o_primeiro_pod.yaml\n</code></pre> <p>E criamos nosso primeiro POD de modo declarativo.</p>"},{"location":"Kubernetes/pods/#servicos","title":"Servi\u00e7os","text":"<p>Os PODs possuem uma comunica\u00e7\u00e3o interna, e que \u00e9 gerenciando de forma autonoma por um recurso chamado service ou svc, documenta\u00e7\u00e3o do service, \u00e9 importante entender o funcionamento do svc pois ele vai ser usado dentro do nosso cluster para definir comunica\u00e7\u00e3o entre os PODs, entre suas tarefas est\u00e3o:</p> <ul> <li>Abstra\u00e7\u00e3o para expor aplica\u00e7\u00f5es em um POD ou mais.</li> <li>Proveem IPs fixos para comunica\u00e7\u00e3o</li> <li>Proveem DNS em um POD ou mais</li> <li>Fazem o balanceamento de carga</li> </ul> <p>E service possui quatro tipos internos que s\u00e3o:</p> <ul> <li>ClusterIP: Exp\u00f5e o servi\u00e7o em um IP interno no cluster. Esse tipo torna o servi\u00e7o acess\u00edvel apenas de dentro do cluster.</li> <li>NodePort: Exp\u00f5e o servi\u00e7o na mesma porta de cada n\u00f3 selecionado no cluster usando NAT. Torna um servi\u00e7o acess\u00edvel de fora do cluster usando :. Superconjunto de ClusterIP. <li>Load Balance: Cria um balanceador de carga externo na nuvem atual (se compat\u00edvel) e atribui um IP externo fixo ao servi\u00e7o. Superconjunto de NodePort.</li> <li>ExternalName: Mapeia o servi\u00e7o para o conte\u00fado do campo externalName (por exemplo, foo.bar.example.com), retornando um registro CNAME com seu valor. Nenhum proxy de qualquer tipo \u00e9 configurado. Este tipo requer v1.7 ou superior de kube-dns, ou CoreDNS vers\u00e3o 0.0.8 ou superior.</li>"},{"location":"Kubernetes/pods/#clusterip","title":"ClusterIP","text":"<p>Para criar um ClusterIP:</p> POD - 1Service   <p>Crie uma pasta</p> <p>Dentro da pasta crie um arquivo, vou nomealo de pod-1.yaml</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: pod-1\n    labels:\n        app: pod-label\nspec:\n    containers:\n        - name: pod-1\n          image: nginx\n          ports:\n            - containerPort: 80\n</code></pre>   <p>Dentro da pasta crie um arquivo, vou nomealo de svc-pod.yaml</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    name: svc-pod\nspec:\n    type: ClusterIP\n    selector:\n        app: pod-label\n    ports:\n        - port: 4050\n          targetPort: 80\n</code></pre> <p>Repare que no spec especificamos um ClusterIP e n\u00e3o um container, e em selector estamos dizendo para o Servi\u00e7o procura um r\u00f3tulo/labels nos PODs. Que \u00e9 o link de conex\u00e3o, \u00e9 essa informa\u00e7\u00e3o que faz o trabalho de criar a conex\u00e3o. </p> <p>Em ports estou dizendo para entrar a informa\u00e7\u00e3o/requis\u00e3o na porta 4050 e dizendo para sair targetPort na porta 80.</p> <p>E em kind, estamos passando a informa\u00e7\u00e3o de Service e n\u00e3o Pod</p>     <p>Agora, vamos subir os servi\u00e7os e depois os PODs, sempre nessa ordem.</p> 12   <pre><code>kubectl apply -f svc-pod.yaml\n</code></pre>   <pre><code>kubectl apply -f pod-1.yaml\n</code></pre>     <p>Para verificar os IPs internos usamos o kubectl get pods e passamos a flag -o e wide:</p> ExecuteSaida   <pre><code>kubectl get pods -o wide\n</code></pre>   <pre><code>NAME             READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES\no-primeiro-pod   1/1     Running   0          5m37m   172.17.0.3   minikube   &lt;none&gt;           &lt;none&gt;\npod-1            1/1     Running   0          2m26s   172.17.0.4   minikube   &lt;none&gt;           &lt;none&gt;\n</code></pre>     <p>Para verificar o cluster IP precisamo usar o kubectl get svc.</p> <pre><code>kubectl get svc\n</code></pre> <pre><code>```bash\nNAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nkubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    2d23h\nsvc-pod      ClusterIP   10.105.54.122   &lt;none&gt;        4050/TCP   9m4s\n```\n</code></pre> <p>Com isso a comunica\u00e7\u00e3o interna do Cluster est\u00e1 com um fluxo, se houvesse mais PODs, eles conseguiriam se comunicar com o POD1 atraves do ClusterIP que definimos, por\u00e9m o POD1.</p>"},{"location":"Kubernetes/pods/#nodeport","title":"NodePort","text":"<p>Criando um NodePort.</p> POD - 2Service   <p>Dentro da pasta crie um arquivo, vou nomealo de pod-1.yaml</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: pod-2\n    labels:\n        app: pod-label2\nspec:\n    containers:\n        - name: pod-2\n          image: sposigor/app-flask-teste:1\n          ports:\n            - containerPort: 5000\n</code></pre>   <p>Dentro da pasta crie um arquivo, vou nomealo de svc-pod.yaml</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    name: svc-pod2\nspec:\n    type: NodePort\n    ports:\n        - port: 5000\n          nodePort: 30000\n    selector:\n        app: pod-label2\n</code></pre>    <p>Novamente, precisamos inicializar o servi\u00e7o e em seguida o POD.</p> ExecuteSaida   <pre><code>kubectl get nodes -o wide\n</code></pre>   <pre><code>NAME       STATUS   ROLES                  AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nminikube   Ready    control-plane,master   3d22h   v1.23.3   192.168.49.2   &lt;none&gt;        Ubuntu 20.04.2 LTS   5.15.25-1-MANJARO   docker://20.10.12\n</code></pre>    <p>Precisamos do INTERNAL-IP e com isso passamos a porta 30000 no navegador, no meu caso 192.168.49.2:30000.</p>"},{"location":"Kubernetes/pods/#load-balancer","title":"Load Balancer","text":"<p>Por padr\u00e3o, os servi\u00e7os em nuvem que prestam a integra\u00e7\u00e3o do Kubernetes j\u00e1 disponibilizam o Load Balance, por tr\u00e1s dos panos ele faz tanto o trabalho de ClusterIP e se integra de forma automatica ao NodePort, ou seja, a porta externa \u00e9 disponibilizada pelo Load Balance do provedor.</p> <p>Vou deixar um exemplo de .yaml para Load Balance.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-loadbalancer\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      nodePort: 30000\n  selector:\n    app: pod-label1\n</code></pre>"},{"location":"Kubernetes/projeto/","title":"Projeto","text":"<p>Que tal um projeto integrando toda a explica\u00e7\u00e3o at\u00e9 aqui?</p> <p>Vamos usar o docker samples.</p> <p>Crie um .yaml:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: db\n  labels:\n    app: words-db\nspec:\n  ports:\n    - port: 5432\n      targetPort: 5432\n      name: db\n  selector:\n    app: words-db\n  clusterIP: None\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db\n  labels:\n    app: words-db\nspec:\n  selector:\n    matchLabels:\n      app: words-db\n  template:\n    metadata:\n      labels:\n        app: words-db\n    spec:\n      containers:\n      - name: db\n        image: dockersamples/k8s-wordsmith-db\n        ports:\n        - containerPort: 5432\n          name: db\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: words\n  labels:\n    app: words-api\nspec:\n  ports:\n    - port: 8080\n      targetPort: 8080\n      name: api\n  selector:\n    app: words-api\n  clusterIP: None\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: words\n  labels:\n    app: words-api\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: words-api\n  template:\n    metadata:\n      labels:\n        app: words-api\n    spec:\n      containers:\n      - name: words\n        image: dockersamples/k8s-wordsmith-api\n        ports:\n        - containerPort: 8080\n          name: api\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web\n  labels:\n    app: words-web\nspec:\n  ports:\n    - port: 8081\n      targetPort: 80\n      name: web\n  selector:\n    app: words-web\n  type: LoadBalancer\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: words-web\nspec:\n  selector:\n    matchLabels:\n      app: words-web\n  template:\n    metadata:\n      labels:\n        app: words-web\n    spec:\n      containers:\n      - name: web\n        image: dockersamples/k8s-wordsmith-web\n        ports:\n        - containerPort: 80\n          name: words-web\n</code></pre> <p>Em seguida:</p> ExecuteSaida   <pre><code>kubectl apply -f kube-deployment.yml\n</code></pre> <p>Para verificar os servi\u00e7os ativos:</p> <pre><code>kubectl get svc\n</code></pre>   <pre><code>NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\ndb           ClusterIP      None            &lt;none&gt;        5432/TCP         17s\nkubernetes   ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP          3d23h\nsvc-pod      ClusterIP      10.105.54.122   &lt;none&gt;        9000/TCP         24h\nsvc-pod2     NodePort       10.98.89.245    &lt;none&gt;        5000:30000/TCP   61m\nweb          LoadBalancer   10.97.254.156   &lt;pending&gt;     8081:31555/TCP   17s\nwords        ClusterIP      None            &lt;none&gt;        8080/TCP         17s\n</code></pre>    <p>No meu caso ele redirecionou o Load Balance para porta 31555, ficando assim 192.168.49.2:31555. Se precisar do INTERNAL-IP, basta executar kubectl get nodes -o wide.</p>"},{"location":"Kubernetes/replicaset/","title":"ReplicaSet","text":"<p>A finalidade de um ReplicaSet \u00e9 manter um conjunto est\u00e1vel de pods de r\u00e9plica em execu\u00e7\u00e3o a qualquer momento. Como tal, \u00e9 frequentemente usado para garantir a disponibilidade de um n\u00famero especificado de Pods id\u00eanticos.</p>"},{"location":"Kubernetes/replicaset/#_1","title":"ReplicaSet","text":""},{"location":"Kubernetes/replicaset/#como-funciona-os-replicaset","title":"Como funciona os ReplicaSet","text":"<p>Um ReplicaSet \u00e9 definido com campos, incluindo um seletor que especifica como identificar os pods que ele pode adquirir, um n\u00famero de r\u00e9plicas indicando quantos pods ele deve manter e um modelo de pod especificando os dados dos novos pods que ele deve criar para atender ao n\u00famero de crit\u00e9rios de r\u00e9plicas. Um ReplicaSet cumpre sua finalidade criando e excluindo Pods conforme necess\u00e1rio para atingir o n\u00famero desejado. Quando um ReplicaSet precisa criar pods, ele usa seu modelo de pod.</p> <p>Um ReplicaSet \u00e9 vinculado a seus pods por meio do campo metadata.ownerReferences dos pods, que especifica de qual recurso o objeto atual pertence. Todos os Pods adquiridos por um ReplicaSet t\u00eam as informa\u00e7\u00f5es de identifica\u00e7\u00e3o de seu pr\u00f3prio ReplicaSet no campo ownerReferences. \u00c9 por meio desse link que o ReplicaSet conhece o estado dos Pods que est\u00e1 mantendo e planeja de acordo.</p> <p>Um ReplicaSet identifica novos pods a serem adquiridos usando seu seletor. Se houver um pod que n\u00e3o tenha OwnerReference ou OwnerReference n\u00e3o for um Controlador e corresponder ao seletor de um ReplicaSet, ele ser\u00e1 imediatamente adquirido pelo referido ReplicaSet.</p>"},{"location":"Kubernetes/replicaset/#quando-usar-um-replicaset","title":"Quando usar um ReplicaSet?","text":"<p>Um ReplicaSet garante que um n\u00famero especificado de r\u00e9plicas de pod esteja em execu\u00e7\u00e3o a qualquer momento. No entanto, uma implanta\u00e7\u00e3o \u00e9 um conceito de n\u00edvel superior que gerencia ReplicaSets e fornece atualiza\u00e7\u00f5es declarativas para Pods com muitos outros recursos \u00fateis. Portanto, recomendamos o uso de implanta\u00e7\u00f5es em vez de usar diretamente ReplicaSets, a menos que voc\u00ea exija uma orquestra\u00e7\u00e3o de atualiza\u00e7\u00e3o personalizada ou n\u00e3o exija nenhuma atualiza\u00e7\u00e3o.</p> <p>Na verdade, isso significa que talvez voc\u00ea nunca precise manipular objetos ReplicaSet: use um Deployment e defina seu aplicativo na se\u00e7\u00e3o spec.</p>"},{"location":"Kubernetes/replicaset/#configurando-o-replicaset","title":"Configurando o replicaset","text":"<p>Vamos usar o exemplo disponivel na documenta\u00e7\u00e3o:</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  # modify replicas according to your case\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n</code></pre> <p>Antes de continuar, precisamos entender melhor cada parte dessa configura\u00e7\u00e3o feita.</p> Parte 1Parte 2   <pre><code>kind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\n</code></pre> <p>Estamos passando para o kind o ReplicaSet para o kubernetes entender ser um servi\u00e7o do tipo ReplicaSet</p> <p>Em metadata mais especificamente no labels foi inserido o tier que \u00e9 um nome para um conjunto, ent\u00e3o imagine que tem varios PODs e ao inv\u00e9s de criar um matchLabels para cada app, criamos um tier e inserimos essa informa\u00e7\u00e3o em todos os PODs desse conjunto, assim precisamos criar somente um ReplicaSet para todos esses PODs.</p>   <pre><code>spec:\n  # modify replicas according to your case\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n</code></pre> <p>replicas \u00e9 a quantidade de PODs ideal para aplica\u00e7\u00e3o, mas tamb\u00e9m \u00e9 a quantidade de PODs que sobem. Teremos um n\u00famero ideal de PODs dispon\u00edveis e a quantidade de PODs que est\u00e3o efetivamente dispon\u00edveis e nesse momento o ReplicaSet faz sua m\u00e1gica, sendo a substitui\u00e7\u00e3o autom\u00e1tica dos PODs que efetivamente caem.</p> <p>selector estamos falando para ele procurar todos os PODs, do tier frontend para realizar o processo de monitorar, replicar e substituir.</p> <p>template \u00e9 o padr\u00e3o que vai conter as informa\u00e7\u00f5es do POD.</p>     <p>Com isso, basta criar o arquivo.yaml e executar.</p> <pre><code>kubectl apply -f replicas.yaml\n</code></pre> <p>Ele vai executar o comando e vamos verificar:</p> <pre><code>kubectl get rs\n</code></pre> <p>Saida:</p> <pre><code>NAME       DESIRED   CURRENT   READY   AGE\nfrontend   3         3         3       3s\n</code></pre> <p>Temos o nome do ReplicaSet que criamos, a quantidade de Desired que \u00e9 um n\u00famero desejado informado em replicas o Current que \u00e9 a informa\u00e7\u00e3o sobre os PODs que est\u00e3o normais e o Ready que est\u00e1 informando que est\u00e3o todos ativos.</p>"},{"location":"Kubernetes/replicaset/#testando-o-replicaset","title":"Testando o ReplicaSet","text":"<p>Com a mesma aplica\u00e7\u00e3o, observaremos na pr\u00e1tica como ele funciona, primeiro abra dois terminas e deixe um do lado do outro.</p> <p>No primeiro vamos digitar o kubectl get rs e passar o par\u00e2metro --watch para conseguimos verificar em tempo real.</p> <pre><code>kubectl get rs --watch\n</code></pre> <p>No outro terminal, faremos primeiro um kubectl get pod e em seguida kubectl delete pod e passe um dos PODs dispon\u00edveis.</p> get poddelete   <pre><code>kubectl get pod\n</code></pre> <pre><code>NAME             READY   STATUS    RESTARTS   AGE\nfrontend-hvvnp   1/1     Running   0          6m53s\nfrontend-t4l87   1/1     Running   0          6m53s\nfrontend-xqfsq   1/1     Running   0          6m53s\n</code></pre> <p>Vou usar o frontend-hvvnp para o delete</p>   <pre><code>kubectl delete pod frontend-hvvnp\n</code></pre>    <p>Ap\u00f3s concluir o procedimento, perceba que o outro terminal teve uma altera\u00e7\u00e3o:</p> <pre><code>NAME       DESIRED   CURRENT   READY   AGE\nfrontend   3         3         3       8m18s\nfrontend   3         2         2       8m50s\nfrontend   3         3         2       8m50s\nfrontend   3         3         3       8m51s\n</code></pre> <p>Aqui estamos vendo que o valor de Current e Ready tiveram mudan\u00e7a e em seguida retornaram ao valor original.</p> <p>Com isso nosso teste est\u00e1 conclu\u00eddo.</p>"},{"location":"Kubernetes/replicaset/#balanceamento-de-carga","title":"Balanceamento de Carga","text":"<p>O ReplicaSet tamb\u00e9m se integra no balanceamento de carga, visto que o servi\u00e7o pode ser distribu\u00eddo igualmente pelas r\u00e9plicas dispon\u00edveis.</p> <p>Para verificar mais informa\u00e7\u00f5es consultar a Documenta\u00e7\u00e3o</p>"},{"location":"docker/aprende_mais/","title":"Quer continuar com os estudos e se apefei\u00e7oar no Docker?","text":"<p>Vou deixa algumas recomenda\u00e7\u00f5es para entender qual os proximos passos que podemos tomar para nos aprofundar em docker.</p>"},{"location":"docker/aprende_mais/#para-revisar","title":"Para revisar","text":"<p>Docker em 120 - LinuxTips</p> <p>Docker em 7 passos - Fireship</p>"},{"location":"docker/aprende_mais/#para-acompanhar","title":"Para Acompanhar","text":"<p>A alguns anos um evento chamado DockerCon, come\u00e7ou a ser realizado anualmente, l\u00e1 s\u00e3o mostrados cases e muitas informa\u00e7\u00f5es importantes.</p> <p>Vou deixa a DockerCon 2021 aqui como referencia do evento:</p> <p>DockerCon 2021: Sala Brasil</p>"},{"location":"docker/aprende_mais/#para-aplicar","title":"Para Aplicar","text":"<p>Dei exemplos de usar imagens docker para servidores sql, mas tamb\u00e9m \u00e9 para usar em aplica\u00e7\u00f5es web, back-end e at\u00e9 mesmo aplica\u00e7\u00e3o em dados.</p> <p>Basta criar o habitor de aplicar o docker nos projetos que voc\u00ea cria, seja para simplificar em uma unica imagem ou parte de um processo maior.</p>"},{"location":"docker/compose/","title":"Docker Compose","text":""},{"location":"docker/compose/#compose-o-que-e","title":"Compose o que \u00e9?","text":"<p>O docker compose \u00e9 uma ferramenta desenvolvida para ajudar a definir e compartilhar aplicativos de v\u00e1rios cont\u00eaineres. A grande vantagem de usar o Compose \u00e9 que voc\u00ea pode definir todos os aplicativos em um arquivo, mant\u00ea-la na raiz do reposit\u00f3rio do seu projeto (agora com controle de vers\u00e3o) e permitir que outra pessoa contribua facilmente para o seu projeto.</p> <p>Algu\u00e9m precisaria apenas clonar seu reposit\u00f3rio e iniciar o aplicativo de composi\u00e7\u00e3o. Na verdade, voc\u00ea pode ver alguns projetos no GitHub/GitLab fazendo exatamente isso agora.</p> <p>O arquivo de defini\u00e7\u00e3o do Docker Compose \u00e9 o local onde \u00e9 especificado todo o ambiente (rede, volume e servi\u00e7os), ele \u00e9 escrito seguindo o formato YAML. Esse arquivo por padr\u00e3o tem como nome docker-compose.yml.</p>"},{"location":"docker/compose/#o-que-e-o-yaml","title":"O que \u00e9 o YAML?","text":"<p>YAML \u00e9 uma linguagem de serializa\u00e7\u00e3o de dados que \u00e9 frequentemente usada para gravar arquivos de configura\u00e7\u00e3o. Dependendo de quem voc\u00ea pergunta, YAML significa ainda outra linguagem de marca\u00e7\u00e3o ou YAML n\u00e3o \u00e9 uma linguagem de marca\u00e7\u00e3o (um acr\u00f4nimo recursivo), que enfatiza que YAML \u00e9 para dados, n\u00e3o documentos.</p> <p>YAML \u00e9 uma linguagem de programa\u00e7\u00e3o popular porque \u00e9 leg\u00edvel e f\u00e1cil de entender. Tamb\u00e9m pode ser usado em conjunto com outras linguagens de programa\u00e7\u00e3o.</p>"},{"location":"docker/compose/#instalando-o-docker-compose","title":"Instalando o Docker-Compose","text":"WindowsMacLinux   <ul> <li>O docker compose \u00e9 instalado junto ao docker</li> </ul> <pre><code>docker-compose\n</code></pre>   <ul> <li>O docker compose \u00e9 instalado junto ao docker</li> </ul> <pre><code>docker-compose\n</code></pre>   <ul> <li>No linux, basta seguir a documenta\u00e7\u00e3o</li> <li>Se houve instala\u00e7\u00e3o via Snap ou Flatpak, o docker compose precisa usar a mesma via</li> <li>Ap\u00f3s a instala\u00e7\u00e3o</li> </ul> <pre><code>docker-compose\n</code></pre>"},{"location":"docker/compose/#anatomia-do-docker-composeyml","title":"Anatomia do docker-compose.yml","text":"<p>O padr\u00e3o YAML utiliza a indenta\u00e7\u00e3o como separador dos blocos de c\u00f3digos das defini\u00e7\u00f5es, por conta disso o uso da indenta\u00e7\u00e3o \u00e9 um fator muito importante, ou seja, caso n\u00e3o a utilize corretamente, o docker-compose falhar\u00e1 em sua execu\u00e7\u00e3o.</p> <p>Cada linha desse arquivo pode ser definida com uma chave valor ou uma lista. Nesse caso, vamos usar a imagem que criamos:</p> <ol> <li>Crie uma pasta</li> <li>Crie um arquivo chamado docker-compose.yml na pasta</li> <li>Copiei e cole:</li> </ol> <pre><code>version: \"1\"\nservices:\n    flask_web:\n        image: sposigor/app-flask-teste:1\n        ports:\n          - 5000:5000\n</code></pre> <p>Abra o terminal na pasta, que est\u00e1 o arquivo docker-compose.yml e execute:</p> <pre><code>docker-compose up -d\n</code></pre> <p>Ilustra\u00e7\u00e3o usando o docker-compose up </p> <p>V\u00e1 em localhost:5000 para verificar.</p> <p>Ai est\u00e1 a m\u00e1gica do compose, com um simples documento yml, podemos configurar nosso cont\u00eaineres para rodar com um simples comando e para finalizar:</p> <pre><code>docker-compose down\n</code></pre> <p>Vou deixar a documenta\u00e7\u00e3o da docker para todas as possibilidades da estrutura docker github \u00e9 importante ler, n\u00e3o fique preocupado com todas as especifica\u00e7\u00f5es, geralmente com algumas \u00e9 o suficiente para subir o cont\u00eainer a partir do .yml.</p> <p>Pare de fazer confus\u00e3o com o Docker Compose</p>"},{"location":"docker/compose/#vamos-brincar-um-pouco-com-o-compose","title":"Vamos brincar um pouco com o compose","text":"<p>Agora que voc\u00ea entendeu um pouco sobre o compose, vou deixar alguns exemplos de arquivos docker-compose.yml.</p> MySQLPostgresSQLPostgresSQL + pgadmin   <pre><code>version: \"1\"\nservices:\n  db:\n    image: mysql:8.0-oracle\n    restart: always\n    environment:\n      MYSQL_DATABASE: 'db'\n      MYSQL_USER: 'user'\n      MYSQL_PASSWORD: 'password'\n      MYSQL_ROOT_PASSWORD: 'password'\n    ports:\n      - '3306:3306'\n    expose:\n      - '3306'\n    volumes:\n      - my-db:/var/lib/mysql\nvolumes:\n  my-db:\n</code></pre> <ul> <li>Use um editor sql de sua preferencia e fa\u00e7a a conex\u00e3o.</li> </ul>   <pre><code>version: '1'\nservices:\n  postgres-compose:\n    image: postgres:14.-alpine\n    restart: always\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n    ports:\n      - '5432:5432'\n    volumes:\n      - db:/var/lib/postgresql/data\nvolumes:\n  db:\n    driver: local\n</code></pre> <ul> <li>Use um editor sql de sua preferencia e fa\u00e7a a conex\u00e3o.</li> </ul>   <pre><code>version: '1'\nservices:\n  postgres-compose:\n    image: postgres:14.2-alpine\n    environment:\n      POSTGRES_USERNAME: \"postgres\"\n      POSTGRES_PASSWORD: \"postgres\"\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - db:/var/lib/postgresql/data\n    networks:\n      - postgres-compose-network\n\n  pgadmin-compose:\n    image: dpage/pgadmin4\n    environment:\n      PGADMIN_DEFAULT_EMAIL: \"qualquer@email.com\"\n      PGADMIN_DEFAULT_PASSWORD: \"postgres\"\n    ports:\n      - \"1515:80\"\n    depends_on:\n      - postgres-compose\n    networks:\n      - postgres-compose-network\n\nvolumes:\n  db:\n    driver: local\n\nnetworks:\n  postgres-compose-network:\n    driver: bridge\n</code></pre> <ul> <li>V\u00e1 no localhost:1515</li> <li>login: qualquer@email.com</li> <li>Senha: postgres</li> <li>Pronto mais uma conex\u00e3o bem sucedida</li> </ul>     <p>Al\u00e9m disso, recomendo verificar esse reposit\u00f3rio da docker com m\u00faltiplos exemplos awesome-compose</p> <p>Tente bastante, repita cada passo, crie suas varia\u00e7\u00f5es.</p>"},{"location":"docker/docker_hub/","title":"Docker Hub","text":""},{"location":"docker/docker_hub/#o-que-e-o-docker-hub","title":"O que \u00e9 o Docker Hub? \ud83e\udd14","text":"<p>Para exemplificar, o Docker Hub \u00e9 um reposit\u00f3rio que possui imagens para o docker</p> <p>Para acessar Docker Hub</p> <ol> <li>Na p\u00e1gina inicial, pesquisaremos o hello-world.</li> <li>Pesquise e chegaremos nessa p\u00e1gina hello-world</li> <li>L\u00e1 encontraremos uma refer\u00eancia e informa\u00e7\u00f5es importantes</li> <li>Tamb\u00e9m repare que existe uma op\u00e7\u00e3o de fazer um pull, ou seja, baixa a imagem localmente</li> </ol> <p>Existe um selo oficial das imagens que est\u00e3o no docker hub: </p> <p>Quando for necess\u00e1rio pesquise a imagem.</p>"},{"location":"docker/docker_hub/#ubuntu-um-so","title":"Ubuntu: um SO \u2b50","text":"<p>Realizaremos um exerc\u00edcio</p> <ul> <li>Fa\u00e7a o pull da imagem do Ubuntu</li> </ul> <pre><code>    docker pull ubuntu\n</code></pre> <ul> <li>Rode a imagem no docker</li> </ul> <pre><code>    docker run ubuntu\n</code></pre> <p>Ap\u00f3s ter realizado a etapa 1 e 2, porque a interface do Ubuntu n\u00e3o apareceu?</p> <p>Agora temos mais perguntas que respostas.</p> <ol> <li>Porque o Cont\u00eainer n\u00e3o rodou que nem o hello-world</li> <li>Se rodou o que o docker run fez ent\u00e3o?</li> </ol>"},{"location":"docker/fluxo_containers/","title":"Fluxo","text":""},{"location":"docker/fluxo_containers/#fluxo-de-criacao-do-conteiner","title":"Fluxo de Cria\u00e7\u00e3o do Cont\u00eainer","text":"<p>Vamos retorna para nossas perguntas</p> <ol> <li>Porque o Cont\u00eainer n\u00e3o rodou que nem o hello-world</li> <li>Se rodou o que o docker run fez ent\u00e3o?</li> </ol> <p>Para tentar responder vamos explorar mais alguns comandos do docker.</p> <p>Execute a fun\u00e7\u00e3o ps do docker para observar a lista dos cont\u00eaineres que est\u00e3o rodando</p> <pre><code>docker ps\n</code></pre> <p>Mesmo ap\u00f3s ter dado o comando docker run ubuntu, o docker ps somente retorna o cabe\u00e7alho da listagem, ou seja, n\u00e3o h\u00e1 nenhum cont\u00eainer em execu\u00e7\u00e3o.</p> <pre><code>CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n</code></pre> <p>Bem, se o docker ps n\u00e3o mostrou, tem um comando mais verboso para explicitar a listagem:</p> <pre><code>docker container ls\n</code></pre> <p>O docker cont\u00eainer ls lista os cont\u00eaineres como o ps.</p> <p>Por\u00e9m, o resultado foi o mesmo, para resolver, devemos acrescentar o verbo -a.</p> docker psdocker container ls   <pre><code>docker ps -a\n</code></pre>   <pre><code>docker container ls -a\n</code></pre>    <p>Ap\u00f3s a execu\u00e7\u00e3o a sa\u00edda, vai ser parecida com essa:</p> <pre><code>CONTAINER ID   IMAGE         COMMAND    CREATED          STATUS                      PORTS     NAMES\n65adb2cb2cd5   ubuntu        \"bash\"     4 seconds ago    Exited (0) 4 seconds ago              competent_thompson\nf690741c0bc7   ubuntu        \"bash\"     47 seconds ago   Exited (0) 47 seconds ago             infallible_williams\n6914d474838d   hello-world   \"/hello\"   2 hours ago      Exited (0) 2 hours ago                upbeat_lichterman\n</code></pre> <p>Ent\u00e3o vamos entender o significado de cada cabe\u00e7alho.</p> <ul> <li>CONTAINER ID: \u00c9 o ID do cont\u00eainer, identifica\u00e7\u00e3o.</li> <li>IMAGE: \u00c9 a imagem do cont\u00eainer.</li> <li>COMMAND: \u00c9 o comando que est\u00e1 setado para executar</li> <li>CREATED: \u00c9 o tempo que passou ap\u00f3s a cria\u00e7\u00e3o</li> <li>STATUS: \u00c9 a lista de status created, restarting, running, removing, paused, exited.</li> <li>PORTS: \u00c9 a porta da rede que o docker est\u00e1 rodando, exemplo 8080</li> <li>NAMES: S\u00e3o os nomes dos cont\u00eaineres</li> </ul> <p>Al\u00e9m disso, \u00e9 poss\u00edvel visualizar mais op\u00e7\u00f5es com o comando docker ps e docker container ls.</p>"},{"location":"docker/fluxo_containers/#voltando-para-as-questoes","title":"Voltando para as quest\u00f5es","text":"<p>Ent\u00e3o conforme a sa\u00edda do comando, o ubuntu subiu a imagem, executou o comando bash e finalizou.</p> <p>Certo, que tal, alterarmos o comando padr\u00e3o?</p> <p>Vamos pedir aquela ajuda do --help</p> <pre><code>docker run --help\n</code></pre> <p>Agora que retornou algumas op\u00e7\u00f5es para o comando, temos tamb\u00e9m a sintaxe do comando que \u00e9:</p>  <p>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</p>  <p>Interessante, nessa sintaxe est\u00e1 dizendo que podemos executar comandos da imagem, no caso do ubuntu, podemos usar o terminal.</p> <pre><code>docker run ubuntu sleep 2m\n</code></pre> <p>Pedir para o ubuntu rodar um sleep de 2 minutos para verificamos o status desse novo cont\u00eainer. Ap\u00f3s enviar o comando ele travou o terminal, significa que est\u00e1 rodando. Abra uma nova aba do terminal e execute um docker ps -a para observar a sa\u00edda</p> <pre><code>CONTAINER ID   IMAGE         COMMAND      CREATED          STATUS                      PORTS     NAMES\nbb1bf42f3fc3   ubuntu        \"sleep 2m\"   18 seconds ago   Up 17 seconds                         frosty_ishizaka\n65adb2cb2cd5   ubuntu        \"bash\"       25 minutes ago   Exited (0) 25 minutes ago             competent_thompson\nf690741c0bc7   ubuntu        \"bash\"       25 minutes ago   Exited (0) 25 minutes ago             infallible_williams\n6914d474838d   hello-world   \"/hello\"     3 hours ago      Exited (0) 3 hours ago                upbeat_lichterman\n</code></pre> <p>Op\u00e1, ai est\u00e1, o comando mudou, e o cont\u00eainer est\u00e1 com um status diferente Up, est\u00e1 indicando que est\u00e1 rodando.</p>"},{"location":"docker/fluxo_containers/#comandos-uteis","title":"Comandos \u00dateis","text":"<p>Certo, agora entendemos um pouco do fluxo que o docker faz para rodar uma imagem. Agora vamos aprender mais alguns comandos para auxiliar nessa jornada.</p> <p>Volte no terminal e execute:</p> <pre><code>docker run ubuntu sleep 5m\n</code></pre> <p>Ele vai congelar o terminal, abra uma nova aba ou terminal e execute o docker ps para visualizar o cont\u00eainer que est\u00e1 ativo e agora vamos parar esse cont\u00eainer que est\u00e1 em execu\u00e7\u00e3o. Execute agora o docker stop, mas passe o CONTAINER ID no seu terminal.</p> <pre><code>docker stop 19bf7322160d\n</code></pre> <p>Ap\u00f3s a execu\u00e7\u00e3o do comando ele vai parar o cont\u00eainer que estava ativo, para visualizar docker ps -a</p> <pre><code>CONTAINER ID   IMAGE         COMMAND      CREATED          STATUS                       PORTS     NAMES\n19bf7322160d   ubuntu        \"sleep 5m\"   41 seconds ago   Exited (137) 7 seconds ago             wizardly_shtern\nbb1bf42f3fc3   ubuntu        \"sleep 2m\"   15 minutes ago   Exited (0) 13 minutes ago              frosty_ishizaka\n65adb2cb2cd5   ubuntu        \"bash\"       40 minutes ago   Exited (0) 40 minutes ago              competent_thompson\nf690741c0bc7   ubuntu        \"bash\"       40 minutes ago   Exited (0) 40 minutes ago              infallible_williams\n6914d474838d   hello-world   \"/hello\"     3 hours ago      Exited (0) 3 hours ago                 upbeat_lichterman\n</code></pre> <p>Bem o stop para a execu\u00e7\u00e3o, que tal tentamos o docker start, basta passar o CONTAINER ID para iniciar o cont\u00eainer novamente:</p> <pre><code>docker start 19bf7322160d\n</code></pre> <p>Ele retorna o CONTAINER ID, basta executar o docker ps para validar que est\u00e1 rodando novamente, com isso voc\u00ea pode pausar com stop e reiniciar um cont\u00eainer com start. Mas, ainda n\u00e3o interagimos com o terminal do ubuntu, para conseguimos acessar o terminal do cont\u00eainer, vamos usar o comando docker exec, mas antes de executar vamos verificar o --help?</p> <pre><code>docker exec --help\n</code></pre> <p>Ele vai retorna a sintaxe e as op\u00e7\u00f5es, mas vamos verificar a sintaxe primeiro.</p>  <p>docker exec [OPTIONS] CONTAINER COMMAND [ARG...]</p>  <p>Ele est\u00e1 nos dizendo que al\u00e9m das op\u00e7\u00f5es que nos forneceu, podemos d\u00e1 um comando no cont\u00eainer e que precisamos do CONTAINER ID, vamos tentar? Veja se tem algum cont\u00eainer com a imagem do ubuntu ativa, se n\u00e3o, ative usando o start. Agora verifique no ps e vamos l\u00e1.</p> <pre><code>docker exec -it 19bf7322160d bash\n</code></pre> <p>O que estou falando para o docker? Que a gente quer executar um comando interativo -i e alocar um pseudo-terminal -t no CONTAINER ID  e executa o bash.</p> <p>Agora no seu terminal, deve est\u00e1 vendo a linha de comando do cont\u00eainer que voc\u00ea ativou. Nesse momento voc\u00ea deve est\u00e1 vendo algo +- assim: </p> <p>Legal n\u00e9, agora temos vamos falar do docker pause, podemos pausa um cont\u00eainer ativo:</p> <pre><code>docker pause 19bf7322160d\n</code></pre> <p>E para sair da pausa, vamos usar o docker unpause</p> <pre><code>docker unpause 19bf7322160d\n</code></pre> <p>Que tal removemos o cont\u00eainer que est\u00e1 inativo? Para isso vamos usar o docker rm</p> <pre><code>docker rm 19bf7322160d\n</code></pre> <p>Com isso voc\u00ea pode fazer a remo\u00e7\u00e3o dos cont\u00eaineres.</p>"},{"location":"docker/fluxo_containers/#agora-que-aprendeu-uns-comandos-novos","title":"Agora que aprendeu uns comandos novos \ud83d\ude42","text":"<p>Por \u00faltimo, que tal fazemos aquele:</p> <pre><code>docker run ubuntu\n</code></pre> <p>Contudo, sabemos agora que o que acontece quando o fazemos, vamos adicionar uma op\u00e7\u00e3o nesse comando, vamos inserir as op\u00e7\u00f5es -i e -t no comando para rodar o ubuntu e observar o que acontece.</p> <pre><code>docker run -it ubuntu bash\n</code></pre> <p>E agora nosso comando run, gerou o resultado que est\u00e1vamos esperando. Voc\u00ea est\u00e1 no terminal do cont\u00eainer.</p>"},{"location":"docker/fluxo_containers/#portas","title":"Portas","text":"<p>Vamos acessar uma imagem um pouco diferente agora, j\u00e1 vimos um hello world e um SO, vamos ver um site est\u00e1tico imagem, voc\u00ea j\u00e1 deve saber o que \u00e9 preciso para rodar essa imagem, repita os passos:</p> <pre><code>docker pull dockersamples/static-site\n</code></pre> <p>Antes de usamos o docker run, vamos passar o comando -d, para ele n\u00e3o congelar no terminal enquanto usamos:</p> <pre><code>docker run -d dockersamples/static-site\n</code></pre> <p>Apos finalizar a execu\u00e7\u00e3o do comando, fa\u00e7a um docker ps para observar o resultado:</p> <pre><code>CONTAINER ID   IMAGE                       COMMAND                  CREATED         STATUS         PORTS             NAMES\n26040d6c5529   dockersamples/static-site   \"/bin/sh -c 'cd /usr\u2026\"   9 seconds ago   Up 8 seconds   80/tcp, 443/tcp   nervous_goldberg\n</code></pre> <p>Perceba que temos agora uma informa\u00e7\u00e3o em PORTS, ele est\u00e1 dizendo que est\u00e1 dispon\u00edvel na porta 80, vamos verificar?</p> <p>No seu navegador:</p> <pre><code>localhost:80\n</code></pre> <p>O navegador retorna um erro, vamos tentar a porta 443 e o resultado \u00e9 o mesmo, porque n\u00e3o conseguimos ver? Por causa da estrutura de isolamento dos cont\u00eaineres, n\u00e3o devemos esquecer que o isolamento est\u00e1 presente. Para resolver isso, remover o cont\u00eainer que acabamos de criar e passa a op\u00e7\u00e3o --force.</p> <pre><code>docker rm 26040d6c5529 --force\n</code></pre> <p>Ap\u00f3s a execu\u00e7\u00e3o, vamos passa mais uma op\u00e7\u00e3o para o comando run o -P que vai explicitar as portas e redirecionar automaticamente as portas do cont\u00eainer para as portas da nossa maquina:</p> <pre><code>docker run -d -P dockersamples/static-site\n</code></pre> <p>Use o docker ps para observar a sa\u00edda, e em PORTS a sa\u00edda est\u00e1 um pouco diferente, para visualizamos com mais clareza, vamos usar o docker port.</p> <pre><code>docker port ca557128fd50\n</code></pre> <p>E a sa\u00edda vai ser \u00b1 assim:</p> <pre><code>443/tcp -&gt; 0.0.0.0:49153\n443/tcp -&gt; :::49153\n80/tcp -&gt; 0.0.0.0:49154\n80/tcp -&gt; :::49154\n</code></pre> <p>A porta 443 foi redirecionada para a porta 49153 do nosso sistema e a 80 para 49154, por padr\u00e3o a porta 443 e 80 s\u00e3o sempre liberadas no cont\u00eaineres, vamos verificar?</p> <p>No navegador:</p> <pre><code>localhost:49154\n</code></pre> <p>E a resposta do navegador vai ser essa aqui: </p> <p>Al\u00e9m do -P tamb\u00e9m tem o -p, que permite que fa\u00e7amos o mapeamento espec\u00edfico da porta, vamos tentar?</p> <p>Antes vou remover o cont\u00eainer atual:</p> <pre><code>docker rm ca557128fd50 --force\n</code></pre> <p>E vamos usar o comando:</p> <pre><code>docker -d -p 8080:80 dockersamples/static-site\n</code></pre> <p>Explicando o que fiz, passei a porta 8080 da minha maquina, para refletir a porta 80 do cont\u00eainer.</p>"},{"location":"docker/imagem/","title":"Imagem","text":""},{"location":"docker/imagem/#o-que-e-a-imagem","title":"O que \u00e9 a imagem","text":"<p>Pense nela como um conjunto de instru\u00e7\u00f5es para criar um cont\u00eainer no docker, como um modelo. A imagem do docker, cont\u00e9m c\u00f3digo de aplicativo, bibliotecas, ferramentas, depend\u00eancias e outros arquivos necess\u00e1rios para executar um aplicativo. Quando um usu\u00e1rio executa uma imagem, ela pode se tornar uma ou v\u00e1rias inst\u00e2ncias de um cont\u00eainer. As imagens do Docker t\u00eam v\u00e1rias camadas, cada uma se originando da camada anterior, mas difere dela. As camadas aceleram as compila\u00e7\u00f5es do Docker enquanto aumentam a capacidade de reutiliza\u00e7\u00e3o e diminuem o uso do disco. Camadas de imagem tamb\u00e9m s\u00e3o arquivos somente (leitura). Depois que um cont\u00eainer \u00e9 criado, uma camada grav\u00e1vel \u00e9 adicionada sobre as imagens imut\u00e1veis, permitindo que o usu\u00e1rio fa\u00e7a altera\u00e7\u00f5es.</p> <p>Sabemos sobre o docker hub e o sobre fluxo dos cont\u00eaineres, voc\u00ea deve estar se perguntando se \u00e9 poss\u00edvel ter nossa pr\u00f3pria imagem e para responder, vamos come\u00e7ar com o comando docker images.</p> <pre><code>docker images\n</code></pre> <p>Vai ter um retorno das imagens dispon\u00edveis localmente</p> <pre><code>REPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\nubuntu                      latest    54c9d81cbb44   4 weeks ago    72.8MB\npostgres                    latest    da2cb49d7a8d   4 weeks ago    374MB\ndpage/pgadmin4              latest    e52ca07eba62   7 weeks ago    272MB\nhello-world                 latest    feb5d9fea6a5   5 months ago   13.3kB\ndockersamples/static-site   latest    f589ccde7957   5 years ago    190MB\n</code></pre> <p>Vamos entender o cabe\u00e7alho:</p> <ul> <li>REPOSITORY: \u00c9 o reposit\u00f3rio da imagem</li> <li>TAG: \u00c9 a tag, geralmente associada a vers\u00e3o</li> <li>IMAGE ID: \u00c9 o ID da imagem, n\u00e3o confunda com o ID do cont\u00eainer</li> <li>CREATED: Data da cria\u00e7\u00e3o da imagem</li> <li>SIZE: Tamanho da imagem</li> </ul> <p>Copie o IMAGE ID de uma das suas imagens locais e vamos usar o comando docker inspect.</p> <pre><code>docker inspect 54c9d81cbb44\n</code></pre> <p>A sa\u00edda retorna um JSON array por padr\u00e3o, mostrando informa\u00e7\u00f5es sobre a imagem, antes de continuamos, vamos observar o docker history.</p> <pre><code>docker history 54c9d81cbb44\n</code></pre> <p>E ela vai retorna uma lista, o que \u00e9 essa lista? S\u00e3o a camadas que quando unidas(amarrado) formam a imagem do docker que usamos. E \u00e9 poss\u00edvel usar essas camadas em outras imagens docker, o que significa isso? Quando fazemos um pull de uma imagem, ele, na verdade, est\u00e1 baixando essas camadas, e o modelo(imagem) \u00e9 o respons\u00e1vel por unificar elas para formar o resultado, ent\u00e3o se voc\u00ea baixa uma imagem nova ela aproveitar as camadas que j\u00e1 est\u00e3o dispon\u00edveis, que fazem parte do seu modelo(imagem), e baixam somente as camadas que faltam. Essa reutiliza\u00e7\u00e3o otimiza o espa\u00e7o e evitar que ocorra duplica\u00e7\u00e3o de camadas.</p>"},{"location":"docker/imagem/#criando-nossa-primeira-imagem","title":"Criando nossa primeira imagem","text":"<p>Para continuar a explica\u00e7\u00e3o da cria\u00e7\u00e3o da vamos apresentar uma nova categoria de arquivo, o dockerfile.</p>  <p>Esse processo de cria\u00e7\u00e3o de imagem \u00e9 um pouco confuso, mas leia a documenta\u00e7\u00e3o do dockerfile para facilitar na compreens\u00e3o. Nesse processo vou subir uma aplica\u00e7\u00e3o flask.</p> <ol> <li>Fa\u00e7a uma pasta para esse projeto.</li> <li>Criar um arquivo chamado app.py na pasta.</li> <li>Crie uma pasta chamada templates na pasta do projeto.</li> <li>E por \u00faltimo um index.html na pasta templates.</li> </ol> app.pyindex.html   <pre><code>from flask import Flask, render_template\nimport os\n\napp = Flask(__name__)\n\n\n@app.route('/')\ndef home():\n    return render_template('index.html')\n\n\nif __name__ == \"__main__\":\n    port = int(os.environ.get('PORT', 5000))\n    app.run(debug=True, host='0.0.0.0', port=port)\n</code></pre>   <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;Flask no Docker&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Eu amo Docker&lt;/h1&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>    <p>Se quiser mudar fazer altera\u00e7\u00e3o no html fique a vontade.</p> <p>Dentro do arquivo requirements.txt, insira:</p> <pre><code>click==8.0.3\ncolorama==0.4.4\nFlask==2.0.2\nitsdangerous==2.0.1\nJinja2==3.0.3\nMarkupSafe==2.0.1\nWerkzeug==2.0.2\ngunicorn==20.1.0\n</code></pre> <p>S\u00f3 mais uma coisa, por padr\u00e3o o flask usa a porta 5000, ent\u00e3o vamos guarda essa informa\u00e7\u00e3o para depois.</p>"},{"location":"docker/imagem/#dockerfile","title":"Dockerfile","text":"<p>Crie um arquivo chamado Dockerfile na pasta do projeto.</p> <p>Insira no Dockerfile:</p> <pre><code># Iniciando a image do Python\nFROM python:3.10-alpine3.15\n\n# Copiando o requirements para o /app da imagem\nCOPY requirements.txt /app/requirements.txt\n\n# Expor a porta 5000 da aplica\u00e7\u00e3o flask\nEXPOSE 5000\n\n# Mudando para pasta de trabalho\nWORKDIR /app\n\n# instalando as libs do requirements\nRUN pip install -r requirements.txt\n\n# Copiando o restante do conteudo no /app da imagem\nCOPY . /app\n\n# Configurando o enrtypoint para rodar o python\nENTRYPOINT [ \"python\" ]\n\nCMD [\"app.py\" ]\n</code></pre> <p>Antes de continuar com o processo, vou listar os comandos que usei no dockerfile para compreender esse processo que estamos fazendo.</p> <pre><code>FROM python:3.10-alpine3.15\n</code></pre> <p>O comando FROM fala para o docker que estou buscando uma imagem, a imagem do python, e ele fornece essa estrutura do python para usamos, como base nesse processo. Lembrando que j\u00e1 existe milhares de imagens no docker hub.</p> <pre><code>WORKDIR /app\n</code></pre> <p>Aqui estamos dizendo ao docker qual pasta usar para continuar o processo de cria\u00e7\u00e3o da imagem. O comando WORKDIR \u00e9 o local de trabalho que ele deve usar.</p> <pre><code>COPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\n</code></pre> <p>O comando COPY, diz para copiar o conte\u00fado do nosso diret\u00f3rio, no caso o requirements.txt para o diret\u00f3rio da imagem e em seguida executar o pip para instalar o requirements.txt.</p> <pre><code>COPY . /app\n</code></pre> <p>Continuando com a c\u00f3pia, agora copiamos o restante dos arquivos em nosso diret\u00f3rio de trabalho local para o diret\u00f3rio na imagem docker.</p> <pre><code>ENTRYPOINT [ \"python\" ]\n</code></pre> <p>ENTRYPOINT este \u00e9 o comando que executa a aplica\u00e7\u00e3o no cont\u00eainer.</p> <pre><code>CMD [ \"app.py\" ]\n</code></pre> <p>CMD anexa a lista de par\u00e2metros ao par\u00e2metro EntryPoint para executar o comando que executa o aplicativo.</p> <p>J\u00e1 revisamos o arquivo Dockerfile, est\u00e1 na hora de subir efetivamente a imagem, vamos usar um comando chamado docker build. Abra o terminal a partir da pasta que deve est\u00e1 com essa estrutura:</p> <pre><code>app_web_flask\n\u251c\u2500\u2500 app.py\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 templates\n    \u251c\u2500\u2500 index.html\n</code></pre> <p>Com o terminal, aberto a partir da pasta vamos executar esse comando</p> <pre><code>docker build -t univesp-github/app-flask-teste:1 .\n</code></pre> <p>Antes de executar vamos compreender, toda essa instru\u00e7\u00e3o, o -t \u00e9 para nomear a imagem. O :1 para a vers\u00e3o da nossa imagem e o ponto no final para dizer que queremos que seja feito o build a partir do diret\u00f3rio atual.</p> <p>Para alterar o nome da imagem basta substituir o univesp-github/app-flask-teste.</p> <p>Na execu\u00e7\u00e3o ele roda o Dockerfiler, baixa as camadas do python e depois criar uma camada com a estrutura que criamos, para verificar, primeiro vamos dar um docker imagens:</p> <pre><code>REPOSITORY                       TAG                IMAGE ID       CREATED              SIZE\nunivesp-github/app-flask-teste   1                  0c1acf96e7e1   About a minute ago   126MB\npython                           3.10-slim-buster   41471b406cc5   4 days ago           115MB\nubuntu                           latest             54c9d81cbb44   4 weeks ago          72.8MB\npostgres                         latest             da2cb49d7a8d   4 weeks ago          374MB\ndpage/pgadmin4                   latest             e52ca07eba62   7 weeks ago          272MB\nhello-world                      latest             feb5d9fea6a5   5 months ago         13.3kB\ndockersamples/static-site        latest             f589ccde7957   5 years ago          190MB\n</code></pre> <p>E l\u00e1 est\u00e1 nossa imagem rec\u00e9m-criada, lembrando que a porta que o flask redireciona \u00e9 a porta 5000, para manter o padr\u00e3o vamos direcionar para porta 5000 da nossa maquina.</p> <pre><code>docker run -d -p 5000:5000 univesp-github/app-flask-teste:1\n</code></pre> <p>Agora no navegador, acessar a porta 5000.</p> <pre><code>localhost:5000\n</code></pre> <p>Espero que tenha entendido o processo de cria\u00e7\u00e3o de imagem, lembrando que a documenta\u00e7\u00e3o dockerfile, as vari\u00e1veis que podemos inserir no Dockerfiler podem nos ajudar na constru\u00e7\u00e3o de imagens mais completas.</p>"},{"location":"docker/imagem/#subindo-a-imagem-para-o-docker-hub","title":"Subindo a imagem para o Docker Hub","text":"<p>Esse processo vai disponibilizar a imagem que fizemos, no reposit\u00f3rio do docker hub.</p> <ol> <li>Fa\u00e7a login em docker hub</li> <li>Fa\u00e7a login no terminal, para isso execute o comando docker login -u ap\u00f3s o -u inserir o seu nome de usu\u00e1rio.</li> <li>O terminal vai solicitar sua senha.</li> </ol> <p>Exemplo: <pre><code>docker login -u sposigor\n</code></pre></p> <p>Logado no terminal, vamos realizar o docker push. Antes de continuar com o push, devemos corrigir o nome da imagem para o docker hub aceitar. Vamos usar o docker tag.</p> <ul> <li>Nome atual da imagem: univesp-github/app-flask-teste</li> <li>Nome alterado da imagem: seu usuario/app-flask-teste</li> </ul> <p>Com esse ajuste, vamos subir a imagem no reposit\u00f3rio atrelado ao usu\u00e1rio, se voc\u00ea logou no docker hub, deve ter observado o reposit\u00f3rio da sua conta.</p> <pre><code>docker tag univesp-github/app-flask-teste:1 sposigor/app-flask-teste:1\n</code></pre> <p>N\u00e3o esque\u00e7a da vers\u00e3o, assim que a execu\u00e7\u00e3o finalizar, verifique as imagens docker images, observe a altera\u00e7\u00e3o e vamos fazer o push dessa imagem.</p> <pre><code>docker push sposigor/app-flask-teste:1\n</code></pre> <p>V\u00e1 novamente no reposit\u00f3rio e veja sua imagem dispon\u00edvel. Para visualizar o resultado do meu push app-flask-teste.</p>"},{"location":"docker/instala%C3%A7%C3%A3o/","title":"Instala\u00e7\u00e3o","text":""},{"location":"docker/instala%C3%A7%C3%A3o/#para-instalar","title":"Para Instalar \u2728","text":"<p>Vamos iniciar com a instala\u00e7\u00e3o do docker.</p> WindowsLinuxMac   <ul> <li>A documenta\u00e7\u00e3o: Instala\u00e7\u00e3o no Windows</li> <li>Vou deixa aqui um passo a passo que vai ajudar nesse processo de instala\u00e7\u00e3o no windows.</li> <li>Lembre-se de que o Docker para Windows possui restri\u00e7\u00f5es acerca de utiliza\u00e7\u00e3o e instabilidades, sendo prefer\u00edvel usar a vers\u00e3o para Linux</li> </ul>   <ul> <li>A documenta\u00e7\u00e3o para verificar as distros: Distros</li> <li>Selecione sua distribui\u00e7\u00e3o e continue com a instala\u00e7\u00e3o</li> <li>Tamb\u00e9m \u00e9 possivel usar o repositorio oficial da sua distro e op\u00e7\u00f5es como o Snap e Flatpak</li> <li>non-root user: para rodar o docker no Linux, temos que chamar o sudo todas \u00e0s vezes, para contornar isso aqui documenta\u00e7\u00e3o</li> <li>Ou basta usar o comando:</li> </ul> <pre><code>    sudo usermod -aG docker $USER\n</code></pre>   <ul> <li>A documenta\u00e7\u00e3o: Instala\u00e7\u00e3o no MAC</li> <li>Lembre-se de que o Docker para Mac possui restri\u00e7\u00f5es acerca de utiliza\u00e7\u00e3o e instabilidades, sendo prefer\u00edvel usar a vers\u00e3o para Linux</li> </ul>"},{"location":"docker/instala%C3%A7%C3%A3o/#hello-world","title":"Hello World \ud83e\ude90","text":"<p>Ap\u00f3s a instala\u00e7\u00e3o abra seu terminal e execute:</p> WindowsLinuxMac   <pre><code>    docker run hello-world\n</code></pre>   <pre><code>    docker run hello-world\n</code></pre> <ul> <li>Se o erro do docker daemon ocorrer basta acessar a documenta\u00e7\u00e3o</li> <li>Para rodar manualmente o backend, abra o terminal e use o comando:</li> </ul> <pre><code>    sudo dockerd\n</code></pre> <ul> <li>\u00c9 necess\u00e1rio manter o terminal aberto rodando o backend.]</li> <li>Para finalizar o processo no terminal, ctrl + c</li> </ul>   <pre><code>    docker run hello-world\n</code></pre>"},{"location":"docker/instala%C3%A7%C3%A3o/#entendendo-o-hello-world","title":"Entendendo o Hello World","text":"<p>Se tudo ocorreu bem no hello world na saida vai ser assim:</p> <pre><code>docker run hello-world                                                                                                    126 \u2718\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n2db29710123e: Pull complete\nDigest: sha256:97a379f4f88575512824f3b352bc03cd75e239179eea0fecc38e597b2209f49a\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n</code></pre> <p>Essa saida nos d\u00e1 informa\u00e7\u00f5es importantes, ent\u00e3o vamos analisar por partes:</p> <pre><code>Unable to find image 'hello-world:latest' locally\n</code></pre> <p>Esse trecho fala sobre uma imagem que n\u00e3o foi encontrada localmente, o proximo trecho:</p> <pre><code>latest: Pulling from library/hello-world\n2db29710123e: Pull complete\nDigest: sha256:97a379f4f88575512824f3b352bc03cd75e239179eea0fecc38e597b2209f49a\nStatus: Downloaded newer image for hello-world:latest\n</code></pre> <p>J\u00e1 nesse trecho o docker est\u00e1 baixando uma imagem de algum lugar e ap\u00f3s isso ele executa o hello world normalmente.</p>"},{"location":"docker/instala%C3%A7%C3%A3o/#afinal-o-que-aconteceu","title":"Afinal o que aconteceu?","text":"<p>Para exemplificar o docker n\u00e3o encontrou a imagem local e baixou de um reposit\u00f3rio, mas qual?</p>  Espiar a resposta <p>O reposit\u00f3rio com as imagens \u00e9 o Docker Hub.</p>"},{"location":"docker/redes/","title":"Rede","text":""},{"location":"docker/redes/#rede-bridge-ponte","title":"Rede Bridge - Ponte","text":"<p>A rede, uma rede de ponte \u00e9 um dispositivo de camada de link que encaminha o tr\u00e1fego entre os segmentos de rede. Uma ponte pode ser um dispositivo de hardware ou um dispositivo de software executado no kernel de uma m\u00e1quina host.</p> <p>No Docker, uma rede de ponte usa uma ponte de software que permite que os cont\u00eaineres conectados \u00e0 mesma rede de ponte se comuniquem, enquanto fornece isolamento de cont\u00eaineres que n\u00e3o est\u00e3o conectados a essa rede de ponte. O driver de ponte do Docker instala automaticamente as regras na m\u00e1quina host para que os cont\u00eaineres em diferentes redes de ponte n\u00e3o possam se comunicar diretamente entre si.</p> <p>As redes de ponte se aplicam a cont\u00eaineres em execu\u00e7\u00e3o no mesmo host do daemon do Docker. Para comunica\u00e7\u00e3o entre cont\u00eaineres em execu\u00e7\u00e3o em diferentes hosts do daemon do Docker, voc\u00ea pode gerenciar o roteamento no n\u00edvel do sistema operacional ou usar uma rede de sobreposi\u00e7\u00e3o.</p> <p>Quando voc\u00ea inicia o Docker, uma rede de ponte padr\u00e3o (tamb\u00e9m chamada Bridge) \u00e9 criada automaticamente e os cont\u00eaineres rec\u00e9m-iniciados se conectam a ela, a menos que especificado de outra forma. Voc\u00ea tamb\u00e9m pode criar redes de ponte personalizadas definidas pelo usu\u00e1rio. As redes de ponte definidas pelo usu\u00e1rio s\u00e3o superiores \u00e0 rede de ponte padr\u00e3o.</p>  <p>Realizaremos um teste:</p> <ol> <li>execute o docker run -it ubuntu bash, deixei o terminal aberto e abra uma nova aba no terminal</li> <li>Nesse terminal execute docker ps, copie o id do cont\u00eainer e vamos usar o docker inspect</li> </ol> <pre><code>docker inspect 8c9e5c315c0f\n</code></pre> <p>A sa\u00edda vai ser um JSON array e l\u00e1 no final vai esta as informa\u00e7\u00f5es sobre a rede:</p> <pre><code>            \"Networks\": {\n                \"bridge\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": null,\n                    \"NetworkID\": \"f2265fd0f92380df4ff74210e72ae304c76b3720cfb07bb134d97aa6ed215d70\",\n                    \"EndpointID\": \"86e57b1d3e2f1b3019246d6b50da7d88744c6bdc4acdb62f8b69236d1407185b\",\n                    \"Gateway\": \"172.17.0.1\",\n                    \"IPAddress\": \"172.17.0.2\",\n                    \"IPPrefixLen\": 16,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"MacAddress\": \"02:42:ac:11:00:02\",\n                    \"DriverOpts\": null\n                }\n</code></pre> <p>Entre todas essas configura\u00e7\u00f5es, temos o NetworkID que \u00e9 a rede do docker0, ou seja, por padr\u00e3o o docker mant\u00e9m os mesmo para todos os containers NetworkID.</p> <p>Execute o docker network ls para verificar a saida:</p> <pre><code>NETWORK ID     NAME      DRIVER    SCOPE\nf2265fd0f923   bridge    bridge    local\n95dc365e7cbf   host      host      local\n093d70e2475c   none      null      local\n</code></pre> <ul> <li>NETWORK ID: Id da rede</li> <li>NAME: Nome da rede</li> <li>DRIVER: Driver da rede</li> <li>SCOPE: \u00c9 o alcance, com as op\u00e7\u00f5es de local ou global</li> </ul> <p>Para mais informa\u00e7\u00f5es docker network.</p>"},{"location":"docker/redes/#testando-a-rede-bridge","title":"Testando a rede Bridge","text":"<p>Para isso, vamos usar dois cont\u00eaineres ubuntu:</p> Ubuntu 1Ubuntu 2   <pre><code>docker run -it ubuntu bash\n</code></pre> <ul> <li>Quando o bash iniciar:</li> </ul> <pre><code>apt-get update\n</code></pre> <ul> <li>E vamos instalar o ping</li> </ul> <pre><code>apt-get install iputils-ping -y\n</code></pre>   <pre><code>docker run -it ubuntu bash\n</code></pre> <ul> <li>Assim que o bash iniciar deixei esse ubuntu de lado</li> </ul>     <ol> <li>Precisamos do IP dos cont\u00eaineres, para isso vamos usar docker ps e em seguida o docker inspect</li> <li>Geralmente o Gateway \u00e9 172.17.0.1 e os cont\u00eaineres continuar a numera\u00e7\u00e3o. Fa\u00e7a uma anota\u00e7\u00e3o de qual \u00e9 o cont\u00eainer/IP para usar.</li> <li>Use no ubuntu 1 o comando ping e passe o IP do ubuntu 2.</li> <li>No meu caso a sa\u00edda ficou assim:</li> </ol> <pre><code>root@8c9e5c315c0f:/# ping 172.17.0.2\nPING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.\n64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.313 ms\n64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.147 ms\n64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.123 ms\n64 bytes from 172.17.0.2: icmp_seq=4 ttl=64 time=0.151 ms\n</code></pre> <p>Para finalizar a execu\u00e7\u00e3o basta aperta ctrl + c, e acabamos de usar a rede do docker para realizar a comunica\u00e7\u00e3o entre dois cont\u00eaineres.</p>"},{"location":"docker/redes/#melhorando-a-comunicacao","title":"Melhorando a comunica\u00e7\u00e3o","text":"<p>Bem, deve ter percebido que ficar procurando IP dos cont\u00eaineres e anotando cada IP n\u00e3o \u00e9 uma boa ideia, al\u00e9m de que os cont\u00eaineres podem perder a conex\u00e3o ou cair, pensando nesse problema o docker criou uma coluna NAME no cont\u00eaineres e com ela que vamos resolver todo esse problema. Vamos passa a flag --name durante o docker run para nomear nossos cont\u00eaineres e vamos criar nossa rede, \u00e9 um processo bem simples:</p> <p>Usaremos o docker network create para criar uma rede, vamos usar a flag --driver para passar o driver bridge.</p> <pre><code>docker network create --driver bridge rede-teste\n</code></pre> <p>E vamos passar nossa rede na cria\u00e7\u00e3o do cont\u00eainer:</p> PongPing   <pre><code>docker run -it --name pong --network rede-teste ubuntu sleep 30m\n</code></pre>   <pre><code>docker run -it --name ubuntu1 --network rede-teste ubuntu bash\n</code></pre> <ul> <li>Quando o bash iniciar:</li> </ul> <pre><code>apt-get update\n</code></pre> <ul> <li>E vamos instalar o ping</li> </ul> <pre><code>apt-get install iputils-ping -y\n</code></pre> <ul> <li>Execute o ping, mas agora vamos passar o nome do cont\u00eainer e n\u00e3o o IP</li> </ul> <pre><code>ping pong\n</code></pre>     <p>A diferen\u00e7a entre o padr\u00e3o do docker bridge network e o user-defined bridges \u00e9 que quando criamos nossa pr\u00f3pria rede bridge, no processo \u00e9 provido uma solu\u00e7\u00e3o DNS autom\u00e1tica nos cont\u00eaineres, possibilitando esse teste que fizemos, sem ocorrer a necessidade de mais configura\u00e7\u00f5es.</p> <p>Quando executamos o docker network ls, verificamos que temos mais duas categorias de drivers de redes dispon\u00edveis, none e host.</p> <p>A rede com o driver none basicamente n\u00e3o criar conex\u00f5es no cont\u00eainer, removendo a interface de rede:</p> <pre><code>docker run -d --network none ubuntu sleep 30m\n</code></pre> <p>De um docker inspect nesse cont\u00eainer e o resultado:</p> <pre><code>            \"Networks\": {\n                \"none\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": null,\n                    \"NetworkID\": \"093d70e2475c6ddd793426f4f6d3aa5560fd946ae182219aff56b2f406f1d290\",\n                    \"EndpointID\": \"59be8bf2afca5f2a103aa48fe86a85a676dd8ffe48a4d1101d128dcb5e9ad8b1\",\n                    \"Gateway\": \"\",\n                    \"IPAddress\": \"\",\n                    \"IPPrefixLen\": 0,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"MacAddress\": \"\",\n                    \"DriverOpts\": null\n                }\n            }\n</code></pre> <p>Os campos est\u00e3o sem informa\u00e7\u00e3o de IP ou est\u00e3o com null. Isolando completamente o cont\u00eainer.</p> <p>Enquanto o driver host, elimina o isolamento proporcionado pela estrutura do docker, ou seja, acessa a rede diretamente da host machine:</p> <pre><code>docker run -d --network host sposigor/app-flask-teste:1\n</code></pre> <p>Se lembra que no projeto o flask define a porta 5000, e para acessar no navegador era preciso passa a flag -p para redirecionar? Com o driver host da rede, basta acessar diretamente, copie e cole no navegador:</p> <pre><code>localhost:5000\n</code></pre> <p>Para lembrar ainda \u00e9 poss\u00edvel ter problemas de portas, ou seja, se houve mais alguma aplica\u00e7\u00e3o usando a porta 5000 vai ocorrer um erro.</p>"},{"location":"docker/volumes/","title":"Gerenciamento de Dados","text":"<p>As refer\u00eancias ao espa\u00e7o em disco em imagens e cont\u00eaineres do Docker podem ser confusas. \u00c9 importante distinguir entre armazenamento local e armazenamento virtual. Armazenamento local refere-se ao espa\u00e7o em disco que a camada grav\u00e1vel de um cont\u00eainer usa, enquanto o armazenamento virtual \u00e9 o espa\u00e7o em disco usado para o cont\u00eainer e a camada grav\u00e1vel. A camada somente leitura de uma imagem podem ser compartilhadas entre qualquer cont\u00eainer iniciado a partir da mesma imagem.</p> <p>Que tal verificar essa informa\u00e7\u00e3o? Antes de continuamos, vou limpar minhas imagens e meus cont\u00eaineres que est\u00e3o armazenados, como pode ver:</p> ImagensCont\u00eaineres   <pre><code>REPOSITORY                       TAG                IMAGE ID       CREATED        SIZE\nsposigor/app-flask-teste         1                  c704ddb7c517   2 hours ago    60.3MB\nunivesp-github/app-flask-teste   1                  c704ddb7c517   2 hours ago    60.3MB\npython                           3.10-alpine        a0e2910f7263   4 days ago     48.6MB\npython                           3.10-slim-buster   41471b406cc5   4 days ago     115MB\npython                           3.10               e4bdbb3cacf1   4 days ago     917MB\nubuntu                           latest             54c9d81cbb44   4 weeks ago    72.8MB\npostgres                         latest             da2cb49d7a8d   4 weeks ago    374MB\ndpage/pgadmin4                   latest             e52ca07eba62   7 weeks ago    272MB\nhello-world                      latest             feb5d9fea6a5   5 months ago   13.3kB\ndockersamples/static-site        latest             f589ccde7957   5 years ago    190MB\n</code></pre>   <pre><code>CONTAINER ID   IMAGE                              COMMAND                  CREATED        STATUS                    PORTS     NAMES\n46cab0670fc8   univesp-github/app-flask-teste:1   \"python app.py\"          2 hours ago    Exited (0) 2 hours ago              elegant_brown\nb50c4ac105c3   f32698e186b5                       \"python app.py\"          2 hours ago    Exited (0) 2 hours ago              boring_knuth\n40ea6243bbc3   64160aa2a8dc                       \"python app.py\"          2 hours ago    Exited (0) 2 hours ago              unruffled_roentgen\n7ef49050e8df   1647b4c47b68                       \"python app.py\"          2 hours ago    Exited (0) 2 hours ago              nostalgic_dirac\n19bf7322160d   ubuntu                             \"sleep 5m\"               20 hours ago   Exited (0) 19 hours ago             wizardly_shtern\nbb1bf42f3fc3   ubuntu                             \"sleep 2m\"               20 hours ago   Exited (0) 20 hours ago             frosty_ishizaka\n65adb2cb2cd5   ubuntu                             \"bash\"                   20 hours ago   Exited (0) 20 hours ago             competent_thompson\nf690741c0bc7   ubuntu                             \"bash\"                   20 hours ago   Exited (0) 20 hours ago             infallible_williams\n6914d474838d   hello-world                        \"/hello\"                 23 hours ago   Exited (0) 20 hours ago             upbeat_lichterman\n</code></pre>    <p>Vou usar um comando um pouco diferente o docker system prune</p> <pre><code>docker system prune -af\n</code></pre> <p>Ele remove todas as imagens, cont\u00eaineres, volumes e o que mais tiver armazenado no docker, passei o par\u00e2metro -a de all e -f de force. Vou lista mais alguns comandos de remo\u00e7\u00e3o que podem ser uteis:</p> <ul> <li>docker container rm: Remove somente com o CONTAINER ID.</li> <li>docker container prune: Remove todos os containeres</li> <li>docker image rm: Remove somente a imagem com o IMAGE ID</li> <li>docker image prune: Remove todas as imagens</li> <li>docker volume rm: Remove o volume com o VOLUME NAME</li> <li>docker volume prune: Remove todos os volumes</li> <li>docker network rm: Remove o network com o NETWORK ID</li> <li>docker network prune: Remove todos os network</li> </ul>  <p>Para come\u00e7ar a entender melhor o volume, armazenamento local e armazenamento virtual, recorremos ao nosso querido ubuntu, dessa vez, vamos chamar ele duas vezes:</p> PrimeiraSegunda   <pre><code>docker run -it ubuntu bash\n</code></pre> <ul> <li>Assim que subir o bash execute o apt-get update</li> <li>Escreva exit para finalizar</li> </ul>   <pre><code>docker run -it ubuntu bash\n</code></pre> <ul> <li>Nesse caso apenas e escreva exit para finalizar</li> </ul>    <p>Com o comando docker ps -as a sa\u00edda:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND   CREATED          STATUS                            PORTS     NAMES            SIZE\na4928a7ff9a6   ubuntu    \"bash\"    47 seconds ago   Exited (0) 44 seconds ago                   funny_thompson   5B (virtual 72.8MB)\n0516c2a0ac00   ubuntu    \"bash\"    4 minutes ago    Exited (130) About a minute ago             fervent_turing   34MB (virtual 107MB)\n</code></pre> <p>Temos uma nova coluna que \u00e9 a SIZE e ela possui duas informa\u00e7\u00f5es o armazenamento local e armazenamento virtual, usando o ubuntu como exemplo, em um apenas subimos a imagem sendo o primeiro na lista e o segundo fizemos o apt-get update.</p> <ul> <li>Armazenamento Local: \u00c9 a camada da qual temos acesso a gravar.</li> <li>Armazenamento Virtual: \u00c9 o tamanho da imagem sem qualquer grava\u00e7\u00e3o.</li> </ul> <p>Quando fazemos grava\u00e7\u00f5es, esses dados reflete tanto no armazenamento local quanto no armazenamento virtual.</p>"},{"location":"docker/volumes/#persistencia-de-dados","title":"Persist\u00eancia de dados","text":"<p>Podemos querer que os dados da nossa aplica\u00e7\u00e3o sejam persistentes, porque assim garantimos que ela esteja distribu\u00edda e dispon\u00edvel se precisarmos consult\u00e1-la. Por\u00e9m, se escrevermos os dados nos cont\u00eaineres, por padr\u00e3o eles n\u00e3o ficar\u00e3o armazenados nesta camada, criada para ser descart\u00e1vel. Existem tr\u00eas possibilidades para contornar esta situa\u00e7\u00e3o com o Docker.</p>"},{"location":"docker/volumes/#bind-mounts","title":"Bind Mounts","text":"<p>Os bind mounts est\u00e3o dispon\u00edveis no Docker desde seus primeiros dias para a persist\u00eancia de dados. Os bind mounts montar\u00e3o um arquivo ou diret\u00f3rio em seu cont\u00eainer a partir de sua m\u00e1quina host, que voc\u00ea pode referenciar atrav\u00e9s de seu caminho absoluto.</p> <p>Para usar o bind mounts, o arquivo ou diret\u00f3rio n\u00e3o precisa j\u00e1 existir em seu host do Docker. Se n\u00e3o existir, ser\u00e1 criado sob demanda. As montagens de liga\u00e7\u00e3o dependem do sistema de arquivos da m\u00e1quina host ter uma estrutura de diret\u00f3rio espec\u00edfica dispon\u00edvel. Voc\u00ea deve criar explicitamente um caminho para o arquivo ou pasta para colocar o armazenamento.</p> <p>Outra informa\u00e7\u00e3o importante sobre bind mounts \u00e9 que elas d\u00e3o acesso a arquivos confidenciais. Conforme os documentos do Docker, voc\u00ea pode alterar o sistema de arquivos do host atrav\u00e9s de processos executados em um cont\u00eainer. Isso inclui criar, modificar e excluir arquivos e diret\u00f3rios do sistema, que podem ter implica\u00e7\u00f5es de seguran\u00e7a bastante graves. Pode at\u00e9 impactar processos n\u00e3o-Docker.</p> <p>Para come\u00e7ar a usar o bind mounts, precisamos criar uma pasta na nossa maquina, vou cria na \u00e1rea de trabalho e nome\u00e1-la de bind. Copie a localiza\u00e7\u00e3o que vamos usar.</p> -v--mount   <pre><code>docker run -v /home/sposigor/bind:/teste -it ubuntu bash\n</code></pre>   <pre><code>docker run -it --mount type=bind,src=/home/sposigor/bind,dst=/teste ubuntu bash\n</code></pre>    <p>A docker recomenda usar --mount e n\u00e3o o -v</p> <p>Se tiver d\u00favidas, na documenta\u00e7\u00e3o do docker run tem mais informa\u00e7\u00f5es.</p> <p>Para realizar o teste:</p> <ol> <li>Fa\u00e7a o docker run, seja em -v ou --mount.</li> <li>No bash do cont\u00eainer, de um ls</li> <li>Veja a pasta teste criada e vamos entrar nela com o cd teste/</li> <li>Na pasta touch teste1.txt e escreva exit para sair</li> <li>V\u00e1 na sua pasta que voc\u00ea criou na m\u00e1quina local e abra ela e o arquivo teste1.txt vai est\u00e1 l\u00e1.</li> <li>Repita o processo de 1 \u00e1 3, com um novo cont\u00eainer e o arquivo criado no cont\u00eainer anterior persiste l\u00e1.</li> </ol>"},{"location":"docker/volumes/#volumes","title":"Volumes","text":"<p>Os volumes s\u00e3o um \u00f3timo mecanismo para adicionar uma camada de persist\u00eancia de dados em seus cont\u00eaineres do Docker, especialmente para uma situa\u00e7\u00e3o em que voc\u00ea precisa persistir dados ap\u00f3s desligar seus cont\u00eaineres.</p> <p>Volumes, portanto \u00e9 a solu\u00e7\u00e3o mais recomendada pelo docker para usar em ambientes de produ\u00e7\u00e3o e afins.</p> <p>O docker volume ls observa os volumes dispon\u00edveis:</p> <pre><code>DRIVER    VOLUME NAME\n</code></pre> <ul> <li>DRIVER: \u00c9 o volume do drive</li> <li>VOLUME NAME: \u00c9 o nome do volume</li> </ul> <p>Criar um volume novo docker volume create e o nome do volume</p> <pre><code>docker volume create testando\n</code></pre> <p>Vamos colocar em produ\u00e7\u00e3o:</p> -v--mount   <pre><code>docker run -v testando:/teste -it ubuntu bash\n</code></pre>   <pre><code>docker run -t -i --mount type=bind,src=testando,dst=/teste ubuntu bash\n</code></pre>    <ol> <li>Fa\u00e7a o docker run, seja em -v ou --mount.</li> <li>No bash do container, de um ls</li> <li>Veja a pasta teste que foi criada e vamos entrar nela com o cd teste/</li> <li>Dentro da pasta touch teste1.txt e escreva exit para sair</li> <li>Repita o processo de 1 \u00e1 3, com um novo container e o arquivo criado no container anterios persiste l\u00e1.</li> </ol> <p>Deve ter surgido uma duvida, aonde est\u00e1 esse arquivo que criei?</p> <p>Para isso vamos para o terminal:</p> <ol> <li>No terminal sudo su</li> <li>cd /var/lib/docker/volumes</li> <li>ls para verificar os arquivos</li> <li>O volume criado vai est\u00e1 aqui, cd testando</li> <li>ls vai ter uma pasta _data</li> <li>cd _data e em seguida ls</li> <li>Encontrou o arquivo que foi criado</li> </ol> <p>Os volumes do Docker s\u00e3o completamente manipulados pelo pr\u00f3prio Docker e, portanto, independentes da estrutura de diret\u00f3rios e do sistema operacional da m\u00e1quina host. Quando voc\u00ea usa um volume, um novo diret\u00f3rio \u00e9 criado no diret\u00f3rio de armazenamento do Docker na m\u00e1quina host e o Docker gerencia o conte\u00fado desse diret\u00f3rio.</p> <p>Por ultimo podemos passar diretamente do docker run o volume que ele se encarrega de criar</p> -v--mount   <pre><code>docker run -v volume_novo:/teste -it ubuntu bash\n</code></pre>   <pre><code>docker run -t -i --mount type=bind,src=volume_novo,dst=/teste ubuntu bash\n</code></pre>    <p>docker volume ls e a sa\u00edda:</p> <pre><code>DRIVER    VOLUME NAME\nlocal     testando\nlocal     volume_novo\n</code></pre>"},{"location":"docker/volumes/#tmpfs-mounts","title":"tmpfs mounts","text":"<p>O tmpfs mounts \u00e9 para espa\u00e7os de armazenamento pequenos e ef\u00eameros que s\u00f3 podem ser usados \u200b\u200bpor um \u00fanico cont\u00eainer, existe o sistema de arquivos tmpfs. Ele \u00e9 apoiado apenas pelo armazenamento RAM no sistema host.</p> <p>Essa funcionalidade s\u00f3 est\u00e1 dispon\u00edvel se voc\u00ea estiver executando o Docker no Linux.</p> <p>Seguindo a ordem para criar um tmpfs \u00e9 mais simples, j\u00e1 possui a pr\u00f3pria flag --tmpfs:</p> <p><pre><code>docker run -it --tmpfs=/app ubuntu bash\n</code></pre> </p> <p>Perceba que a pasta app que criamos usando o --tmpfs, est\u00e1 com um fundo verde, assim como o tmp, ent\u00e3o significa que ela \u00e9 tempor\u00e1ria, quando o cont\u00eainer for finalizado, os dados ali s\u00e3o apagados.</p> <p>Ent\u00e3o qual \u00e9 a utilidade de usar o tmpfs? \u00c9 que ela armazena os arquivos na mem\u00f3ria do host e n\u00e3o na camada de escrita do cont\u00eaineres r/w, ou seja, se existem dados sens\u00edveis para serem armazenados no processo e por quest\u00f5es de segura n\u00e3o deseja escrever na camada de escrita do cont\u00eainer r/w, o tmpfs \u00e9 recomendando nessa situa\u00e7\u00e3o.</p> <p>Para mais uma aplica\u00e7\u00e3o podemos usar como par\u00e2metro do --mount:</p> <pre><code>docker run -it --mount type=tmpfs,dst=/teste ubuntu bash\n</code></pre>"},{"location":"docker%20swarm/docker%20stack/","title":"Docker Stack","text":"<p>A Docker Stack \u00e9 um comando do Docker para gerenciar as pilhas do Docker. Podemos usar este comando para implantar uma nova pilha ou atualizar uma existente, listar pilhas, listar as tarefas na pilha, remover uma ou mais pilhas e listar os servi\u00e7os na pilha. Devemos habilitar o modo swarm para executar este comando, pois s\u00f3 podemos implantar pilhas no modo swarm do Docker e ele est\u00e1 inclu\u00eddo no mecanismo do Docker, para n\u00e3o precisarmos instalar nenhum pacote adicional, apenas precisamos habilit\u00e1-lo, pois est\u00e1 desabilitado por predefini\u00e7\u00e3o.</p> <p>A proposta aqui \u00e9 roda uma infraestrutura composto por v\u00e1rios servi\u00e7os e usando o docker swarm.</p> <p>No docker hub. tem uma imagem bem popular para exemplificar todo esse processo, aqui voc\u00ea acessa o material referente a essa aplica\u00e7\u00e3o.</p> <p>Antes de continuamos, vou reiniciar a p\u00e1gina do docker play e selecionar a configura\u00e7\u00e3o de 3 manager and 2 workes.</p> <p>Vamos criar um docker-compose.yml:</p> <p>Copie o Conte\u00fado abaixo:</p> <pre><code>version: \"3\"\nservices:\n\n  redis:\n    image: redis:alpine\n    networks:\n      - frontend\n    deploy:\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n\n  db:\n    image: postgres:9.4\n    volumes:\n      - db-data:/var/lib/postgresql/data\n    networks:\n      - backend\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n    environment:\n        POSTGRES_HOST_AUTH_METHOD: trust\n\n  vote:\n    image: dockersamples/examplevotingapp_vote:before\n    ports:\n      - 5000:80\n    networks:\n      - frontend\n    depends_on:\n      - redis\n    deploy:\n      replicas: 2\n      restart_policy:\n        condition: on-failure\n\n  result:\n    image: dockersamples/examplevotingapp_result:before\n    ports:\n      - 5001:80\n    networks:\n      - backend\n    depends_on:\n      - db\n    deploy:\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n\n  worker:\n    image: dockersamples/examplevotingapp_worker\n    networks:\n      - frontend\n      - backend\n    depends_on:\n      - db\n      - redis\n    deploy:\n      mode: replicated\n      replicas: 1\n      labels: [APP=VOTING]\n      restart_policy:\n        condition: on-failure\n      placement:\n        constraints: [node.role == worker]\n\n  visualizer:\n    image: dockersamples/visualizer:stable\n    ports:\n      - 8080:8080\n    stop_grace_period: 1m30s\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\nnetworks:\n  frontend:\n  backend:\n\nvolumes:\n  db-data:\n</code></pre> <p>V\u00e1 no terminal e digite:</p> <pre><code>cat &gt; docker-compose.yml\n</code></pre> <p>De um enter, depois um shift + insert para colar o conteudo copiado.</p> <p>De um enter para abrir um espa\u00e7o no final e ctrl + d, para sair da edi\u00e7\u00e3o do arquivo.</p> <p>Agora com o .yml certinho, vamos usar o docker stack deploy para subir toda essa aplica\u00e7\u00e3o.</p> ExecuteSaida   <pre><code>docker stack deploy --compose-file docker-compose.yml vote\n</code></pre>   <pre><code>Creating network vote_backend\nCreating network vote_frontend\nCreating network vote_default\nCreating service vote_db\nCreating service vote_vote\nCreating service vote_result\nCreating service vote_worker\nCreating service vote_visualizer\nCreating service vote_redis\n</code></pre>     <p>Vamos verificar agora com o docker stack ls.</p> docker stack lsdocker service ls   <pre><code>docker stack ls\n</code></pre> <p>Saida:</p> <pre><code>NAME      SERVICES   ORCHESTRATOR\nvote      6          Swarm\n</code></pre>   <pre><code>docker service ls\n</code></pre> <p>Saida</p> <pre><code>ID             NAME              MODE         REPLICAS   IMAGE                                          PORTS\na71bfc34hnw6   vote_db           replicated   1/1        postgres:9.4                                   \n3c11wqgvyvhe   vote_redis        replicated   1/1        redis:alpine                                   \n5bghvabs9joq   vote_result       replicated   1/1        dockersamples/examplevotingapp_result:before   *:5001-&gt;80/tcp\n5pn9qep3hqji   vote_visualizer   replicated   1/1        dockersamples/visualizer:stable                *:8080-&gt;8080/tcp\nu8pgdms926le   vote_vote         replicated   2/2        dockersamples/examplevotingapp_vote:before     *:5000-&gt;80/tcp\nvwlu9rb7yycs   vote_worker       replicated   1/1        dockersamples/examplevotingapp_worker:latest\n</code></pre>     <p>E com isso subimos completamente a aplica\u00e7\u00e3o, se por acaso voc\u00ea executar muito r\u00e1pido as instru\u00e7\u00f5es, talvez veja as r\u00e9plicas subindo.</p> <p>Com isso vamos abrir as portas para verificar o resultado.</p> <ul> <li>8080: Para visualizar o dashboard</li> <li>5000: Para interagir com a vota\u00e7\u00e3o</li> <li>5001: Para visualizar o resultado da vota\u00e7\u00e3o</li> </ul> <p>No dashboard \u00e9 poss\u00edvel ver quais n\u00f3s, est\u00e3o rodando os cont\u00eaineres.</p> <p>Com isso finalizamos esse conte\u00fado sobre docker swarm, espero que tenha gostado e aprendido.</p>"},{"location":"docker%20swarm/ingress/","title":"Ingress","text":"<p>Que tal damos uma olhada na rede?</p> ExecuteExecute em outro n\u00f3   <pre><code>docker network ls\n</code></pre> <p>Saida:</p> <pre><code>NETWORK ID     NAME              DRIVER    SCOPE\nb61db738d8e1   bridge            bridge    local\na1dc5572a15b   docker_gwbridge   bridge    local\n0310735b3f9c   host              host      local\nk6lk5evd8ifb   ingress           overlay   swarm\n8e6d45cb021f   none              null      local\n</code></pre>   <pre><code>docker network ls\n</code></pre> <p>Saida:</p> <pre><code>NETWORK ID     NAME              DRIVER    SCOPE\n97f34fa9fd5d   bridge            bridge    local\n97e970ad94f3   docker_gwbridge   bridge    local\ne8c6e013648a   host              host      local\nk6lk5evd8ifb   ingress           overlay   swarm\n20f0f809422d   none              null      local\n</code></pre>     <p>Com isso podemos percebe uma coisa muito interessante que \u00e9 que independente do N\u00f3, seja ele gerente ou trabalhador, a rede com nome de ingress \u00e9 a mesma em qualquer n\u00f3.</p> <p>O que significa que todos as maquinas dentro do swarm tem essa rede em comum, vamos nos lembrar do routing mesh ent\u00e3o \u00e9 basicamente isso que estamos vendo, o driver overlay \u00e9 o responsavel por fazer boa parte da m\u00e1gica acontecer. Resolvendo todas as requisi\u00e7\u00f5es antes de chegarem nos n\u00f3s. E ela tem mais uma caracter\u00edstica importante, \u00e9 que est\u00e1 criptografada, garantindo seguran\u00e7a para o swarm.</p>"},{"location":"docker%20swarm/ingress/#service-discovery","title":"Service Discovery","text":"<p>O Docker Engine possui um servidor DNS embutido nele, usado por cont\u00eaineres quando o Docker n\u00e3o est\u00e1 sendo executado no modo Swarm e para tarefas, quando est\u00e1. Ele fornece resolu\u00e7\u00e3o de nomes para todos os cont\u00eaineres que est\u00e3o no host em redes bridge, overlay. Cada cont\u00eainer encaminha suas consultas para o Docker Engine, que por sua vez verifica se o cont\u00eainer ou servi\u00e7o est\u00e1 na mesma rede que o cont\u00eainer que enviou a solicita\u00e7\u00e3o em primeiro lugar. Se for, ele pesquisa o endere\u00e7o IP (ou IP virtual) que corresponde a um cont\u00eainer, uma tarefa ou um nome de servi\u00e7o em seu armazenamento de chave-valor interno e o retorna ao cont\u00eainer que enviou a solicita\u00e7\u00e3o. Bem legal, hein?</p> <p>Como eu disse antes, o Docker Engine s\u00f3 retornar\u00e1 um endere\u00e7o IP se o recurso correspondente estiver na mesma rede que o cont\u00eainer que gerou a solicita\u00e7\u00e3o. O que tamb\u00e9m \u00e9 legal nisso \u00e9 que os Docker Hosts armazenam apenas as entradas DNS que pertencem \u00e0s redes nas quais o n\u00f3 possui cont\u00eaineres ou tarefas. Isso significa que eles n\u00e3o armazenar\u00e3o informa\u00e7\u00f5es que sejam praticamente irrelevantes para eles ou que outros cont\u00eaineres n\u00e3o precisem saber.</p> <p>Traduzindo o texto, se lembra do teste que fizemos do ping pong? Basicamente conseguimos fazer chamadas diretas pelo nome dos servi\u00e7os de outros servi\u00e7os usando uma rede overlay, mas n\u00e3o h\u00e1, por padr\u00e3o, e sim uma que vamos criar.</p>"},{"location":"docker%20swarm/ingress/#criando-nossa-rede-overlay","title":"Criando nossa rede overlay","text":"<p>Para acessar a documenta\u00e7\u00e3o overlay.</p> <p>Para criar uma rede network:</p> <pre><code>docker network create -d overlay minha_rede_overlay\n</code></pre> <p>Para verificar:</p> GerenteTrabalhadores   <pre><code>docker network ls\n</code></pre> <p>Saida:</p> <pre><code>NETWORK ID     NAME                 DRIVER    SCOPE\na0d052078dbe   bridge               bridge    local\n561b12e09e46   docker_gwbridge      bridge    local\n4881488b5453   host                 host      local\nk6lk5evd8ifb   ingress              overlay   swarm\nwpmvutc3phrd   minha_rede_overlay   overlay   swarm\n255b8ee96504   none                 null      local\n</code></pre>   <pre><code>NETWORK ID     NAME              DRIVER    SCOPE\n97f34fa9fd5d   bridge            bridge    local\n97e970ad94f3   docker_gwbridge   bridge    local\ne8c6e013648a   host              host      local\nk6lk5evd8ifb   ingress           overlay   swarm\n20f0f809422d   none              null      local\n</code></pre>     <p>Como podem ver, quando executamos o docker network ls nos gerentes conseguimos ver a rede que criamos, e nos trabalhadores n\u00e3o. Porque isso acontece? Bem, por de tr\u00e2s dos panos ali, todos os n\u00f3 j\u00e1 sabem da existencia dessa rede, por\u00e9m os n\u00f3s trabalhadores s\u00e3o v\u00e3o listar essa rede, se um servi\u00e7o usar ela explicitamente.</p> <p>Vamos usar uma distro chamada alpine, que \u00e9 um linux super leve que vai servi para nossa ilustra\u00e7\u00e3o do processo.</p> <pre><code>docker service create --name teste_discovery_search --network minha_rede_overlay --replicas 2 alpine sleep 50m\n</code></pre> <p>Verifique aonde o servi\u00e7o est\u00e1 rodando com o docker service ps e passe o ID do servi\u00e7o:</p> <pre><code>ID             NAME                       IMAGE           NODE       DESIRED STATE   CURRENT STATE                ERROR     PORTS\nwuj7ee913pop   teste_discovery_search.1   alpine:latest   manager3   Running         Running about a minute ago             \n178njdwf3bjx   teste_discovery_search.1   alpine:latest   manager1   Running         Running about a minute ago\n</code></pre> <p>No meu caso, est\u00e1 rodando no manager3 e no manager1, para acessa o \"bash\" do alpine a gente precisa passar o parametro sh, se por acaso cair numa maquina trabalhadora, voc\u00ea vai conseguir visualizar a rede que criamos.</p>  <p>Vamos acessar o terminal do alpine:</p> manager1manager3   <p>Vamos pega o nome do servi\u00e7o na rede que criamo no manager1</p> <pre><code>docker network inspect minha_rede_overlay\n</code></pre> <p>Saida:</p> <pre><code>    \"Containers\": {\n        \"c25892b0685a6a5c3ea0337c58ce694d4a6da01712988c9cdb5fbad4ef24a656\": {\n            \"Name\": \"teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh\",\n            \"EndpointID\": \"41c6b2419c382238425e7afc0882ecdf2ef2038003d5d80bfb7322037982e840\",\n            \"MacAddress\": \"02:42:0a:00:01:03\",\n            \"IPv4Address\": \"10.0.1.3/24\",\n            \"IPv6Address\": \"\"\n        },\n</code></pre> <p>Na saida JSON vai ter esse trecho, e precisamos do Name para conseguir realizar essa comunica\u00e7\u00e3o. No meu caso teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh</p> <p>Com o nome do servi\u00e7o vamos usar o exec para acessar o terminal do alpine</p> <pre><code>docker exec -it teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh sh\n</code></pre>   <p>Vamos executar o mesmo procedimento para pegar o nome do servi\u00e7o no manager3</p> <pre><code>docker network inspect minha_rede_overlay\n</code></pre> <p>Saida:</p> <pre><code>    \"Containers\": {\n        \"ccabd4e0efd2862300cbb27ffebf8ca6718ab6d0163633bdb0a388766ebe937a\": {\n            \"Name\": \"teste_discovery_search.1.wuj7ee913popf8wsysemruslc\",\n            \"EndpointID\": \"e518f6eeab368d6ffabd439a4b902dccd3d5996bcbabd1f79c1cb2eb56b9be42\",\n            \"MacAddress\": \"02:42:0a:00:01:04\",\n            \"IPv4Address\": \"10.0.1.4/24\",\n            \"IPv6Address\": \"\"\n        },\n</code></pre> <p>Na sa\u00edda JSON vai ter esse trecho, e precisamos do Name para conseguir realizar essa comunica\u00e7\u00e3o. No meu caso teste_discovery_search.1.wuj7ee913popf8wsysemruslc</p> <p>Com o nome do servi\u00e7o vamos usar o exec para acessar o terminal do alpine</p> <pre><code>docker exec -it teste_discovery_search.1.wuj7ee913popf8wsysemruslc sh\n</code></pre>     <p>Agora que estamos com o terminal de ambos os servi\u00e7os, vamos usar o ping pong para realizar uma chamada.</p> manager1manager3   <p>Use o nome do servi\u00e7o do manager3</p> <pre><code>ping teste_discovery_search.1.wuj7ee913popf8wsysemruslc\n</code></pre> <p>Saida:</p> <pre><code>PING teste_discovery_search.1.wuj7ee913popf8wsysemruslc (10.0.1.4): 56 data bytes\n64 bytes from 10.0.1.4: seq=0 ttl=64 time=0.083 ms\n64 bytes from 10.0.1.4: seq=1 ttl=64 time=0.107 ms\n64 bytes from 10.0.1.4: seq=2 ttl=64 time=0.087 ms\n64 bytes from 10.0.1.4: seq=3 ttl=64 time=0.134 ms\n64 bytes from 10.0.1.4: seq=4 ttl=64 time=0.156 ms\n64 bytes from 10.0.1.4: seq=5 ttl=64 time=0.075 ms\n</code></pre>   <p>Use o nome do servi\u00e7o do manager1</p> <pre><code>ping teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh\n</code></pre> <p>Saida:</p> <pre><code>PING teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh (10.0.1.3): 56 data bytes\n64 bytes from 10.0.1.3: seq=0 ttl=64 time=0.060 ms\n64 bytes from 10.0.1.3: seq=1 ttl=64 time=0.082 ms\n64 bytes from 10.0.1.3: seq=2 ttl=64 time=0.079 ms\n64 bytes from 10.0.1.3: seq=3 ttl=64 time=0.273 ms\n64 bytes from 10.0.1.3: seq=4 ttl=64 time=0.097 ms\n64 bytes from 10.0.1.3: seq=5 ttl=64 time=0.203 ms\n</code></pre>    <p>Ent\u00e3o nosso teste foi realizado, de m\u00e1quinas diferentes, acessamos cont\u00eaineres e com apenas o nome do servi\u00e7o, o docker se encarregou de encontrar.</p>"},{"location":"docker%20swarm/iniciando%20o%20swarm/","title":"Iniciando o Swarm","text":"<p>Bem, chegamos a um ponto importante aqui que \u00e9 a cria\u00e7\u00e3o de uma rede de computadores, mas criar uma rede n\u00e3o \u00e9 t\u00e3o simples, mesmo que fizesse o passo a passo, vai levar muito tempo com muitas vari\u00e1veis que podem dar errado, ent\u00e3o para conseguimos usar o docker swarm vamos usar o Play With Docker</p> <p>Vai facilitar demais e vamos conseguir subir imagens normalmente, ele j\u00e1 vem com o docker instalado.</p> <ol> <li>Aperte Start</li> <li>Ele vai solicitar seu login do docker.</li> <li>Em seguida se ele n\u00e3o redirecionar basta aperta Start novamente.</li> <li>Assim que ele abrir a p\u00e1gina, n\u00e3o tenha nenhuma aplica\u00e7\u00e3o no momento, basta ficar tentando at\u00e9 conseguir acessar.</li> <li>Quando a aplica\u00e7\u00e3o subir vamos ter essa pagina:</li> </ol>  <p>Vamos adicionar 5 instancias, basta pressionar o +ADD NEW INSTANCE que ele vai gerar automaticamente os computadores:</p>"},{"location":"docker%20swarm/iniciando%20o%20swarm/#primeiro-gerente","title":"Primeiro Gerente","text":"<p>Para iniciar o swarm, usaremos o docker swarm init.</p> <pre><code>docker swarm init\n</code></pre> <p>Escolha umas das m\u00e1quinas para roda o comando e observe a sa\u00edda:</p>  <p>Nesse erro, ele nos diz que n\u00e3o anunciamos o IP que devemos usar, o IP que as maquinas est\u00e3o na rede, para isso vamos usar a vari\u00e1vel que ele solicita  --advertise-addr e passar o IP, \u00e9 importante sempre passar essa flag e o IP de umas das m\u00e1quinas, para fixamos a m\u00e1quina que criou o swarm.</p> <pre><code>docker swarm init --advertise-addr 192.168.0.13\n</code></pre> <p>E iniciamos o nosso swarm:</p>  <p>A maquina que inicializa o comando, automaticamente se torna l\u00edder, ou seja, j\u00e1 temos nosso primeiro gerente.</p> <p>Nesse momento, ele vai passar dois comandos para que possamos adicionar novos gerentes ou novos trabalhadores:</p> <ol> <li>Gerentes: docker swarm join com a flag -token manager</li> <li>Trabalhadores: docker swarm join mais a flag --token SWMTKN-1-317lss7q287qzea30syrpzsnb7dlzcp5xqppqomkz0dpwlf0jv-3qzxl99iwvxo1b7zr2463xgdf 192.168.0.13:2377 passando o token e o IP para os novos computadores se unirem ao swarm.</li> </ol> <p>Antes de continuamos, vamos imaginar que voc\u00ea tenha deixado o ambiente rodando l\u00e1 e ficou um ano sem acessar, ai um belo dia voc\u00ea volta contudo n\u00e3o lembra qual maquina \u00e9 a que iniciou o swarm ou se ela est\u00e1 no swarm, para isso vamos usar o docker info.</p> <pre><code>docker info\n</code></pre> <p>E a sa\u00edda vai ser \u00b1 essa aqui:</p>  Saida <pre><code>Client:\n Context:    default\n Debug Mode: false\n Plugins:\n  app: Docker App (Docker Inc., v0.9.1-beta3)\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 0\n Server Version: 20.10.0\n Storage Driver: overlay2\n  Backing Filesystem: xfs\n  Supports d_type: true\n  Native Overlay Diff: true\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: active\n  NodeID: tv5szgmtbqunlbhkv4jqqdjrz\n  Is Manager: true\n  ClusterID: qxz7jhjgyd5ldoy7o1vcutbfn\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.0.13\n  Manager Addresses:\n   192.168.0.13:2377\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 269548fa27e0089a8b8278fc4fc781d7f65a939b\n runc version: ff819c7e9184c13b7c2607fe6c30ae19403a7aff\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: default\n Kernel Version: 4.4.0-210-generic\n Operating System: Alpine Linux v3.12 (containerized)\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 31.42GiB\n Name: node1\n ID: MLNO:DP7B:WIP5:4LLV:IUGJ:B5DP:KMRL:PZ2Z:A7Y3:7XLT:Z4NJ:UULR\n Docker Root Dir: /var/lib/docker\n Debug Mode: true\n  File Descriptors: 38\n  Goroutines: 155\n  System Time: 2022-03-05T23:02:27.17700525Z\n  EventsListeners: 0\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: true\n Insecure Registries:\n  127.0.0.1\n  127.0.0.0/8\n Live Restore Enabled: false\n Product License: Community Engine\n</code></pre>  <p>E nessa lista todas temos o Swarm: active, significa que o swarm est\u00e1 ativo e o Is Manager: true que diz que a maquina \u00e9 um gerente.</p>"},{"location":"docker%20swarm/iniciando%20o%20swarm/#primeiro-trabalhador","title":"Primeiro Trabalhador","text":"<p>Agora que temos nosso swarm, vamos inserir mais computadores, vamos usar o docker swarm join</p> <p>Nas outras instancias que voc\u00ea tiver, basta copia o comando com shift+insert que o docker fornece, no meu caso \u00e9:</p> <pre><code>docker swarm join --token SWMTKN-1-317lss7q287qzea30syrpzsnb7dlzcp5xqppqomkz0dpwlf0jv-3qzxl99iwvxo1b7zr2463xgdf 192.168.0.13:2377\n</code></pre> <p>E a saida:</p> <pre><code>This node joined a swarm as a worker.\n</code></pre> <p>Agora fa\u00e7a isso em todas as inst\u00e2ncias que voc\u00ea criou, recomendo n\u00e3o passar de 5 por quest\u00e3o de estabilidade e desempenho do servi\u00e7o.</p> <p>Se voc\u00ea perde o token, n\u00e3o precisa anotar nenhum comando, basta no ir ao terminal do gerente e pergunta para ele, usando o comando docker swarm join-token e passe quem voc\u00ea quer saber, worker ou maneger</p> <pre><code>docker swarm join-token worker\n</code></pre>"},{"location":"docker%20swarm/iniciando%20o%20swarm/#no-nodes","title":"N\u00f3 - Nodes","text":"<p>Precisamos ver de uma forma geral nosso swarm, para isso vamos usar o comando docker node ls, no gerente, se usar no trabalhador, vai d\u00e1 um erro.</p> <pre><code>ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\ntv5szgmtbqunlbhkv4jqqdjrz *   node1      Ready     Active         Leader           20.10.0\nsomcfzr1i52h02s32g8162jv5     node2      Ready     Active                          20.10.0\nm3ug6hoe28zz1dap3ejhqug6n     node3      Ready     Active                          20.10.0\nle62bq94ze1eagudy8dtx12ed     node4      Ready     Active                          20.10.0\naedfo2gsp1ky5cbzb7vmj1eqy     node5      Ready     Active                          20.10.0\n</code></pre> <p>Explicando:</p> <ul> <li>ID: \u00c9 o ID do n\u00f3, e o n\u00f3 com * \u00e9 o n\u00f3 que iniciou o swarm.</li> <li>HOSTNAME: Nome das m\u00e1quinas</li> <li>STATUS: Que est\u00e1 funcionando normalmente</li> <li>AVAILABILITY: Que vai rodar cont\u00eaineres normalmente</li> <li>MANAGER STATUS: Quem s\u00e3o os gerentes e o l\u00edder, todos os, n\u00f3 sem informa\u00e7\u00e3o, vazio, s\u00e3o workes para o docker.</li> <li>ENGINE VERSION: Engine \u00e9 a vers\u00e3o do docker.</li> </ul> <p>Para exemplificar, vamos roda o comando em um trabalhador:</p> <pre><code>Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.\n</code></pre> <p>Ent\u00e3o todos os comandos de altera\u00e7\u00e3o ou visualiza\u00e7\u00e3o, somente em n\u00f3 gerente. Por exemplo, vamos remover um n\u00f3 do cluster, usando o docker node rm, basta passar o ID do n\u00f3 que voc\u00ea quer derrubar.</p> ExecuteSaida   <pre><code>docker node rm aedfo2gsp1ky5cbzb7vmj1eqy\n</code></pre>   <pre><code>Error response from daemon: rpc error: code = FailedPrecondition desc = node aedfo2gsp1ky5cbzb7vmj1eqy is not down and can't be removed\n</code></pre>    <p>O erro est\u00e1 dizendo que o n\u00f3 que tentamos remover n\u00e3o est\u00e1 com o status down, para resolver isso precisamos ir ao n\u00f3 que queremos derrubar e usar o docker swarm leavepara muda o status do n\u00f3.</p> ExecuteSaida   <pre><code>docker swarm leave\n</code></pre>   <pre><code>Node left the swarm.\n</code></pre>    <p>Retorne para o n\u00f3 gerente:</p> <pre><code>```bash\ndocker node ls\n```\n</code></pre> Saida   <pre><code>ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\ntv5szgmtbqunlbhkv4jqqdjrz *   node1      Ready     Active         Leader           20.10.0\nsomcfzr1i52h02s32g8162jv5     node2      Ready     Active                          20.10.0\nm3ug6hoe28zz1dap3ejhqug6n     node3      Ready     Active                          20.10.0\nle62bq94ze1eagudy8dtx12ed     node4      Ready     Active                          20.10.0\naedfo2gsp1ky5cbzb7vmj1eqy     node5      Down      Active                          20.10.0\n</code></pre>    <p>O status do n\u00f3 5 mudou para down e com isso podemos remover efetivamente o n\u00f3.</p> <pre><code>docker node rm aedfo2gsp1ky5cbzb7vmj1eqy\n</code></pre> <p>E de outro docker node ls para confirmar.</p> <p>Adicione o n\u00f3 novamente para continuamos, use o docker swarm join</p>"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/","title":"O que \u00e9 o Swarm?","text":"<p>Imagine uma orquestra, com todas aquelas pessoas tocando instrumentos e com um maestro que transforma, direcionar e guia o ritmo da m\u00fasica.</p> <p>Exemplo de Orquestra</p> <p>E em seguida transformamos isso em c\u00f3digo e estrutura, chegamos nesse modelo:</p> <ul> <li>Pessoas que tocam a orquestra: S\u00e3o os computadores, para cada pessoa, imagine um computador</li> <li>Instrumentos: Para cada instrumento, imagine que seja um docker/trabalhador</li> <li>Maestro: Esse \u00e9 o cara, no docker, chamamos ele de Swarm/gerente, \u00e9 o respons\u00e1vel por dividir, gerenciar e garantir a estabilidade e tudo isso sem precisar que algu\u00e9m fique de olho.</li> <li>M\u00fasica: Por \u00faltimo a m\u00fasica, qual nota tocar? Quando tocar? Atento a cada nota, se houver um erro \u00e9 resolvido por ele e essa vamos chamar conta ineres, que v\u00e3o ser controlados pelo gerente.</li> </ul> <p>Ilustra\u00e7\u00e3o simples:</p>"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#um-pouco-de-teoria","title":"Um pouco de teoria","text":"<p>As caracter\u00edsticas de gerenciamento de agrupamento e orquestra\u00e7\u00e3o incorporadas no Docker Engine s\u00e3o constru\u00eddas utilizando o swarmkit. O Swarmkit \u00e9 um projeto separado que implementa a camada de orquestra\u00e7\u00e3o do Docker sendo utilizado diretamente dentro do Docker.</p> <p>Um swarm consiste em m\u00faltiplos hospedeiros Docker que funcionam em modo swarm e agem como gerentes (para gerir membros e delega\u00e7\u00e3o) e trabalhadores (que gerem servi\u00e7os de swarm). Um determinado anfitri\u00e3o Docker pode ser um gerente, um trabalhador, ou desempenhar ambas as fun\u00e7\u00f5es. Quando se cria um servi\u00e7o, define-se o seu estado ideal (n\u00famero de r\u00e9plicas, rede e recursos de armazenamento dispon\u00edveis, portas que o servi\u00e7o exp\u00f5e ao mundo exterior, e mais). O Docker trabalha para manter esse estado desejado. Por exemplo, se um n\u00f3 de trabalhador ficar indispon\u00edvel, Docker programa as tarefas desse n\u00f3 em outros n\u00f3s. Uma tarefa \u00e9 um cont\u00eainer em funcionamento que faz parte de um servi\u00e7o de swarm sendo gerido por um gerente de swarm, em oposi\u00e7\u00e3o a um cont\u00eainer independente.</p> <p>Uma das principais vantagens dos servi\u00e7os de swarm sobre os cont\u00eaineres independentes \u00e9 que se pode modificar a configura\u00e7\u00e3o de um servi\u00e7o, incluindo as redes e os volumes a que est\u00e1 ligado, sem necessidade de reiniciar manualmente o servi\u00e7o. O Docker ir\u00e1 atualizar a configura\u00e7\u00e3o, parar as tarefas do servi\u00e7o com a configura\u00e7\u00e3o desatualizada, e criar que correspondam \u00e0 configura\u00e7\u00e3o desejada.</p> <p>Quando o Docker funciona em modo de swarm, ainda \u00e9 poss\u00edvel executar cont\u00eaineres independentes em qualquer um dos anfitri\u00f5es Docker que participam no swarm, bem como servi\u00e7os de swarm. Uma diferen\u00e7a chave entre cont\u00eaineres independentes e servi\u00e7os de swarm \u00e9 que apenas os gerentes de swarm podem gerir um swarm, enquanto os cont\u00eaineres independentes podem ser iniciados em qualquer daemon. O Docker daemons da doca podem participar num swarm como gerentes, trabalhadores, ou ambos.</p> <p>Da mesma forma que se pode usar Docker Compose para definir e gerir cont\u00eaineres, pode-se definir e gerir pilhas de servi\u00e7os de Swarm.</p> <p>Continue a ler para obter detalhes sobre conceitos relacionados com os servi\u00e7os de swarm de Docker, incluindo n\u00f3s, servi\u00e7os, tarefas, e equil\u00edbrio de carga mais conhecido como Load balancing.</p> <p>A documenta\u00e7\u00e3o docker, vai definir o swarm/gerentes como manager e os trabalhadores de works.</p>"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#nodes-no","title":"Nodes - n\u00f3","text":"<p>Um n\u00f3 \u00e9 um exemplo do motor Docker que participa no swarm. Tamb\u00e9m se pode pensar nisto como um n\u00f3 de Docker. Pode-se correr um ou mais n\u00f3s num \u00fanico computador f\u00edsico, ou em um servidor de nuvem, mas as implementa\u00e7\u00f5es de swarm de produ\u00e7\u00e3o incluem tipicamente n\u00f3s Docker distribu\u00eddos por m\u00faltiplas m\u00e1quinas f\u00edsicas e de nuvem.</p> <p>Para implantar a sua aplica\u00e7\u00e3o num swarm, submete uma defini\u00e7\u00e3o de servi\u00e7o a um n\u00f3 gerente. O n\u00f3 gerente despacha unidades de trabalho chamadas tarefas para n\u00f3s de trabalhadores.</p> <p>Os n\u00f3s gerentes tamb\u00e9m executam as fun\u00e7\u00f5es de orquestra\u00e7\u00e3o e gerenciamento de agrupamento necess\u00e1rias para manter o estado desejado do swarm. Os n\u00f3s gerentes elegem um \u00fanico l\u00edder para realizar tarefas de orquestra\u00e7\u00e3o.</p> <p>Esses n\u00f3s trabalhadores recebem e executam as tarefas enviadas pelos n\u00f3s gerentes. Por defeito, os n\u00f3s gerentes tamb\u00e9m executam servi\u00e7os como n\u00f3s trabalhadores, mas pode configur\u00e1-los para executar tarefas de gerenciamento exclusivamente e ser n\u00f3s apenas gerentes. Um agente corre em cada n\u00f3 de trabalhadores e informa sobre as tarefas que lhe s\u00e3o atribu\u00eddas. O n\u00f3 trabalhador notifica o n\u00f3 gerente do estado atual das suas tarefas atribu\u00eddas, para que o gerente possa manter o estado desejado de cada trabalhadores.</p>"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#tarefas-e-servicos","title":"Tarefas e servi\u00e7os","text":"<p>Um servi\u00e7o \u00e9 a defini\u00e7\u00e3o das tarefas a executar nos n\u00f3s do gerente ou do trabalhador. \u00c9 a estrutura central do sistema do swarm e a raiz prim\u00e1ria da intera\u00e7\u00e3o do usu\u00e1rio com o swarm.</p> <p>Quando se cria um servi\u00e7o, especifica-se a imagem do cont\u00eainer a utilizar e os comandos a executar dentro de cont\u00eaineres em funcionamento.</p> <p>No modelo de servi\u00e7os replicados, o gerente do swarm distribui um n\u00famero espec\u00edfico de tarefas replicadas entre os n\u00f3s, com base na escala que se define no estado desejado.</p> <p>Para servi\u00e7os globais, o swarm executa uma tarefa para o servi\u00e7o em cada n\u00f3 dispon\u00edvel no agrupamento.</p> <p>Uma tarefa transporta um cont\u00eainer Docker e os comandos para correr dentro do cont\u00eainer. \u00c9 a unidade de programa\u00e7\u00e3o at\u00f4mica do swarm. Os n\u00f3s gerentes atribuem tarefas aos n\u00f3s de trabalhadores conforme o n\u00famero de r\u00e9plicas definidas na escala de servi\u00e7o. Uma vez atribu\u00edda uma tarefa a um n\u00f3, este n\u00e3o pode deslocar-se para outro n\u00f3. S\u00f3 pode funcionar com o n\u00f3 atribu\u00eddo ou falhar.</p>"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#load-balancing","title":"Load balancing","text":"<p>O gerente do swarm utiliza o equil\u00edbrio da carga de entrada para expor os servi\u00e7os que pretende disponibilizar externamente ao swarm. O gerente de swarm pode atribuir automaticamente ao servi\u00e7o em uma porta ou pode configurar uma porta para o servi\u00e7o. \u00c9 poss\u00edvel especificar qualquer porta n\u00e3o utilizada. Se n\u00e3o especificar uma porta, o gerente do swarm atribui ao servi\u00e7o uma porta no intervalo 30000-32767.</p> <p>Componentes externos, tais como equilibradores de carga de nuvem, podem acessar o servi\u00e7o na porta de qualquer n\u00f3 do cluster, quer o n\u00f3 esteja ou n\u00e3o a executar a tarefa para o servi\u00e7o. Todos os n\u00f3s na rota de entrada do swarm de liga\u00e7\u00f5es a uma inst\u00e2ncia de tarefa em execu\u00e7\u00e3o.</p> <p>O modo swarm tem um componente DNS interno que atribui automaticamente a cada servi\u00e7o no swarm uma entrada DNS. O gerente do swarm utiliza o equil\u00edbrio de carga interno para distribuir pedidos entre servi\u00e7os dentro do cluster, com base no nome DNS do servi\u00e7o.</p>"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#raft-consenso-distribuidos","title":"Raft - Consenso Distribu\u00eddos","text":"<p>Antes de continuamos a estudar sobre swarm \u00e9 importante falar sobre um algoritmo que se chama Raft, o respons\u00e1vel pelas decis\u00f5es, \u00e9 o c\u00e9rebro dessa estrutura, para isso vou deixa aqui a Raft.</p> <p>\u00c9 muito importante que voc\u00ea entenda um pouco sobre o Raft, j\u00e1 que essa express\u00e3o vai se repetir mais algumas vezes, em kubernetes, por exemplo. Entender o raft \u00e9 criar melhores aplica\u00e7\u00f5es que usam docker, kubernetes e outras solu\u00e7\u00f5es que usam esse c\u00e9rebro por tr\u00e1s das decis\u00f5es.</p> <p>Nesse ponto, encontrei dois v\u00eddeos did\u00e1ticos sobre o assunto.</p> <p>Linuxtips usar a explica\u00e7\u00e3o visual do Ben Johnson para explicar sobre raft</p> <p>E tamb\u00e9m Algoritmos de consenso em sistemas distribu\u00eddos (teoria e pr\u00e1tica) - Edward Ribeiro que vai abordar o raft e mais alguns algoritmos de consenso.</p>"},{"location":"docker%20swarm/servi%C3%A7os/","title":"Servi\u00e7o","text":"<p>Antes de subimos nossa primeira imagem, vamos realizar um teste, para entender melhor o fluxo que devemos seguir quando usamos o swarm:</p> <ol> <li>Escolha um dos n\u00f3s trabalhadores</li> <li>Execute o comando docker run -d -p 5000:5000 sposigor/app-flask-teste:1</li> <li>Libere a porta em OPEN PORT e digite 5000:</li> </ol>  <p>Ele vai abrir uma nova aba com a aplica\u00e7\u00e3o.</p> <p>Se lembra de como funciona o swarm? Ent\u00e3o, escolha outro n\u00f3 trabalhador e execute o docker ps, fa\u00e7a o mesmo para o n\u00f3 gerente e em ambos vamos ter esse resultado:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n</code></pre> <p>Estranho n\u00e9, se o swarm cria essa conex\u00e3o com todos, porque o cont\u00eainer que subimos n\u00e3o foi refletido nos demais?</p>  <p>\u00c9 porque rodamos uma imagem local, e n\u00e3o no modo swarm.</p>  <p>Certo, o modo swarm precisa de um comando mais espec\u00edfico para roda a imagem que \u00e9 o docker service create.</p> <p>Antes de executar, vamos remover a imagem local, v\u00e1 ao terminal do n\u00f3 que a imagem est\u00e1 rodando:</p> <pre><code>docker rm 56e3abd7f344 --force\n</code></pre> <p>E agora sim, vamos criar nosso servi\u00e7o:</p> ExecuteSaida   <pre><code>docker service create -p 5000:5000 sposigor/app-flask-teste:1\n</code></pre> <ul> <li>A diferen\u00e7a \u00e9 somente o comando inicial, service create, que vai permitir que todos os n\u00f3s se comuniquem no modo swarm.</li> </ul>   <pre><code>Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.\n</code></pre>     <p>Se voc\u00ea executou o comando em um n\u00f3 trabalhador, vai receber o erro do qual os trabalhadores n\u00e3o podem iniciar um servi\u00e7o, apesar do docker run executar, para subir uma imagem no modo swarm \u00e9 preciso fazer atrav\u00e9s do gerente.</p> <p>V\u00e1 ao n\u00f3 gerente e execute o comando.</p> ExecuteSaida   <pre><code>docker service create -p 5000:5000 sposigor/app-flask-teste:1\n</code></pre>   <pre><code>9q3asggjagwfw89vrojc1urew\noverall progress: 1 out of 1 tasks\n1/1: running   [==================================================&gt;]\nverify: Service converged\n</code></pre>     <p>Ent\u00e3o ele est\u00e1 informando que o Service converged, mas o que isso significa? Basicamente que ocorreu bem o processo de cria\u00e7\u00e3o de um servi\u00e7o e com isso ele criou uma tarefa: overall progress: 1 out of 1 tasks.</p> <p>Vamos por partes, execute o docker servie ls.</p> ExecuteSaida   <pre><code>docker service ls\n</code></pre> <ul> <li>Lembrando s\u00f3 o n\u00f3 gerente pode executar</li> </ul>   <pre><code>ID             NAME           MODE         REPLICAS   IMAGE                        PORTS\n9q3asggjagwf   epic_almeida   replicated   1/1        sposigor/app-flask-teste:1   *:5000-&gt;5000/tcp\n</code></pre>     <p>Analisar o cabe\u00e7alho para entender o que significa cada coluna:</p> <ul> <li>ID: \u00c9 o ID do servi\u00e7o</li> <li>NAME: \u00c9 o nome do servi\u00e7o</li> <li>MODE: \u00c9 o modo do servi\u00e7o</li> <li>REPLICAS: \u00c9 a quantidade de replicas</li> <li>IMAGE: \u00c9 a imagem que o servi\u00e7o est\u00e1 rodando</li> <li>PORTS: \u00c9 a porta que est\u00e1 disponivel</li> </ul> <p>Para saber qual maquina est\u00e1 executando o container, use o docker service ps, e passe o ID do servi\u00e7o.</p> ExecuteSaida   <pre><code>docker service ps 9q3asggjagwf\n</code></pre> <ul> <li>Lembrando s\u00f3 o n\u00f3 gerente pode executar</li> <li>Se quiser como \u00e9 possivel passar s\u00f3 alguns caracteres docker service ps 9q3, desde que n\u00e3o tenha outro servi\u00e7o com o mesmo inicio.</li> </ul>   <pre><code>ID             NAME             IMAGE                        NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS\n3qpfhbbmigm4   epic_almeida.1   sposigor/app-flask-teste:1   node2     Running         Running 28 minutes ago      \n</code></pre>     <p>Analisar o cabe\u00e7alho para entender o que significa cada coluna:</p> <ul> <li>ID: \u00c9 o ID do servi\u00e7o.</li> <li>NAME: \u00c9 o nome do servi\u00e7o.</li> <li>IMAGE: \u00c9 a imagem que o servi\u00e7o est\u00e1 rodando.</li> <li>NODE: \u00c9 o n\u00f3 que est\u00e1 executando a imagem naquele momento.</li> <li>DESIRED STATE: \u00c9 o estado desejado da tarefa, pode est\u00e1 em (running, shutdown, or accepted).</li> <li>CURRENT STATE: \u00c9 o estado atual da tarefa.</li> <li>ERROR: Se houver algum erro pe informado</li> <li>PORTS: \u00c9 a porta que est\u00e1 dispon\u00edvel.</li> </ul> <p>Bem, agora que o servi\u00e7o est\u00e1 dispon\u00edvel como swarm, vamos testa a aplica\u00e7\u00e3o em outras m\u00e1quinas, v\u00e1 em um n\u00f3 trabalhador e libere a porta 5000.</p>  <p>Por mais que apenas uma das m\u00e1quinas esteja rodando a aplica\u00e7\u00e3o, ela est\u00e1 dispon\u00edvel na porta 5000 de todas as maquinas. Quem \u00e9 que o cara que est\u00e1 ajudando a resolver esse problema \u00e9 o Routing Mesh atrav\u00e9s do Load balancing.</p>"},{"location":"docker%20swarm/servi%C3%A7os/#routing-mesh","title":"Routing Mesh","text":"<p>O modo swarm do Docker Engine facilita a publica\u00e7\u00e3o de portas para servi\u00e7os para disponibiliz\u00e1-los para recursos fora do swarm. Todos os n\u00f3s, ficam dispon\u00edveis em uma malha de roteamento de entrada. A malha de roteamento permite que cada n\u00f3 no enxame aceite conex\u00f5es em portas publicadas para qualquer servi\u00e7o em execu\u00e7\u00e3o no enxame, mesmo que n\u00e3o haja nenhuma tarefa em execu\u00e7\u00e3o no n\u00f3. A malha de roteamento roteia todas as solicita\u00e7\u00f5es de entrada para portas publicadas em n\u00f3s dispon\u00edveis para um cont\u00eainer ativo.</p>  <p>Testar na pr\u00e1tica, vamos subir a mesma imagem na mesma porta para ver o que ocorre:</p> ExecuteSaida   <pre><code>docker service create -p 5000:5000 sposigor/app-flask-teste:1\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>Error response from daemon: rpc error: code = InvalidArgument desc = port '5000' is already in use by service 'epic_almeida' (9q3asggjagwfw89vrojc1urew) as an ingress port\n</code></pre>     <p>E assim ele nos d\u00e1 um erro, para verificar melhor vamos remover o cont\u00eainer que no meu caso est\u00e1 rodando no node2:</p> Execute   <pre><code>docker rm 6f21 --force\n</code></pre>     <p>Voltei para o n\u00f3 gerente e execute o docker service ls:</p> ExecuteSaida   <pre><code>docker service ls\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>ID             NAME           MODE         REPLICAS   IMAGE                        PORTS\n9q3asggjagwf   epic_almeida   replicated   1/1        sposigor/app-flask-teste:1   *:5000-&gt;5000/tcp\n</code></pre>     <p>Est\u00e1 dizendo que o servi\u00e7o ainda est\u00e1 rodando, vamos usar o docker service ps e passa o ID do servi\u00e7o:</p> ExecuteSaida   <pre><code>docker service ps 9q3asggjagwf\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>ID             NAME                 IMAGE                        NODE      DESIRED STATE   CURRENT STATE           ERROR                         PORTS\nf91ba1oajs39   epic_almeida.1       sposigor/app-flask-teste:1   node3     Running         Running 2 minutes ago                                 \n3qpfhbbmigm4    \\_ epic_almeida.1   sposigor/app-flask-teste:1   node2     Shutdown        Failed 3 minutes ago    \"task: non-zero exit (137)\"\n</code></pre>    <p>E o swarm se encarregou de subir a imagem automaticamente em outro n\u00f3, no caso do node2 para o node3, ent\u00e3o se fomos l\u00e1 na porta 5000 novamente, a aplica\u00e7\u00e3o vai estar rodando normalmente.</p>"},{"location":"docker%20swarm/servi%C3%A7os/#gerente","title":"Gerente","text":"<p>Imagine que sem querer voc\u00ea desligue o computador que est\u00e1 o gerente? E agora ser\u00e1 que deu ruim?</p> <p>Sim, se voc\u00ea possui somente um gerente no seu swarm, o swarm vai ser desconfigurado, sendo necess\u00e1rio, reconfigurar o swarm. Mas os servi\u00e7os param? N\u00e3o, eles v\u00e3o continuar, contudo, n\u00e3o existe mais ningu\u00e9m para garantir a estabilidade do servi\u00e7o.</p> <p>Como podemos resolver essa categoria de situa\u00e7\u00e3o? Basta fazer o backup da pasta do swarm.</p> <p>Isso vai ser feito de forma manual, porque o docker n\u00e3o disponibiliza ainda nenhuma ferramenta para backup nesse sentido.</p> <ol> <li>Fa\u00e7a uma c\u00f3pia da sua pasta que cont\u00e9m as informa\u00e7\u00f5es do swarm, no Linux ela fica em /var/lib/docker/swarm/</li> <li>No Linux basta usar o comando cp -r /var/lib/docker/swarm/ backup</li> </ol> <p>Agora com seu backup feito, para subir um swarm do zero com as configura\u00e7\u00f5es nesse backup:</p> <ol> <li>Vamos fazer o caminho inverso, cp -r backup/* /var/lib/docker/swarm/</li> <li>Durante a cria\u00e7\u00e3o do swarm vamos passa uma nova flag --force-new-cluster</li> <li>docker swarm init --force-new-cluster --advertise-addr 192.168.0.13</li> </ol> <p>Essa \u00e9 a f\u00f3rmula mais usada para recupera\u00e7\u00e3o de swarm, independente do que tenha acontecido. Numa aplica\u00e7\u00e3o pequena, n\u00e3o faz sentido ter que criar esses backups, por\u00e9m quando os servi\u00e7os s\u00e3o muitos e a infraestrutura \u00e9 maior ainda, \u00e9 necess\u00e1rio ter garantias para recuperar rapidamente o estado anterior ao problema.</p>"},{"location":"docker%20swarm/servi%C3%A7os/#criando-gerentes","title":"Criando Gerentes","text":"<p>Talvez, ter mais gerentes no seu swarm possa ajudar a resolve esse problema, afinal por mais que o backup seja uma solu\u00e7\u00e3o, para determinadas situa\u00e7\u00f5es, podemos simplesmente adicionar novos Gerentes.</p> <p>Adicione mais uma inst\u00e2ncia no docker play e vamos passar para o token do comando docker swarm join-token e passe o manager:</p> ExecuteSaida   <pre><code>docker swarm join-token manager\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>To add a manager to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-1waduqjabmpmygpy6xicwtt1srwbchelwnu8fsv3fj8qk4jdvi-8tke9gy4e6crw0euq2tki1m0y 192.168.0.13:2377\n</code></pre>     <p>V\u00e1 \u00e0 inst\u00e2ncia criada e execute o comando passado.</p> ExecuteSaida   <pre><code>docker swarm join --token SWMTKN-1-1waduqjabmpmygpy6xicwtt1srwbchelwnu8fsv3fj8qk4jdvi-8tke9gy4e6crw0euq2tki1m0y 192.168.0.13:2377\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>This node joined a swarm as a manager.\n</code></pre>     <p>Se por acaso estive com a configura\u00e7\u00e3o de 1 gerente e 4 trabalhadores, basta derruba um dos trabalhadores e remover do swarm, e em seguida suba novamente como gerente.</p> <p>Em um dos gerentes:</p> ExecuteSaida   <pre><code>docker node ls\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\nfhiexwgtupwto0yzz5i4nbve7     node1      Ready     Active         Leader           20.10.0\nx3ghs9kh5nftbnnd7otoyg4hu     node2      Ready     Active                          20.10.0\nnufnx3dlv0orjgb0dm5li4aah     node3      Ready     Active                          20.10.0\njvxn6dt0zob66po3d5rdrbaru     node4      Ready     Active                          20.10.0\n1bk98ofc3qasq9n6v298x4jjv *   node5      Ready     Active         Reachable        20.10.0\n</code></pre>    <p>E pronto o seu novo gerente j\u00e1 est\u00e1 rodando normalmente.</p>"},{"location":"docker%20swarm/servi%C3%A7os/#raft-na-pratica","title":"Raft na pr\u00e1tica","text":"<p>Vamos reiniciar nossa aplica\u00e7\u00e3o no docker play feche com o close session e de outro start, lembrando dependendo o servi\u00e7o pode estar cheio, mas basta continuar tentando at\u00e9 conseguir.</p> <p>Click na chave de boca e selecione a op\u00e7\u00e3o:</p>  <p>Ap\u00f3s carregar a configura\u00e7\u00e3o, execute docker node ls em qualquer um dos gerentes:</p> ExecuteSaida   <pre><code>docker node ls\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\n3g4vyx694h4pd3dxlj442x88z     manager1   Ready     Active         Reachable        20.10.0\nlheo3at6gjkka6uoehk3lr0ow *   manager2   Ready     Active         Leader           20.10.0\nptvwj6n2ums5o8b0lsfabhxgk     manager3   Ready     Active         Reachable        20.10.0\nn2230i73ivqhyqqg0ty2hw3nd     worker1    Ready     Active                          20.10.0\ntdiufxyidqv47atj937ffthb1     worker2    Ready     Active                          20.10.0\n</code></pre>     <p>Agora temos 3 gerentes, sendo um deles o lider. Vamos derrubar o lider com docker swarm leave --force no terminal do lider.</p> <p>Assim que executar v\u00e1 em outro gerente:</p> ExecuteSaida   <pre><code>docker node ls\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\n3g4vyx694h4pd3dxlj442x88z *   manager1   Ready     Active         Leader           20.10.0\nlheo3at6gjkka6uoehk3lr0ow     manager2   Unknown   Active         Unreachable      20.10.0\nptvwj6n2ums5o8b0lsfabhxgk     manager3   Ready     Active         Reachable        20.10.0\nn2230i73ivqhyqqg0ty2hw3nd     worker1    Ready     Active                          20.10.0\ntdiufxyidqv47atj937ffthb1     worker2    Ready     Active                          20.10.0\n</code></pre>     <p>Temos um novo l\u00edder agora, que pe maneger1, enquanto o manager3 est\u00e1 com o MANAGER STATUS de Unreachable, ou seja, est\u00e1 fora do swarm por algum motivo.</p> <p>O que ocorreu? O Raft, foi acionado para gera uma elei\u00e7\u00e3o, e o vencedor da elei\u00e7\u00e3o foi o maneger1.</p> <p>Recomenda\u00e7\u00e3o do docker sobre os gerentes, eles recomendam que tenhamos n\u00fameros impares, 3, 5 ou 7.</p>"},{"location":"docker%20swarm/servi%C3%A7os/#removendo-gerentes","title":"Removendo Gerentes","text":"<p>Vamos remover o nosso gerente que est\u00e1 Unreachable. Em qualquer gerente execute:</p> ExecuteSaida   <pre><code>docker node rm manager2\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>Error response from daemon: rpc error: code = FailedPrecondition desc = node lheo3at6gjkka6uoehk3lr0ow is a cluster manager and is a member of the raft cluster. It must be demoted to worker before removal\n</code></pre>     <p>E ele deu um erro, ele est\u00e1 dizendo que precisamos rebaixar o menager2 para trabalhador antes de remover efetivamente do swarm, usando o docker node demote:</p> ExecuteSaida   <pre><code>docker node demote manager2\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>Manager manager2 demoted in the swarm.\n</code></pre>    <p>Agora \u00e9 poss\u00edvel remover com o docker node rm, fa\u00e7a e vamos adicionar o gerente no swarm com o docker swarm join-token manager pegue o comando que ele retorna e cola no gerente que voc\u00ea removeu.</p>"},{"location":"docker%20swarm/servi%C3%A7os/#restricao-de-servicos","title":"Restri\u00e7\u00e3o de Servi\u00e7os","text":"<p>Por padr\u00e3o o docker permite que o gerente rode servi\u00e7os, ou seja, \u00e9 poss\u00edvel que a maquina que vai efetivamente roda alguma imagem seja um gerente e n\u00e3o um trabalhador. Mas vamos supor que por algum motivo voc\u00ea n\u00e3o quer permitir que o gerente rode qualquer servi\u00e7o, ou s\u00f3 quer determinado servi\u00e7o rode no gerente.</p> <p>Ent\u00e3o podemos resolver da seguinte forma:</p> 1234   <p>Suba a imagem</p> <pre><code>docker service create -p 5000:5000 sposigor/app-flask-teste:1\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>docker node ls\n</code></pre> <p>E a sa\u00edda:</p> <pre><code>ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\n3g4vyx694h4pd3dxlj442x88z     manager1   Ready     Active         Leader           20.10.0\n3xtgnz4d2098nnhjrq5ilv7l6 *   manager2   Ready     Active         Reachable        20.10.0\nptvwj6n2ums5o8b0lsfabhxgk     manager3   Ready     Active         Reachable        20.10.0\nn2230i73ivqhyqqg0ty2hw3nd     worker1    Ready     Active                          20.10.0\ntdiufxyidqv47atj937ffthb1     worker2    Ready     Active                          20.10.0\n</code></pre>   <p>Aqui vamos alterar o estado da AVAILABILITY, para conseguimos fazer essa restri\u00e7\u00e3o do n\u00f3. Enquanto estiver como Active significa que est\u00e1 dispon\u00edvel para roda servi\u00e7os. Para alterar vamos usar o docker node update Vamos passa a flag --availability que tem 3 estados (\"active\"|\"pause\"|\"drain\"). No nosso caso queremos usar o drain para que o gerente n\u00e3o execute nenhum servi\u00e7o. Por fim basta passa qual maquina voc\u00ea quer alterar o estado.</p> <pre><code>docker node update --availability drain manager2\n</code></pre>   <p>Rode novamente o docker node ls</p> <pre><code>docker node ls\n</code></pre> <p>Saida:</p> <pre><code>ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\n3g4vyx694h4pd3dxlj442x88z     manager1   Ready     Active         Leader           20.10.0\n3xtgnz4d2098nnhjrq5ilv7l6 *   manager2   Ready     Drain          Reachable        20.10.0\nptvwj6n2ums5o8b0lsfabhxgk     manager3   Ready     Active         Reachable        20.10.0\nn2230i73ivqhyqqg0ty2hw3nd     worker1    Ready     Active                          20.10.0\ntdiufxyidqv47atj937ffthb1     worker2    Ready     Active                          20.10.0\n</code></pre>     <p>Assim conseguimos alterar a AVAILABILITY do manager2 para a Drain e com isso ele se torna inacessivel para executar servi\u00e7os, mas continuar a executar a atividade de gerente normalmente.</p> <p>Mas vamos supor que voc\u00ea queira fazer isso em todos os gerentes de uma vez, vamos ter que alterar 1 por 1? N\u00e3o, vamos usar um comando mais generalista que vai definir aonde queremos que o servi\u00e7o rode. Usando o docker service update e passando a flag --constraint-add que vai restrigir determido servi\u00e7o para um grupo especifico.</p> ExecuteSaida   <p>Vamos passa para o comando a flag --constraint-add Nesse caso queremos especificar um tipo de n\u00f3, ent\u00e3o vamos usar o node.role==worker E por ultimo vamos passa o ID do servi\u00e7o para saber qual servi\u00e7o vamos aplicar essa restri\u00e7\u00e3o</p> <pre><code>docker service update --constraint-add node.role==worker gnj34gs43ebi\n</code></pre>   <pre><code>gnj34gs43ebi\noverall progress: 1 out of 1 tasks\n1/1: running   [==================================================&gt;]\nverify: Service converged\n</code></pre>     <p>Alguns cuidados durante a restri\u00e7\u00e3o desses servi\u00e7os, vamos supor que voc\u00ea fa\u00e7a essa restri\u00e7\u00e3o usando um n\u00f3 que n\u00e3o existe, ele vai rodar o comando e todas as suas aplica\u00e7\u00f5es se tornaram indisponiveis, porque a restri\u00e7\u00e3o que houve, restringiu para um n\u00f3 inexistente.</p> <ul> <li>Quando restringir o servi\u00e7os nos gerente?</li> </ul> <p>Queremos rodar servi\u00e7os no manager, que s\u00e3o servi\u00e7os de monitoramento, servi\u00e7os de seguran\u00e7a, ou seja, servi\u00e7os que s\u00e3o cr\u00edticos, que dependemos na nossa aplica\u00e7\u00e3o. \u00c9 importante pondera essa quest\u00e3o, para entender quais s\u00e3o os pontos criticos da aplica\u00e7\u00e3o.</p>"},{"location":"docker%20swarm/servi%C3%A7os/#replicas","title":"Replicas","text":"<p>J\u00e1 vimos essa palavra aqui:</p> ExecuteSaida   <pre><code>docker service ls\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>ID             NAME              MODE         REPLICAS   IMAGE                        PORTS\ngnj34gs43ebi   dazzling_lamarr   replicated   1/1        sposigor/app-flask-teste:1   *:5000-&gt;5000/tcp\n</code></pre>     <p>Certo o que significa isso, bem para explicar a gente precisa descobrir aonde nosso servi\u00e7o est\u00e1 rodando:</p> ExecuteSaida   <pre><code>docker service ps gnj34gs43ebi\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>ID             NAME                IMAGE                        NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS\nw105x64ylhne   dazzling_lamarr.1   sposigor/app-flask-teste:1   worker2   Running         Running 37 minutes ago      \n</code></pre>     <p>O servi\u00e7o est\u00e1 rodando no worker2, para explicar vamos repassar pelo conceito de Load Balancing. Quando a porta 5000 \u00e9 acessada independete da maquina que estiver no swarm, a aplica\u00e7\u00e3o \u00e9 diponibilizada, por\u00e9m se houver intera\u00e7\u00e3o, vamos supor que haja um cadastro de um usuario, o que ocorre na pratica \u00e9 que essa altera\u00e7\u00e3o vai ser redirecionada para a maquina que est\u00e1 rodando essa imagem. E a Replica serve para isso, para que possamos usar mais de uma maquina nesse processo de processamento dessa demanda. Ent\u00e3o vamos replicar essa imagem, que para outras maquinas com o docker service update.</p> ExecuteSaida   <pre><code>docker service update --replicas 8 gnj34gs43ebi\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p>   <pre><code>gnj34gs43ebi\noverall progress: 4 out of 4 tasks\n1/4: running   [==================================================&gt;]\n2/4: running   [==================================================&gt;]\n3/4: running   [==================================================&gt;]\n4/4: running   [==================================================&gt;]\nverify: Service converged     \n</code></pre>     <p>Vamos executar o docker service ls para verificar como est\u00e1 as r\u00e9plicas:</p> docker service lsdocker service psRemovendo --constraint-rm8 Replicas   <pre><code>docker service ls\n</code></pre> <p>Lembrando s\u00f3 o n\u00f3 gerente pode executar</p> <p>Saida:</p> <pre><code>ID             NAME              MODE         REPLICAS   IMAGE                        PORTS\ngnj34gs43ebi   dazzling_lamarr   replicated   4/4        sposigor/app-flask-teste:1   *:5000-&gt;5000/tcp\n</code></pre> <p>Temos agora, 4 replicas</p>   <p>Para saber quais maquinas est\u00e3o rodando o servi\u00e7o:</p> <pre><code>docker service ps gnj34gs43ebi\n</code></pre> <p>Saida:</p> <pre><code>ID             NAME                IMAGE                        NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS\nw105x64ylhne   dazzling_lamarr.1   sposigor/app-flask-teste:1   worker2   Running         Running 52 minutes ago             \nv9bmopdi3nr3   dazzling_lamarr.2   sposigor/app-flask-teste:1   worker1   Running         Running 3 minutes ago              \nsp81t2z56n67   dazzling_lamarr.3   sposigor/app-flask-teste:1   worker1   Running         Running 4 minutes ago              \noxdvxulccegb   dazzling_lamarr.4   sposigor/app-flask-teste:1   worker2   Running         Running 5 minutes ago   \n</code></pre> <p>Lembrando que fizemo a restri\u00e7\u00e3o para somente workes executarem as tarefas. Vamos remover a restri\u00e7\u00e3o e tentar novamente com 8 replicas</p>   <ul> <li>Vamos passar os mesmos par\u00e2metros que usamos para restringir os servi\u00e7os</li> </ul> <pre><code>docker service update --constraint-rm node.role==worker gnj34gs43ebi\n</code></pre>   <ul> <li>J\u00e1 vamos passar 8 no par\u00e2metro.</li> </ul> <pre><code>docker service update --replicas 8 gnj34gs43ebi\n</code></pre> <p>Em seguida:</p> <pre><code>docker service ps gnj34gs43ebi\n</code></pre> <p>E a sa\u00edda:</p> <pre><code>ID             NAME                IMAGE                        NODE       DESIRED STATE   CURRENT STATE               ERROR     PORTS\nw105x64ylhne   dazzling_lamarr.1   sposigor/app-flask-teste:1   worker2    Running         Running about an hour ago             \nv9bmopdi3nr3   dazzling_lamarr.2   sposigor/app-flask-teste:1   worker1    Running         Running 17 minutes ago                \nsp81t2z56n67   dazzling_lamarr.3   sposigor/app-flask-teste:1   worker1    Running         Running 17 minutes ago                \noxdvxulccegb   dazzling_lamarr.4   sposigor/app-flask-teste:1   worker2    Running         Running 18 minutes ago                \ngs8m3brh4sw6   dazzling_lamarr.5   sposigor/app-flask-teste:1   manager3   Running         Running 3 minutes ago                 \nsr1u0kr19hut   dazzling_lamarr.6   sposigor/app-flask-teste:1   manager3   Running         Running 3 minutes ago                 \nz80bqt30sscq   dazzling_lamarr.7   sposigor/app-flask-teste:1   manager3   Running         Running 3 minutes ago                 \n2yjmab0lkl7q   dazzling_lamarr.8   sposigor/app-flask-teste:1   manager1   Running         Running 3 minutes ago    \n</code></pre>     <p>Com isso agora temos diversas maquinas rodando nosso servi\u00e7o, ou seja, o poder de processamento aumentou. Porque assim que ocorrer uma demanda de processamento, o load balancing por tr\u00e1s vai estar resolvendo isso.</p> <p>Tamb\u00e9m \u00e9 poss\u00edvel usar o docker service scale, vamos passar o ID do servi\u00e7o e a quantidade de r\u00e9plicas para escalar</p> <pre><code>docker service scale gnj34gs43ebi=6\n</code></pre>  <p>Temos tamb\u00e9m a op\u00e7\u00e3o de passar diretamente na cria\u00e7\u00e3o do servi\u00e7o, basta passa a flag --mode e o par\u00e2metro global</p> <pre><code>docker service create -p 5000:5000 --mode global sposigor/app-flask-teste:1\n</code></pre> <p>\u00c9 importante lembrar que n\u00e3o \u00e9 recomendado passar todos os servi\u00e7os com o par\u00e2metro global, afinal isso vai exigir um consumo que poderia ser poupado da aplica\u00e7\u00e3o no processador individual. Seja sempre critico com essa necessidade.</p>"}]}