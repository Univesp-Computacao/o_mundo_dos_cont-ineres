{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Falaremos sobre Cont\u00eaineres? \u00b6 Para iniciar essa conversa acho importante ter uma vis\u00e3o geral sobre o conte\u00fado \u00e9 importante que voc\u00ea assista o video do C\u00f3digo Fonte TV que explica sobre cont\u00eaineres. E tamb\u00e9m esse Containers, Docker e Kubernetes com Giovanni Bassi que fala um pouco sobre essas mudan\u00e7as que os cont\u00eaineres fizeram. Como cont\u00eaineres funcionam? \u00b6 Temos agora uma base te\u00f3rica para trabalhar, refor\u00e7aremos o conceito de M\u00e1quina Virtual e Cont\u00eaineres: Cont\u00eaineres s\u00e3o medidos em megabyte. Eles cont\u00eam, no m\u00e1ximo, a aplica\u00e7\u00e3o e os arquivos necess\u00e1rios para execut\u00e1-la. Al\u00e9m disso, eles costumam ser usados para empacotar fun\u00e7\u00f5es individuais que realizam tarefas espec\u00edficas, os famosos microsservi\u00e7os. Como s\u00e3o leves e t\u00eam um sistema operacional compartilhado, os cont\u00eaineres s\u00e3o muito f\u00e1ceis de migrar entre v\u00e1rios ambientes. As m\u00e1quinas virtuais(VM) s\u00e3o medidas em gigabyte. Eles costumam ter seu pr\u00f3prio sistema operacional, possibilitando a execu\u00e7\u00e3o simult\u00e2nea de v\u00e1rias fun\u00e7\u00f5es com uso intenso de recursos. Por terem um n\u00famero maior de recursos \u00e0 disposi\u00e7\u00e3o, as m\u00e1quinas virtuais conseguem abstrair, dividir, duplicar e emular por inteiro servidores, sistemas operacionais, desktops, bancos de dados e redes. Temos essa imagem para ilustra melhor esse conceito: Virtualiza\u00e7\u00e3o : O hipervisor \u00e9 um software que separa os recursos das respectivas m\u00e1quinas f\u00edsicas para eles poderem ser particionados e dedicados \u00e0s m\u00e1quinas virtuais. Quando o usu\u00e1rio emite uma instru\u00e7\u00e3o de m\u00e1quina virtual que exige mais recursos do ambiente f\u00edsico, o hipervisor retransmite a solicita\u00e7\u00e3o ao sistema f\u00edsico e armazena as mudan\u00e7as em mem\u00f3ria transit\u00f3ria. As m\u00e1quinas virtuais s\u00e3o similares aos servidores f\u00edsicos e agem como eles, o que pode multiplicar as desvantagens de grandes infraestruturas de sistema operacional e das depend\u00eancias da aplica\u00e7\u00e3o. Na maioria das vezes, essas infraestruturas n\u00e3o s\u00e3o necess\u00e1rias para executar uma aplica\u00e7\u00e3o ou microsservi\u00e7os. Cont\u00eaineres : Os cont\u00eaineres armazenam um microsservi\u00e7o ou aplica\u00e7\u00e3o, al\u00e9m de todos os elementos necess\u00e1rios para execut\u00e1-los. Tudo que eles cont\u00eam \u00e9 mantido em um recurso chamado imagem: um arquivo baseado em c\u00f3digo que inclui todas as bibliotecas e depend\u00eancias. Pense nesses arquivos como uma instala\u00e7\u00e3o da distribui\u00e7\u00e3o Linux, j\u00e1 que a imagem inclui pacotes RPM e arquivos de configura\u00e7\u00e3o. Como os cont\u00eaineres s\u00e3o muito pequenos, geralmente h\u00e1 centenas deles levemente acoplados. Beneficios de usar Cont\u00eaineres \u00b6 Cont\u00eaineres se tornaram populares porque eles fornecem benef\u00edcios extra, tais como: Cria\u00e7\u00e3o e implanta\u00e7\u00e3o \u00e1gil de aplica\u00e7\u00f5es: aumento da facilidade e efici\u00eancia na cria\u00e7\u00e3o de imagem de cont\u00eainer comparado ao uso de imagem de VM. Desenvolvimento, integra\u00e7\u00e3o e implanta\u00e7\u00e3o cont\u00ednuos: fornece capacidade de cria\u00e7\u00e3o e de implanta\u00e7\u00e3o de imagens de cont\u00eainer de forma confi\u00e1vel e frequente, com a funcionalidade de efetuar revers\u00f5es r\u00e1pidas e eficientes (devido \u00e0 imutabilidade da imagem). Separa\u00e7\u00e3o de interesses entre Desenvolvimento e Opera\u00e7\u00f5es: crie imagens de cont\u00eaineres de aplica\u00e7\u00f5es no momento de constru\u00e7\u00e3o/libera\u00e7\u00e3o em vez de no momento de implanta\u00e7\u00e3o, desacoplando as aplica\u00e7\u00f5es da infraestrutura. A capacidade de observa\u00e7\u00e3o (Observabilidade) n\u00e3o apenas apresenta informa\u00e7\u00f5es e m\u00e9tricas no n\u00edvel do sistema operacional, mas tamb\u00e9m a integridade da aplica\u00e7\u00e3o e outros sinais. Consist\u00eancia ambiental entre desenvolvimento, teste e produ\u00e7\u00e3o: funciona da mesma forma em um laptop e na nuvem. Portabilidade de distribui\u00e7\u00e3o de nuvem e sistema operacional: executa no Ubuntu, RHEL, CoreOS, localmente, nas principais nuvens p\u00fablicas e em qualquer outro lugar. Gerenciamento centrado em aplica\u00e7\u00f5es: eleva o n\u00edvel de abstra\u00e7\u00e3o da execu\u00e7\u00e3o em um sistema operacional em hardware virtualizado \u00e0 execu\u00e7\u00e3o de uma aplica\u00e7\u00e3o em um sistema operacional usando recursos l\u00f3gicos. Microservi\u00e7os fracamente acoplados, distribu\u00eddos, el\u00e1sticos e livres: as aplica\u00e7\u00f5es s\u00e3o divididas em partes menores e independentes e podem ser implantados e gerenciados dinamicamente - n\u00e3o uma pilha monol\u00edtica em execu\u00e7\u00e3o em uma grande m\u00e1quina de prop\u00f3sito \u00fanico. Isolamento de recursos: desempenho previs\u00edvel de aplica\u00e7\u00f5es. Utiliza\u00e7\u00e3o de recursos: alta efici\u00eancia e densidade J\u00e1 que sabemos a diferen\u00e7a entre VM e Cont\u00eaineres, precisamos entender aonde essa solu\u00e7\u00e3o se encaixa, pois, com as mudan\u00e7as que a tecnologia traz no decorrer da sua hist\u00f3ria, \u00e9 preciso entender as novas pr\u00e1ticas de TI com as tradicionais para chegamos a um ponto-chave dessa pergunta: As novas pr\u00e1ticas de TI (desenvolvimento nativo em nuvem, CI/CD e DevOps) existem gra\u00e7as \u00e0 divis\u00e3o das cargas de trabalho nas menores unidades \u00fateis poss\u00edveis, que geralmente s\u00e3o uma fun\u00e7\u00e3o ou um microsservi\u00e7o. Essas unidades s\u00e3o melhor empacotadas em cont\u00eaineres. Assim, v\u00e1rias equipes podem trabalhar em partes separadas de uma aplica\u00e7\u00e3o ou servi\u00e7o sem interromper, ou p\u00f4r em risco o c\u00f3digo empacotado em outros cont\u00eaineres. Nas arquiteturas de TI tradicionais (monol\u00edticas e legadas), todos os elementos de uma carga de trabalho s\u00e3o mantidos em um arquivo grande que n\u00e3o pode ser dividido. Por isso, ele precisa ser empacotado como uma unidade completa em um ambiente maior, frequentemente uma m\u00e1quina virtual. Era comum criar e executar uma aplica\u00e7\u00e3o inteira dentro de uma m\u00e1quina virtual, mesmo sabendo que ao armazenar todo o c\u00f3digo e depend\u00eancias, ela ficava grande demais, e isso poderia gerar falhas em cascata e downtime durante as atualiza\u00e7\u00f5es. Se aprofundando um pouco mais nos Cont\u00eaineres \u00b6 Se em algum momento voc\u00ea j\u00e1 usou uma VM, imagino que tenha passado pelo processo de baixa uma SO, seja Linux ou Windows e alocando os recursos conforme a capacidade do hardware dispon\u00edvel, ent\u00e3o esse processo dos Cont\u00eaineres pode parecer meio confuso, para isso deixei duas perguntas que precisamos responder antes de cont\u00ednua. Se n\u00e3o \u00e9 uma virtualiza\u00e7\u00e3o como ocorre o isolamento? Como ocorre a divis\u00e3o de recursos? Se n\u00e3o \u00e9 uma virtualiza\u00e7\u00e3o como ocorre o isolamento? \u00b6 Existe uma palavra importante, que resolve essa quest\u00e3o Namespaces , permitindo que os cont\u00eaineres consigam isolamento em determinados n\u00edveis que s\u00e3o: PID : Isola os processos rodando dentro do cont\u00eainer NET : Isola as interfaces de redes IPC : Isola a comunica\u00e7\u00e3o entre processos e memoria MNT : Isola o ponto de montagem UTS : Isola o Kernel, ou seja, simula um novo host(usu\u00e1rio) USER : Isola os arquivos do sistema Dentre esse processo o mais importante, \u00e9 o UTS, o respons\u00e1vel pelo isolamento dos cont\u00eaineres, ao simular um novo host, ele usar o Kernel da m\u00e1quina f\u00edsica para o processamento, por\u00e9m isoladamente. Os namespaces fazem parte do kernel do Linux desde 2002, os namespaces s\u00e3o respons\u00e1veis por gerar o isolamento de grupos de processos em seu n\u00edvel l\u00f3gico. Como ocorre a divis\u00e3o de recursos? \u00b6 O gerenciamento de recursos, tem a palavra Cgroups para resolver essa demanda de gerenciar, seja ela automaticamente ou manualmente. Os cgroups fornecem os recursos: Limites de recursos : voc\u00ea pode configurar um cgroup para limitar quanto de um determinado recurso (mem\u00f3ria ou CPU, por exemplo) um processo pode usar. Prioriza\u00e7\u00e3o : Voc\u00ea pode controlar quanto de um recurso (CPU, disco ou rede) um processo pode usar em compara\u00e7\u00e3o com processos em outro cgroup quando h\u00e1 conten\u00e7\u00e3o de recursos. Contabilidade : os limites de recursos s\u00e3o monitorados e relatados no n\u00edvel do cgroup. Controle : Voc\u00ea pode alterar o status (congelado, interrompido ou reiniciado) de todos os processos em um cgroup com um \u00fanico comando. Os cgroups s\u00e3o um componente-chave dos cont\u00eaineres porque geralmente h\u00e1 v\u00e1rios processos em execu\u00e7\u00e3o em um cont\u00eainer que precisam ser controlados juntos. Agora que entendemos o significado de cont\u00eaineres, temos mais duas palavras importantes para aprender: Kubernetes Swarm Essas duas palavras possuem uma frase em comum: orquestra\u00e7\u00e3o de containers Para que serve a orquestra\u00e7\u00e3o de cont\u00eaineres? \u00b6 Basicamente, a orquestra\u00e7\u00e3o de cont\u00eaineres automatiza a implanta\u00e7\u00e3o, o gerenciamento, a escala e a rede dos cont\u00eaineres. Use a orquestra\u00e7\u00e3o de cont\u00eaineres para automatizar e gerenciar tarefas como: Provisionamento e implanta\u00e7\u00e3o Configura\u00e7\u00e3o e programa\u00e7\u00e3o Aloca\u00e7\u00e3o de recursos Disponibilidade dos containers Escala ou remo\u00e7\u00e3o de containers com base no balanceamento de cargas de trabalho na infraestrutura Balanceamento de carga e roteamento de tr\u00e1fego Monitoramento da integridade do container Configura\u00e7\u00e3o da aplica\u00e7\u00e3o com base no container em que ela ser\u00e1 executada Prote\u00e7\u00e3o das intera\u00e7\u00f5es entre os containers As ferramentas de orquestra\u00e7\u00e3o de cont\u00eaineres fornecem um framework para gerenciar arquiteturas de microsservi\u00e7os e cont\u00eaineres em escala. Muitas delas podem ser usadas no gerenciamento do ciclo de vida dos cont\u00eaineres. Algumas op\u00e7\u00f5es s\u00e3o o Kubernetes, Docker Swarm e Apache Mesos, DC/SO e N\u00f4made. Nesse tema vamos aborda sobre e praticar: Docker 100% Docker Swarm: Cluster do Docker 100% Usar o Vagrant para cria\u00e7\u00e3o da infraestrutura do Swarm 0% Kubernetes 60% Amazon Kubernetes 0% Azure Kubernetes 0% GCP Kubernetes 0% Fontes usadas para elabora\u00e7\u00e3o desse conte\u00fado \u00b6 Red Hat: Cont\u00eaineres vs Vms Red Hat: Orquestra\u00e7\u00e3o de containers etcd: O que s\u00e3o namespaces e cgroups","title":"Inicio"},{"location":"#falaremos-sobre-conteineres","text":"Para iniciar essa conversa acho importante ter uma vis\u00e3o geral sobre o conte\u00fado \u00e9 importante que voc\u00ea assista o video do C\u00f3digo Fonte TV que explica sobre cont\u00eaineres. E tamb\u00e9m esse Containers, Docker e Kubernetes com Giovanni Bassi que fala um pouco sobre essas mudan\u00e7as que os cont\u00eaineres fizeram.","title":"Falaremos sobre Cont\u00eaineres?"},{"location":"#como-conteineres-funcionam","text":"Temos agora uma base te\u00f3rica para trabalhar, refor\u00e7aremos o conceito de M\u00e1quina Virtual e Cont\u00eaineres: Cont\u00eaineres s\u00e3o medidos em megabyte. Eles cont\u00eam, no m\u00e1ximo, a aplica\u00e7\u00e3o e os arquivos necess\u00e1rios para execut\u00e1-la. Al\u00e9m disso, eles costumam ser usados para empacotar fun\u00e7\u00f5es individuais que realizam tarefas espec\u00edficas, os famosos microsservi\u00e7os. Como s\u00e3o leves e t\u00eam um sistema operacional compartilhado, os cont\u00eaineres s\u00e3o muito f\u00e1ceis de migrar entre v\u00e1rios ambientes. As m\u00e1quinas virtuais(VM) s\u00e3o medidas em gigabyte. Eles costumam ter seu pr\u00f3prio sistema operacional, possibilitando a execu\u00e7\u00e3o simult\u00e2nea de v\u00e1rias fun\u00e7\u00f5es com uso intenso de recursos. Por terem um n\u00famero maior de recursos \u00e0 disposi\u00e7\u00e3o, as m\u00e1quinas virtuais conseguem abstrair, dividir, duplicar e emular por inteiro servidores, sistemas operacionais, desktops, bancos de dados e redes. Temos essa imagem para ilustra melhor esse conceito: Virtualiza\u00e7\u00e3o : O hipervisor \u00e9 um software que separa os recursos das respectivas m\u00e1quinas f\u00edsicas para eles poderem ser particionados e dedicados \u00e0s m\u00e1quinas virtuais. Quando o usu\u00e1rio emite uma instru\u00e7\u00e3o de m\u00e1quina virtual que exige mais recursos do ambiente f\u00edsico, o hipervisor retransmite a solicita\u00e7\u00e3o ao sistema f\u00edsico e armazena as mudan\u00e7as em mem\u00f3ria transit\u00f3ria. As m\u00e1quinas virtuais s\u00e3o similares aos servidores f\u00edsicos e agem como eles, o que pode multiplicar as desvantagens de grandes infraestruturas de sistema operacional e das depend\u00eancias da aplica\u00e7\u00e3o. Na maioria das vezes, essas infraestruturas n\u00e3o s\u00e3o necess\u00e1rias para executar uma aplica\u00e7\u00e3o ou microsservi\u00e7os. Cont\u00eaineres : Os cont\u00eaineres armazenam um microsservi\u00e7o ou aplica\u00e7\u00e3o, al\u00e9m de todos os elementos necess\u00e1rios para execut\u00e1-los. Tudo que eles cont\u00eam \u00e9 mantido em um recurso chamado imagem: um arquivo baseado em c\u00f3digo que inclui todas as bibliotecas e depend\u00eancias. Pense nesses arquivos como uma instala\u00e7\u00e3o da distribui\u00e7\u00e3o Linux, j\u00e1 que a imagem inclui pacotes RPM e arquivos de configura\u00e7\u00e3o. Como os cont\u00eaineres s\u00e3o muito pequenos, geralmente h\u00e1 centenas deles levemente acoplados.","title":"Como cont\u00eaineres funcionam?"},{"location":"#beneficios-de-usar-conteineres","text":"Cont\u00eaineres se tornaram populares porque eles fornecem benef\u00edcios extra, tais como: Cria\u00e7\u00e3o e implanta\u00e7\u00e3o \u00e1gil de aplica\u00e7\u00f5es: aumento da facilidade e efici\u00eancia na cria\u00e7\u00e3o de imagem de cont\u00eainer comparado ao uso de imagem de VM. Desenvolvimento, integra\u00e7\u00e3o e implanta\u00e7\u00e3o cont\u00ednuos: fornece capacidade de cria\u00e7\u00e3o e de implanta\u00e7\u00e3o de imagens de cont\u00eainer de forma confi\u00e1vel e frequente, com a funcionalidade de efetuar revers\u00f5es r\u00e1pidas e eficientes (devido \u00e0 imutabilidade da imagem). Separa\u00e7\u00e3o de interesses entre Desenvolvimento e Opera\u00e7\u00f5es: crie imagens de cont\u00eaineres de aplica\u00e7\u00f5es no momento de constru\u00e7\u00e3o/libera\u00e7\u00e3o em vez de no momento de implanta\u00e7\u00e3o, desacoplando as aplica\u00e7\u00f5es da infraestrutura. A capacidade de observa\u00e7\u00e3o (Observabilidade) n\u00e3o apenas apresenta informa\u00e7\u00f5es e m\u00e9tricas no n\u00edvel do sistema operacional, mas tamb\u00e9m a integridade da aplica\u00e7\u00e3o e outros sinais. Consist\u00eancia ambiental entre desenvolvimento, teste e produ\u00e7\u00e3o: funciona da mesma forma em um laptop e na nuvem. Portabilidade de distribui\u00e7\u00e3o de nuvem e sistema operacional: executa no Ubuntu, RHEL, CoreOS, localmente, nas principais nuvens p\u00fablicas e em qualquer outro lugar. Gerenciamento centrado em aplica\u00e7\u00f5es: eleva o n\u00edvel de abstra\u00e7\u00e3o da execu\u00e7\u00e3o em um sistema operacional em hardware virtualizado \u00e0 execu\u00e7\u00e3o de uma aplica\u00e7\u00e3o em um sistema operacional usando recursos l\u00f3gicos. Microservi\u00e7os fracamente acoplados, distribu\u00eddos, el\u00e1sticos e livres: as aplica\u00e7\u00f5es s\u00e3o divididas em partes menores e independentes e podem ser implantados e gerenciados dinamicamente - n\u00e3o uma pilha monol\u00edtica em execu\u00e7\u00e3o em uma grande m\u00e1quina de prop\u00f3sito \u00fanico. Isolamento de recursos: desempenho previs\u00edvel de aplica\u00e7\u00f5es. Utiliza\u00e7\u00e3o de recursos: alta efici\u00eancia e densidade J\u00e1 que sabemos a diferen\u00e7a entre VM e Cont\u00eaineres, precisamos entender aonde essa solu\u00e7\u00e3o se encaixa, pois, com as mudan\u00e7as que a tecnologia traz no decorrer da sua hist\u00f3ria, \u00e9 preciso entender as novas pr\u00e1ticas de TI com as tradicionais para chegamos a um ponto-chave dessa pergunta: As novas pr\u00e1ticas de TI (desenvolvimento nativo em nuvem, CI/CD e DevOps) existem gra\u00e7as \u00e0 divis\u00e3o das cargas de trabalho nas menores unidades \u00fateis poss\u00edveis, que geralmente s\u00e3o uma fun\u00e7\u00e3o ou um microsservi\u00e7o. Essas unidades s\u00e3o melhor empacotadas em cont\u00eaineres. Assim, v\u00e1rias equipes podem trabalhar em partes separadas de uma aplica\u00e7\u00e3o ou servi\u00e7o sem interromper, ou p\u00f4r em risco o c\u00f3digo empacotado em outros cont\u00eaineres. Nas arquiteturas de TI tradicionais (monol\u00edticas e legadas), todos os elementos de uma carga de trabalho s\u00e3o mantidos em um arquivo grande que n\u00e3o pode ser dividido. Por isso, ele precisa ser empacotado como uma unidade completa em um ambiente maior, frequentemente uma m\u00e1quina virtual. Era comum criar e executar uma aplica\u00e7\u00e3o inteira dentro de uma m\u00e1quina virtual, mesmo sabendo que ao armazenar todo o c\u00f3digo e depend\u00eancias, ela ficava grande demais, e isso poderia gerar falhas em cascata e downtime durante as atualiza\u00e7\u00f5es.","title":"Beneficios de usar Cont\u00eaineres"},{"location":"#se-aprofundando-um-pouco-mais-nos-conteineres","text":"Se em algum momento voc\u00ea j\u00e1 usou uma VM, imagino que tenha passado pelo processo de baixa uma SO, seja Linux ou Windows e alocando os recursos conforme a capacidade do hardware dispon\u00edvel, ent\u00e3o esse processo dos Cont\u00eaineres pode parecer meio confuso, para isso deixei duas perguntas que precisamos responder antes de cont\u00ednua. Se n\u00e3o \u00e9 uma virtualiza\u00e7\u00e3o como ocorre o isolamento? Como ocorre a divis\u00e3o de recursos?","title":"Se aprofundando um pouco mais nos Cont\u00eaineres"},{"location":"#se-nao-e-uma-virtualizacao-como-ocorre-o-isolamento","text":"Existe uma palavra importante, que resolve essa quest\u00e3o Namespaces , permitindo que os cont\u00eaineres consigam isolamento em determinados n\u00edveis que s\u00e3o: PID : Isola os processos rodando dentro do cont\u00eainer NET : Isola as interfaces de redes IPC : Isola a comunica\u00e7\u00e3o entre processos e memoria MNT : Isola o ponto de montagem UTS : Isola o Kernel, ou seja, simula um novo host(usu\u00e1rio) USER : Isola os arquivos do sistema Dentre esse processo o mais importante, \u00e9 o UTS, o respons\u00e1vel pelo isolamento dos cont\u00eaineres, ao simular um novo host, ele usar o Kernel da m\u00e1quina f\u00edsica para o processamento, por\u00e9m isoladamente. Os namespaces fazem parte do kernel do Linux desde 2002, os namespaces s\u00e3o respons\u00e1veis por gerar o isolamento de grupos de processos em seu n\u00edvel l\u00f3gico.","title":"Se n\u00e3o \u00e9 uma virtualiza\u00e7\u00e3o como ocorre o isolamento?"},{"location":"#como-ocorre-a-divisao-de-recursos","text":"O gerenciamento de recursos, tem a palavra Cgroups para resolver essa demanda de gerenciar, seja ela automaticamente ou manualmente. Os cgroups fornecem os recursos: Limites de recursos : voc\u00ea pode configurar um cgroup para limitar quanto de um determinado recurso (mem\u00f3ria ou CPU, por exemplo) um processo pode usar. Prioriza\u00e7\u00e3o : Voc\u00ea pode controlar quanto de um recurso (CPU, disco ou rede) um processo pode usar em compara\u00e7\u00e3o com processos em outro cgroup quando h\u00e1 conten\u00e7\u00e3o de recursos. Contabilidade : os limites de recursos s\u00e3o monitorados e relatados no n\u00edvel do cgroup. Controle : Voc\u00ea pode alterar o status (congelado, interrompido ou reiniciado) de todos os processos em um cgroup com um \u00fanico comando. Os cgroups s\u00e3o um componente-chave dos cont\u00eaineres porque geralmente h\u00e1 v\u00e1rios processos em execu\u00e7\u00e3o em um cont\u00eainer que precisam ser controlados juntos. Agora que entendemos o significado de cont\u00eaineres, temos mais duas palavras importantes para aprender: Kubernetes Swarm Essas duas palavras possuem uma frase em comum: orquestra\u00e7\u00e3o de containers","title":"Como ocorre a divis\u00e3o de recursos?"},{"location":"#para-que-serve-a-orquestracao-de-conteineres","text":"Basicamente, a orquestra\u00e7\u00e3o de cont\u00eaineres automatiza a implanta\u00e7\u00e3o, o gerenciamento, a escala e a rede dos cont\u00eaineres. Use a orquestra\u00e7\u00e3o de cont\u00eaineres para automatizar e gerenciar tarefas como: Provisionamento e implanta\u00e7\u00e3o Configura\u00e7\u00e3o e programa\u00e7\u00e3o Aloca\u00e7\u00e3o de recursos Disponibilidade dos containers Escala ou remo\u00e7\u00e3o de containers com base no balanceamento de cargas de trabalho na infraestrutura Balanceamento de carga e roteamento de tr\u00e1fego Monitoramento da integridade do container Configura\u00e7\u00e3o da aplica\u00e7\u00e3o com base no container em que ela ser\u00e1 executada Prote\u00e7\u00e3o das intera\u00e7\u00f5es entre os containers As ferramentas de orquestra\u00e7\u00e3o de cont\u00eaineres fornecem um framework para gerenciar arquiteturas de microsservi\u00e7os e cont\u00eaineres em escala. Muitas delas podem ser usadas no gerenciamento do ciclo de vida dos cont\u00eaineres. Algumas op\u00e7\u00f5es s\u00e3o o Kubernetes, Docker Swarm e Apache Mesos, DC/SO e N\u00f4made. Nesse tema vamos aborda sobre e praticar: Docker 100% Docker Swarm: Cluster do Docker 100% Usar o Vagrant para cria\u00e7\u00e3o da infraestrutura do Swarm 0% Kubernetes 60% Amazon Kubernetes 0% Azure Kubernetes 0% GCP Kubernetes 0%","title":"Para que serve a orquestra\u00e7\u00e3o de cont\u00eaineres?"},{"location":"#fontes-usadas-para-elaboracao-desse-conteudo","text":"Red Hat: Cont\u00eaineres vs Vms Red Hat: Orquestra\u00e7\u00e3o de containers etcd: O que s\u00e3o namespaces e cgroups","title":"Fontes usadas para elabora\u00e7\u00e3o desse conte\u00fado"},{"location":"Kubernetes/deployment/","text":"Deployment \u00b6 Voc\u00ea descreve um estado desejado em um Deployment e a Deployment controla e altera o estado real para o estado desejado a uma taxa controlada. Voc\u00ea pode definir Deployment para criar novos ReplicaSets ou remover Deployment existentes e adotar todos os seus recursos com novas Deployment. Bem, para simplificar iremos criar um .yaml e passar no kind o Deployment, que vai criar um versionamento para a aplica\u00e7\u00e3o, por\u00e9m ele engloba o ReplicaSets, passando o a quantidade de replicas. Para simplicar ainda mais \u00e9 praticamente um ReplicaSet com versionamento. Crie o arquivo .yaml e vamos executar: apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 Execute em seguida: kubectl apply -f deployment.yaml Para verificar: kubectl get deployment Versionamento \u00b6 Com o versionamento podemos, criar vers\u00f5es e depois podemos retorna se houve necessidade Execute Saida kubectl rollout history deployment Se houver mais deployment basta passa o nome no final para especificar. REVISION CHANGE-CAUSE 1 <none> Aqui no nosso historico est\u00e1 em none na vers\u00e3o 1 Vou alterar a vers\u00e3o do nginx:1.14.2 para nginx:1 com o set. kubectl set image deployment/nginx-deployment nginx = nginx:1 Em seguida vamos repetir o processo para verificar a revis\u00e3o. Execute Saida kubectl rollout history deployment Se houver mais deployment basta passa o nome no final para especificar. REVISION CHANGE-CAUSE 1 <none> 2 <none> Aqui no nosso historico agora com a vers\u00e3o 2 Que tal mudamos o nome das altera\u00e7\u00f5es que fizemos? Usando o annotate kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause = \"altera\u00e7\u00e3o da imagem para a vers\u00e3o 1\" E a saida do kubectl rollout history deployment agora est\u00e1 com a altera\u00e7\u00e3o: REVISION CHANGE-CAUSE 1 <none> 2 altera\u00e7\u00e3o da imagem para a vers\u00e3o 1 E para retorna para alguma vers\u00e3o especifica: kubectl rollout undo deployment/nginx-deployment --to-revision = 1 Escalando a quantidade de replicas \u00b6 Para escalar \u00e9 bem simples: kubectl scale deployment/nginx-deployment --replicas = 10 Usando o scale e com a flag --replicas definimos a quantidade de replicas.","title":"Deployment"},{"location":"Kubernetes/deployment/#deployment","text":"Voc\u00ea descreve um estado desejado em um Deployment e a Deployment controla e altera o estado real para o estado desejado a uma taxa controlada. Voc\u00ea pode definir Deployment para criar novos ReplicaSets ou remover Deployment existentes e adotar todos os seus recursos com novas Deployment. Bem, para simplificar iremos criar um .yaml e passar no kind o Deployment, que vai criar um versionamento para a aplica\u00e7\u00e3o, por\u00e9m ele engloba o ReplicaSets, passando o a quantidade de replicas. Para simplicar ainda mais \u00e9 praticamente um ReplicaSet com versionamento. Crie o arquivo .yaml e vamos executar: apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 Execute em seguida: kubectl apply -f deployment.yaml Para verificar: kubectl get deployment","title":"Deployment"},{"location":"Kubernetes/deployment/#versionamento","text":"Com o versionamento podemos, criar vers\u00f5es e depois podemos retorna se houve necessidade Execute Saida kubectl rollout history deployment Se houver mais deployment basta passa o nome no final para especificar. REVISION CHANGE-CAUSE 1 <none> Aqui no nosso historico est\u00e1 em none na vers\u00e3o 1 Vou alterar a vers\u00e3o do nginx:1.14.2 para nginx:1 com o set. kubectl set image deployment/nginx-deployment nginx = nginx:1 Em seguida vamos repetir o processo para verificar a revis\u00e3o. Execute Saida kubectl rollout history deployment Se houver mais deployment basta passa o nome no final para especificar. REVISION CHANGE-CAUSE 1 <none> 2 <none> Aqui no nosso historico agora com a vers\u00e3o 2 Que tal mudamos o nome das altera\u00e7\u00f5es que fizemos? Usando o annotate kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause = \"altera\u00e7\u00e3o da imagem para a vers\u00e3o 1\" E a saida do kubectl rollout history deployment agora est\u00e1 com a altera\u00e7\u00e3o: REVISION CHANGE-CAUSE 1 <none> 2 altera\u00e7\u00e3o da imagem para a vers\u00e3o 1 E para retorna para alguma vers\u00e3o especifica: kubectl rollout undo deployment/nginx-deployment --to-revision = 1","title":"Versionamento"},{"location":"Kubernetes/deployment/#escalando-a-quantidade-de-replicas","text":"Para escalar \u00e9 bem simples: kubectl scale deployment/nginx-deployment --replicas = 10 Usando o scale e com a flag --replicas definimos a quantidade de replicas.","title":"Escalando a quantidade de replicas"},{"location":"Kubernetes/instala%C3%A7%C3%A3o/","text":"Instala\u00e7\u00e3o \u00b6 Para instalar o kubernete: Windows Linux Mac Siga o passo a passo da documenta\u00e7\u00e3o Siga o passo a passo da documenta\u00e7\u00e3o Siga o passo a passo da documenta\u00e7\u00e3o Precisamos instalar uma ferramenta para ajudar a criar essa infraestrutura localmente, temos algumas op\u00e7\u00f5es que a documenta\u00e7\u00e3o sugere que s\u00e3o o kind , minikube e kubeadm , vamos instalar somente o minikube para realizar nossos estudos. Na documenta\u00e7\u00e3o basta verificar o passo a passo para sua OS e fazer a instala\u00e7\u00e3o. Para iniciar o minikube : minikube start Ele vai dizer que est\u00e1 faltando um driver , se voc\u00ea j\u00e1 tiver o docker instalado facilita o processo, mas se n\u00e3o basta instalar uns dos drives que ele recomenda para sua OS. Vou usar o docker e setar ele para usar como default: minikube config set driver docker Ele solicitou para d\u00e1 um start no docker daemon: sudo systemctl start docker Como isso basta, iniciar novamente o minikube , vamos testar o kubectl get nodes : NAME STATUS ROLES AGE VERSION minikube Ready control-plane,master 3m54s v1.23.3 Lembrando que, futuramente vou criar cluster em servi\u00e7os de nuvem, como o AWS, GCP, Azure e talvez na Oracle. Mas nesse momento, vamos focar em kubernetes e como funciona toda essa estrutura, pratica e teoricamente.","title":"Instala\u00e7\u00e3o"},{"location":"Kubernetes/instala%C3%A7%C3%A3o/#instalacao","text":"Para instalar o kubernete: Windows Linux Mac Siga o passo a passo da documenta\u00e7\u00e3o Siga o passo a passo da documenta\u00e7\u00e3o Siga o passo a passo da documenta\u00e7\u00e3o Precisamos instalar uma ferramenta para ajudar a criar essa infraestrutura localmente, temos algumas op\u00e7\u00f5es que a documenta\u00e7\u00e3o sugere que s\u00e3o o kind , minikube e kubeadm , vamos instalar somente o minikube para realizar nossos estudos. Na documenta\u00e7\u00e3o basta verificar o passo a passo para sua OS e fazer a instala\u00e7\u00e3o. Para iniciar o minikube : minikube start Ele vai dizer que est\u00e1 faltando um driver , se voc\u00ea j\u00e1 tiver o docker instalado facilita o processo, mas se n\u00e3o basta instalar uns dos drives que ele recomenda para sua OS. Vou usar o docker e setar ele para usar como default: minikube config set driver docker Ele solicitou para d\u00e1 um start no docker daemon: sudo systemctl start docker Como isso basta, iniciar novamente o minikube , vamos testar o kubectl get nodes : NAME STATUS ROLES AGE VERSION minikube Ready control-plane,master 3m54s v1.23.3 Lembrando que, futuramente vou criar cluster em servi\u00e7os de nuvem, como o AWS, GCP, Azure e talvez na Oracle. Mas nesse momento, vamos focar em kubernetes e como funciona toda essa estrutura, pratica e teoricamente.","title":"Instala\u00e7\u00e3o"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/","text":"O que \u00e9 Kubernetes? \u00b6 Para acessar o repositorio Github Kubernetes e o Kubernetes.io O Google tornou Kubernetes um projeto de c\u00f3digo-aberto em 2014. O Kubernetes combina mais de 15 anos de experi\u00eancia do Google executando cargas de trabalho produtivas em escala, com as melhores id\u00e9ias e pr\u00e1ticas da comunidade. Kubernetes \u00e9 um plataforma de c\u00f3digo aberto, port\u00e1vel e extensiva para o gerenciamento de cargas de trabalho e servi\u00e7os distribu\u00eddos em cont\u00eaineres, que facilita tanto a configura\u00e7\u00e3o declarativa quanto a automa\u00e7\u00e3o. Ele possui um ecossistema grande, e de r\u00e1pido crescimento. Servi\u00e7os, suporte, e ferramentas para Kubernetes est\u00e3o amplamente dispon\u00edveis. O Kubernetes veio com grande experi\u00eancia da Google e n\u00e3o para de crescer nas funcionalidades e usu\u00e1rios. \u00c9 ele quem gerencia os containers em execu\u00e7\u00e3o e por isso ele tamb\u00e9m \u00e9 chamado de Orquestrador de Containers. Atrav\u00e9s dele podemos definir o estado de um sistema completo, por exemplo baseado em Microservices, seguindo boas pr\u00e1ticas de infraestrutura como c\u00f3digo, permitindo balanceamento de carga, alta disponibilidade, atualiza\u00e7\u00f5es em lote e rollbacks e muito muito mais. Hoje em dia os grandes provedores de nuvem como Azure, AWS, IBM, Red Hat, Oracle ou Google d\u00e3o suporte ao Kubernetes. Al\u00e9m disso, existe uma implementa\u00e7\u00e3o local chamada de Minikube que simula um cluster Kubernetes, ideal para testes e estudos. O mais legal \u00e9 que as configura\u00e7\u00f5es locais, que definem o estado da aplica\u00e7\u00e3o, tamb\u00e9m rodam no Kubernetes na nuvem. Ou seja, podemos testar o orquestrador local usando Minikube e depois publicar o sistema no AWS ou Azure, apenas com poucas altera\u00e7\u00f5es. Por que voc\u00ea precisa do Kubernetes e o que ele pode fazer \u00b6 Os cont\u00eaineres s\u00e3o uma boa maneira de agrupar e executar suas aplica\u00e7\u00f5es. Em um ambiente de produ\u00e7\u00e3o, voc\u00ea precisa gerenciar os cont\u00eaineres que executam as aplica\u00e7\u00f5es e garantir que n\u00e3o haja tempo de inatividade. Por exemplo, se um cont\u00eainer cair, outro cont\u00eainer precisa ser iniciado. N\u00e3o seria mais f\u00e1cil se esse comportamento fosse controlado por um sistema? \u00c9 assim que o Kubernetes vem ao resgate! O Kubernetes oferece uma estrutura para executar sistemas distribu\u00eddos de forma resiliente. Ele cuida do escalonamento e do recupera\u00e7\u00e3o \u00e0 falha de sua aplica\u00e7\u00e3o, fornece padr\u00f5es de implanta\u00e7\u00e3o e muito mais. Por exemplo, o Kubernetes pode gerenciar facilmente uma implanta\u00e7\u00e3o no m\u00e9todo can\u00e1rio para seu sistema. O Kubernetes oferece a voc\u00ea: Descoberta de servi\u00e7o e balanceamento de carga O Kubernetes pode expor um cont\u00eainer usando o nome DNS ou seu pr\u00f3prio endere\u00e7o IP. Se o tr\u00e1fego para um cont\u00eainer for alto, o Kubernetes pode balancear a carga e distribuir o tr\u00e1fego de rede para que a implanta\u00e7\u00e3o seja est\u00e1vel. Orquestra\u00e7\u00e3o de armazenamento O Kubernetes permite que voc\u00ea monte automaticamente um sistema de armazenamento de sua escolha, como armazenamentos locais, provedores de nuvem p\u00fablica e muito mais. Lan\u00e7amentos e revers\u00f5es automatizadas Voc\u00ea pode descrever o estado desejado para seus cont\u00eaineres implantados usando o Kubernetes, e ele pode alterar o estado real para o estado desejado em um ritmo controlada. Por exemplo, voc\u00ea pode automatizar o Kubernetes para criar novos cont\u00eaineres para sua implanta\u00e7\u00e3o, remover os cont\u00eaineres existentes e adotar todos os seus recursos para o novo cont\u00eainer. Empacotamento bin\u00e1rio autom\u00e1tico Voc\u00ea fornece ao Kubernetes um cluster de n\u00f3s que pode ser usado para executar tarefas nos cont\u00eaineres. Voc\u00ea informa ao Kubernetes de quanta CPU e mem\u00f3ria (RAM) cada cont\u00eainer precisa. O Kubernetes pode encaixar cont\u00eaineres em seus n\u00f3s para fazer o melhor uso de seus recursos. Autocorre\u00e7\u00e3o O Kubernetes reinicia os cont\u00eaineres que falham, substitui os cont\u00eaineres, elimina os cont\u00eaineres que n\u00e3o respondem \u00e0 verifica\u00e7\u00e3o de integridade definida pelo usu\u00e1rio e n\u00e3o os anuncia aos clientes at\u00e9 que estejam prontos para servir. Gerenciamento de configura\u00e7\u00e3o e de segredos O Kubernetes permite armazenar e gerenciar informa\u00e7\u00f5es confidenciais, como senhas, tokens OAuth e chaves SSH. Voc\u00ea pode implantar e atualizar segredos e configura\u00e7\u00e3o de aplica\u00e7\u00f5es sem reconstruir suas imagens de cont\u00eainer e sem expor segredos em sua pilha de configura\u00e7\u00e3o. O que o Kubernetes n\u00e3o \u00e9 \u00b6 O Kubernetes n\u00e3o \u00e9 um sistema PaaS (plataforma como servi\u00e7o) tradicional e completo. Como o Kubernetes opera no n\u00edvel do cont\u00eainer, e n\u00e3o no n\u00edvel do hardware, ele fornece alguns recursos geralmente aplic\u00e1veis comuns \u00e0s ofertas de PaaS, como implanta\u00e7\u00e3o, escalonamento, balanceamento de carga, e permite que os usu\u00e1rios integrem suas solu\u00e7\u00f5es de logging, monitoramento e alerta. No entanto, o Kubernetes n\u00e3o \u00e9 monol\u00edtico, e essas solu\u00e7\u00f5es padr\u00e3o s\u00e3o opcionais e conect\u00e1veis. O Kubernetes fornece os blocos de constru\u00e7\u00e3o para a constru\u00e7\u00e3o de plataformas de desenvolvimento, mas preserva a escolha e flexibilidade do usu\u00e1rio onde \u00e9 importante. Kubernetes: N\u00e3o limita os tipos de aplica\u00e7\u00f5es suportadas. O Kubernetes visa oferecer suporte a uma variedade extremamente diversa de cargas de trabalho, incluindo cargas de trabalho sem estado, com estado e de processamento de dados. Se uma aplica\u00e7\u00e3o puder ser executada em um cont\u00eainer, ele deve ser executado perfeitamente no Kubernetes. N\u00e3o implanta c\u00f3digo-fonte e n\u00e3o constr\u00f3i sua aplica\u00e7\u00e3o. Os fluxos de trabalho de integra\u00e7\u00e3o cont\u00ednua, entrega e implanta\u00e7\u00e3o (CI/CD) s\u00e3o determinados pelas culturas e prefer\u00eancias da organiza\u00e7\u00e3o, bem como pelos requisitos t\u00e9cnicos. N\u00e3o fornece servi\u00e7os em n\u00edvel de aplica\u00e7\u00e3o, tais como middleware (por exemplo, barramentos de mensagem), estruturas de processamento de dados (por exemplo, Spark), bancos de dados (por exemplo, MySQL), caches, nem sistemas de armazenamento em cluster (por exemplo, Ceph), como servi\u00e7os integrados. Esses componentes podem ser executados no Kubernetes e/ou podem ser acessados por aplica\u00e7\u00f5es executadas no Kubernetes por meio de mecanismos port\u00e1teis, como o Open Service Broker. N\u00e3o dita solu\u00e7\u00f5es de logging, monitoramento ou alerta. Ele fornece algumas integra\u00e7\u00f5es como prova de conceito e mecanismos para coletar e exportar m\u00e9tricas. N\u00e3o fornece nem exige um sistema/idioma de configura\u00e7\u00e3o (por exemplo, Jsonnet). Ele fornece uma API declarativa que pode ser direcionada por formas arbitr\u00e1rias de especifica\u00e7\u00f5es declarativas. N\u00e3o fornece nem adota sistemas abrangentes de configura\u00e7\u00e3o de m\u00e1quinas, manuten\u00e7\u00e3o, gerenciamento ou autocorre\u00e7\u00e3o. Adicionalmente, o Kubernetes n\u00e3o \u00e9 um mero sistema de orquestra\u00e7\u00e3o. Na verdade, ele elimina a necessidade de orquestra\u00e7\u00e3o. A defini\u00e7\u00e3o t\u00e9cnica de orquestra\u00e7\u00e3o \u00e9 a execu\u00e7\u00e3o de um fluxo de trabalho definido: primeiro fa\u00e7a A, depois B e depois C. Em contraste, o Kubernetes compreende um conjunto de processos de controle independentes e combin\u00e1veis que conduzem continuamente o estado atual em dire\u00e7\u00e3o ao estado desejado fornecido. N\u00e3o importa como voc\u00ea vai de A para C. O controle centralizado tamb\u00e9m n\u00e3o \u00e9 necess\u00e1rio. Isso resulta em um sistema que \u00e9 mais f\u00e1cil de usar e mais poderoso, robusto, resiliente e extens\u00edvel. Kubernetes: como ele funciona? \u00b6 A principal vantagem que as empresas garantem ao usar o Kubernetes, especialmente se estiverem otimizando o desenvolvimento de aplica\u00e7\u00f5es para a cloud, \u00e9 que elas ter\u00e3o uma plataforma para programar e executar containers em clusters de m\u00e1quinas f\u00edsicas ou virtuais. Em termos mais abrangentes, com o Kubernetes, \u00e9 mais f\u00e1cil implementar e confiar totalmente em uma infraestrutura baseada em containers para os ambientes de produ\u00e7\u00e3o. Como o prop\u00f3sito do Kubernetes \u00e9 automatizar completamente as tarefas operacionais, ele permite que os containers realizem muitas das tarefas possibilitadas por outros sistemas de gerenciamento ou plataformas de aplica\u00e7\u00f5es. O Kubernetes possibilita: Orquestrar containers em v\u00e1rios hosts. Aproveitar melhor o hardware para maximizar os recursos necess\u00e1rios na execu\u00e7\u00e3o das aplica\u00e7\u00f5es corporativas. Controlar e automatizar as implanta\u00e7\u00f5es e atualiza\u00e7\u00f5es de aplica\u00e7\u00f5es. Montar e adicionar armazenamento para executar aplica\u00e7\u00f5es com monitora\u00e7\u00e3o de estado. Escalar rapidamente as aplica\u00e7\u00f5es em containers e recursos relacionados. Gerenciar servi\u00e7os de forma declarativa, garantindo que as aplica\u00e7\u00f5es sejam executadas sempre da mesma maneira como foram implantadas. Verificar a integridade e autorrecupera\u00e7\u00e3o das aplica\u00e7\u00f5es com posicionamento, rein\u00edcio, replica\u00e7\u00e3o e escalonamento autom\u00e1ticos. No entanto, o Kubernetes depende de outros projetos para oferecer plenamente esses servi\u00e7os orquestrados. Com a inclus\u00e3o de outros projetos open source, \u00e9 poss\u00edvel atingir a capacidade total do Kubernetes. Dentre esses projetos necess\u00e1rios, incluem-se: Registro, como o Atomic Registry ou o Docker Registry. Rede, como o OpenvSwitch e roteamento de borda inteligente. Telemetria, como o heapster, o kibana, o hawkular e o elastic. Seguran\u00e7a, como o LDAP, o SELinux, o RBAC e o OAUTH com camadas de multiloca\u00e7\u00e3o. Automa\u00e7\u00e3o, com a adi\u00e7\u00e3o de playbooks do Ansible para a instala\u00e7\u00e3o e o gerenciamento do ciclo de vida do cluster. Servi\u00e7os, oferecidos em um cat\u00e1logo variado de conte\u00fados previamente criados de padr\u00f5es de aplica\u00e7\u00f5es populares. Como ele se encaixa na infraestrutura? \u00b6 O Kubernetes \u00e9 executado em um sistema operacional e interage com pods de containers executados em n\u00f3s. A m\u00e1quina mestre do Kubernetes aceita os comandos de um administrador (ou equipe de DevOps) e retransmite essas instru\u00e7\u00f5es aos n\u00f3s subservientes. Essa retransmiss\u00e3o \u00e9 realizada em conjunto com v\u00e1rios servi\u00e7os para automaticamente decidir qual n\u00f3 \u00e9 o mais adequado para a tarefa. Depois, s\u00e3o alocados os recursos e atribu\u00eddos os pods do n\u00f3 para cumprir a tarefa solicitada. Portanto, do ponto de vista da infraestrutura, s\u00e3o poucas as mudan\u00e7as em compara\u00e7\u00e3o com a forma como voc\u00ea j\u00e1 gerencia os containers. O controle sobre os containers acontece em um n\u00edvel superior, tornando-o mais refinado, sem a necessidade de microgerenciar cada container ou n\u00f3 separadamente. Ser\u00e1 necess\u00e1rio realizar algum trabalho, mas em sua maioria trata-se somente de uma quest\u00e3o de atribuir um master do Kubernetes e definir os n\u00f3s e pods. Fontes usadas para elabora\u00e7\u00e3o desse conte\u00fado \u00b6 Documenta\u00e7\u00e3o Kubernete Red Hat","title":"O que \u00e9 Kubernetes"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#o-que-e-kubernetes","text":"Para acessar o repositorio Github Kubernetes e o Kubernetes.io O Google tornou Kubernetes um projeto de c\u00f3digo-aberto em 2014. O Kubernetes combina mais de 15 anos de experi\u00eancia do Google executando cargas de trabalho produtivas em escala, com as melhores id\u00e9ias e pr\u00e1ticas da comunidade. Kubernetes \u00e9 um plataforma de c\u00f3digo aberto, port\u00e1vel e extensiva para o gerenciamento de cargas de trabalho e servi\u00e7os distribu\u00eddos em cont\u00eaineres, que facilita tanto a configura\u00e7\u00e3o declarativa quanto a automa\u00e7\u00e3o. Ele possui um ecossistema grande, e de r\u00e1pido crescimento. Servi\u00e7os, suporte, e ferramentas para Kubernetes est\u00e3o amplamente dispon\u00edveis. O Kubernetes veio com grande experi\u00eancia da Google e n\u00e3o para de crescer nas funcionalidades e usu\u00e1rios. \u00c9 ele quem gerencia os containers em execu\u00e7\u00e3o e por isso ele tamb\u00e9m \u00e9 chamado de Orquestrador de Containers. Atrav\u00e9s dele podemos definir o estado de um sistema completo, por exemplo baseado em Microservices, seguindo boas pr\u00e1ticas de infraestrutura como c\u00f3digo, permitindo balanceamento de carga, alta disponibilidade, atualiza\u00e7\u00f5es em lote e rollbacks e muito muito mais. Hoje em dia os grandes provedores de nuvem como Azure, AWS, IBM, Red Hat, Oracle ou Google d\u00e3o suporte ao Kubernetes. Al\u00e9m disso, existe uma implementa\u00e7\u00e3o local chamada de Minikube que simula um cluster Kubernetes, ideal para testes e estudos. O mais legal \u00e9 que as configura\u00e7\u00f5es locais, que definem o estado da aplica\u00e7\u00e3o, tamb\u00e9m rodam no Kubernetes na nuvem. Ou seja, podemos testar o orquestrador local usando Minikube e depois publicar o sistema no AWS ou Azure, apenas com poucas altera\u00e7\u00f5es.","title":"O que \u00e9 Kubernetes?"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#por-que-voce-precisa-do-kubernetes-e-o-que-ele-pode-fazer","text":"Os cont\u00eaineres s\u00e3o uma boa maneira de agrupar e executar suas aplica\u00e7\u00f5es. Em um ambiente de produ\u00e7\u00e3o, voc\u00ea precisa gerenciar os cont\u00eaineres que executam as aplica\u00e7\u00f5es e garantir que n\u00e3o haja tempo de inatividade. Por exemplo, se um cont\u00eainer cair, outro cont\u00eainer precisa ser iniciado. N\u00e3o seria mais f\u00e1cil se esse comportamento fosse controlado por um sistema? \u00c9 assim que o Kubernetes vem ao resgate! O Kubernetes oferece uma estrutura para executar sistemas distribu\u00eddos de forma resiliente. Ele cuida do escalonamento e do recupera\u00e7\u00e3o \u00e0 falha de sua aplica\u00e7\u00e3o, fornece padr\u00f5es de implanta\u00e7\u00e3o e muito mais. Por exemplo, o Kubernetes pode gerenciar facilmente uma implanta\u00e7\u00e3o no m\u00e9todo can\u00e1rio para seu sistema. O Kubernetes oferece a voc\u00ea: Descoberta de servi\u00e7o e balanceamento de carga O Kubernetes pode expor um cont\u00eainer usando o nome DNS ou seu pr\u00f3prio endere\u00e7o IP. Se o tr\u00e1fego para um cont\u00eainer for alto, o Kubernetes pode balancear a carga e distribuir o tr\u00e1fego de rede para que a implanta\u00e7\u00e3o seja est\u00e1vel. Orquestra\u00e7\u00e3o de armazenamento O Kubernetes permite que voc\u00ea monte automaticamente um sistema de armazenamento de sua escolha, como armazenamentos locais, provedores de nuvem p\u00fablica e muito mais. Lan\u00e7amentos e revers\u00f5es automatizadas Voc\u00ea pode descrever o estado desejado para seus cont\u00eaineres implantados usando o Kubernetes, e ele pode alterar o estado real para o estado desejado em um ritmo controlada. Por exemplo, voc\u00ea pode automatizar o Kubernetes para criar novos cont\u00eaineres para sua implanta\u00e7\u00e3o, remover os cont\u00eaineres existentes e adotar todos os seus recursos para o novo cont\u00eainer. Empacotamento bin\u00e1rio autom\u00e1tico Voc\u00ea fornece ao Kubernetes um cluster de n\u00f3s que pode ser usado para executar tarefas nos cont\u00eaineres. Voc\u00ea informa ao Kubernetes de quanta CPU e mem\u00f3ria (RAM) cada cont\u00eainer precisa. O Kubernetes pode encaixar cont\u00eaineres em seus n\u00f3s para fazer o melhor uso de seus recursos. Autocorre\u00e7\u00e3o O Kubernetes reinicia os cont\u00eaineres que falham, substitui os cont\u00eaineres, elimina os cont\u00eaineres que n\u00e3o respondem \u00e0 verifica\u00e7\u00e3o de integridade definida pelo usu\u00e1rio e n\u00e3o os anuncia aos clientes at\u00e9 que estejam prontos para servir. Gerenciamento de configura\u00e7\u00e3o e de segredos O Kubernetes permite armazenar e gerenciar informa\u00e7\u00f5es confidenciais, como senhas, tokens OAuth e chaves SSH. Voc\u00ea pode implantar e atualizar segredos e configura\u00e7\u00e3o de aplica\u00e7\u00f5es sem reconstruir suas imagens de cont\u00eainer e sem expor segredos em sua pilha de configura\u00e7\u00e3o.","title":"Por que voc\u00ea precisa do Kubernetes e o que ele pode fazer"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#o-que-o-kubernetes-nao-e","text":"O Kubernetes n\u00e3o \u00e9 um sistema PaaS (plataforma como servi\u00e7o) tradicional e completo. Como o Kubernetes opera no n\u00edvel do cont\u00eainer, e n\u00e3o no n\u00edvel do hardware, ele fornece alguns recursos geralmente aplic\u00e1veis comuns \u00e0s ofertas de PaaS, como implanta\u00e7\u00e3o, escalonamento, balanceamento de carga, e permite que os usu\u00e1rios integrem suas solu\u00e7\u00f5es de logging, monitoramento e alerta. No entanto, o Kubernetes n\u00e3o \u00e9 monol\u00edtico, e essas solu\u00e7\u00f5es padr\u00e3o s\u00e3o opcionais e conect\u00e1veis. O Kubernetes fornece os blocos de constru\u00e7\u00e3o para a constru\u00e7\u00e3o de plataformas de desenvolvimento, mas preserva a escolha e flexibilidade do usu\u00e1rio onde \u00e9 importante. Kubernetes: N\u00e3o limita os tipos de aplica\u00e7\u00f5es suportadas. O Kubernetes visa oferecer suporte a uma variedade extremamente diversa de cargas de trabalho, incluindo cargas de trabalho sem estado, com estado e de processamento de dados. Se uma aplica\u00e7\u00e3o puder ser executada em um cont\u00eainer, ele deve ser executado perfeitamente no Kubernetes. N\u00e3o implanta c\u00f3digo-fonte e n\u00e3o constr\u00f3i sua aplica\u00e7\u00e3o. Os fluxos de trabalho de integra\u00e7\u00e3o cont\u00ednua, entrega e implanta\u00e7\u00e3o (CI/CD) s\u00e3o determinados pelas culturas e prefer\u00eancias da organiza\u00e7\u00e3o, bem como pelos requisitos t\u00e9cnicos. N\u00e3o fornece servi\u00e7os em n\u00edvel de aplica\u00e7\u00e3o, tais como middleware (por exemplo, barramentos de mensagem), estruturas de processamento de dados (por exemplo, Spark), bancos de dados (por exemplo, MySQL), caches, nem sistemas de armazenamento em cluster (por exemplo, Ceph), como servi\u00e7os integrados. Esses componentes podem ser executados no Kubernetes e/ou podem ser acessados por aplica\u00e7\u00f5es executadas no Kubernetes por meio de mecanismos port\u00e1teis, como o Open Service Broker. N\u00e3o dita solu\u00e7\u00f5es de logging, monitoramento ou alerta. Ele fornece algumas integra\u00e7\u00f5es como prova de conceito e mecanismos para coletar e exportar m\u00e9tricas. N\u00e3o fornece nem exige um sistema/idioma de configura\u00e7\u00e3o (por exemplo, Jsonnet). Ele fornece uma API declarativa que pode ser direcionada por formas arbitr\u00e1rias de especifica\u00e7\u00f5es declarativas. N\u00e3o fornece nem adota sistemas abrangentes de configura\u00e7\u00e3o de m\u00e1quinas, manuten\u00e7\u00e3o, gerenciamento ou autocorre\u00e7\u00e3o. Adicionalmente, o Kubernetes n\u00e3o \u00e9 um mero sistema de orquestra\u00e7\u00e3o. Na verdade, ele elimina a necessidade de orquestra\u00e7\u00e3o. A defini\u00e7\u00e3o t\u00e9cnica de orquestra\u00e7\u00e3o \u00e9 a execu\u00e7\u00e3o de um fluxo de trabalho definido: primeiro fa\u00e7a A, depois B e depois C. Em contraste, o Kubernetes compreende um conjunto de processos de controle independentes e combin\u00e1veis que conduzem continuamente o estado atual em dire\u00e7\u00e3o ao estado desejado fornecido. N\u00e3o importa como voc\u00ea vai de A para C. O controle centralizado tamb\u00e9m n\u00e3o \u00e9 necess\u00e1rio. Isso resulta em um sistema que \u00e9 mais f\u00e1cil de usar e mais poderoso, robusto, resiliente e extens\u00edvel.","title":"O que o Kubernetes n\u00e3o \u00e9"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#kubernetes-como-ele-funciona","text":"A principal vantagem que as empresas garantem ao usar o Kubernetes, especialmente se estiverem otimizando o desenvolvimento de aplica\u00e7\u00f5es para a cloud, \u00e9 que elas ter\u00e3o uma plataforma para programar e executar containers em clusters de m\u00e1quinas f\u00edsicas ou virtuais. Em termos mais abrangentes, com o Kubernetes, \u00e9 mais f\u00e1cil implementar e confiar totalmente em uma infraestrutura baseada em containers para os ambientes de produ\u00e7\u00e3o. Como o prop\u00f3sito do Kubernetes \u00e9 automatizar completamente as tarefas operacionais, ele permite que os containers realizem muitas das tarefas possibilitadas por outros sistemas de gerenciamento ou plataformas de aplica\u00e7\u00f5es. O Kubernetes possibilita: Orquestrar containers em v\u00e1rios hosts. Aproveitar melhor o hardware para maximizar os recursos necess\u00e1rios na execu\u00e7\u00e3o das aplica\u00e7\u00f5es corporativas. Controlar e automatizar as implanta\u00e7\u00f5es e atualiza\u00e7\u00f5es de aplica\u00e7\u00f5es. Montar e adicionar armazenamento para executar aplica\u00e7\u00f5es com monitora\u00e7\u00e3o de estado. Escalar rapidamente as aplica\u00e7\u00f5es em containers e recursos relacionados. Gerenciar servi\u00e7os de forma declarativa, garantindo que as aplica\u00e7\u00f5es sejam executadas sempre da mesma maneira como foram implantadas. Verificar a integridade e autorrecupera\u00e7\u00e3o das aplica\u00e7\u00f5es com posicionamento, rein\u00edcio, replica\u00e7\u00e3o e escalonamento autom\u00e1ticos. No entanto, o Kubernetes depende de outros projetos para oferecer plenamente esses servi\u00e7os orquestrados. Com a inclus\u00e3o de outros projetos open source, \u00e9 poss\u00edvel atingir a capacidade total do Kubernetes. Dentre esses projetos necess\u00e1rios, incluem-se: Registro, como o Atomic Registry ou o Docker Registry. Rede, como o OpenvSwitch e roteamento de borda inteligente. Telemetria, como o heapster, o kibana, o hawkular e o elastic. Seguran\u00e7a, como o LDAP, o SELinux, o RBAC e o OAUTH com camadas de multiloca\u00e7\u00e3o. Automa\u00e7\u00e3o, com a adi\u00e7\u00e3o de playbooks do Ansible para a instala\u00e7\u00e3o e o gerenciamento do ciclo de vida do cluster. Servi\u00e7os, oferecidos em um cat\u00e1logo variado de conte\u00fados previamente criados de padr\u00f5es de aplica\u00e7\u00f5es populares.","title":"Kubernetes: como ele funciona?"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#como-ele-se-encaixa-na-infraestrutura","text":"O Kubernetes \u00e9 executado em um sistema operacional e interage com pods de containers executados em n\u00f3s. A m\u00e1quina mestre do Kubernetes aceita os comandos de um administrador (ou equipe de DevOps) e retransmite essas instru\u00e7\u00f5es aos n\u00f3s subservientes. Essa retransmiss\u00e3o \u00e9 realizada em conjunto com v\u00e1rios servi\u00e7os para automaticamente decidir qual n\u00f3 \u00e9 o mais adequado para a tarefa. Depois, s\u00e3o alocados os recursos e atribu\u00eddos os pods do n\u00f3 para cumprir a tarefa solicitada. Portanto, do ponto de vista da infraestrutura, s\u00e3o poucas as mudan\u00e7as em compara\u00e7\u00e3o com a forma como voc\u00ea j\u00e1 gerencia os containers. O controle sobre os containers acontece em um n\u00edvel superior, tornando-o mais refinado, sem a necessidade de microgerenciar cada container ou n\u00f3 separadamente. Ser\u00e1 necess\u00e1rio realizar algum trabalho, mas em sua maioria trata-se somente de uma quest\u00e3o de atribuir um master do Kubernetes e definir os n\u00f3s e pods.","title":"Como ele se encaixa na infraestrutura?"},{"location":"Kubernetes/o%20que%20%C3%A9%20kubernetes/#fontes-usadas-para-elaboracao-desse-conteudo","text":"Documenta\u00e7\u00e3o Kubernete Red Hat","title":"Fontes usadas para elabora\u00e7\u00e3o desse conte\u00fado"},{"location":"Kubernetes/pods/","text":"O que \u00e9 um POD? \u00b6 Os pods s\u00e3o as menores unidades de computa\u00e7\u00e3o implant\u00e1veis \u200b\u200bque voc\u00ea pode criar e gerenciar no Kubernetes. Um Pod \u00e9 um grupo de um ou mais cont\u00eaineres, com armazenamento compartilhado e recursos de rede e uma especifica\u00e7\u00e3o de como executar os cont\u00eaineres. O conte\u00fado de um pod \u00e9 sempre co-localizado e co-agendado e executado em um contexto compartilhado. Um Pod modela um \"host l\u00f3gico\" espec\u00edfico do aplicativo: ele cont\u00e9m um ou mais cont\u00eaineres de aplicativos que s\u00e3o relativamente fortemente acoplados. Em contextos fora da nuvem, os aplicativos executados na mesma m\u00e1quina f\u00edsica ou virtual s\u00e3o an\u00e1logos aos aplicativos em nuvem executados no mesmo host l\u00f3gico. Al\u00e9m dos cont\u00eaineres de aplicativos, um Pod pode conter cont\u00eaineres init que s\u00e3o executados durante a inicializa\u00e7\u00e3o do Pod. Voc\u00ea tamb\u00e9m pode injetar cont\u00eaineres ef\u00eameros para depura\u00e7\u00e3o se seu cluster oferecer isso. O contexto compartilhado de um Pod \u00e9 um conjunto de namespaces do Linux, cgroups e potencialmente outras facetas de isolamento - as mesmas coisas que isolam um cont\u00eainer do Docker. Dentro do contexto de um Pod, os aplicativos individuais podem ter mais subisolamentos aplicados. Em termos de conceitos do Docker, um Pod \u00e9 semelhante a um grupo de cont\u00eaineres do Docker com namespaces compartilhados e volumes de sistema de arquivos compartilhados, ou seja, n\u00e3o vamos manipular diretamente os containers e sim os PODs, que vai se encarregar de criar os containers. Os PODs possuem endere\u00e7os IPs, isso significa que independente de quantos containers estejam no POD, existe somente um endere\u00e7o IP que \u00e9 o do POD, e as portas que s\u00e3o liberadas pelos containers, s\u00e3o sempre acessadas a partir do mesmo endere\u00e7o, mesmo que seja de containers diferentes. Lembrando que os essa estrutura de endere\u00e7amento de IP n\u00e3o permite que dois containers possuam a mesma porta. Os PODs podem se comunicar entre s\u00ed, assim como os containers que agoram compartilha o mesmo IP do POD que est\u00e3o alocado, possuem o mesmo IP. possibilitando a comunica\u00e7\u00e3o via localhost. Primeiro POD \u00b6 Para inicialziar um POD, precisamos ir na documenta\u00e7\u00e3o da API e l\u00e1 vamos encontrar esse comando kubectl run , com isso j\u00e1 podemos ir no terminal para iniciar nosso primeiro POD. kubectl run testando --image = nginx Para verificar os PODs que est\u00e3o ativos kubectl get pods , na documenta\u00e7\u00e3o temos as flags que podemos aplicar, por exemplo o --watch que visualizar os pods, da cria\u00e7\u00e3o at\u00e9 efetivar a ativa\u00e7\u00e3o. Se precisamos de mais informa\u00e7\u00f5es do POD basta usar kubectl describe pod e passa o nome do POD, que no nosso caso \u00e9 testando : kubectl describe pod testando Com isso, temos informa\u00e7\u00e3o do IP do POD, como ele est\u00e1 e os eventos que sucederam ap\u00f3s sua cria\u00e7\u00e3o. Que tal deletamos esse POD com kubectl delete pod . Essa \u00e9 a cria\u00e7\u00e3o simples de um POD, por\u00e9m n\u00e3o \u00e9 a mais recomendada, a documenta\u00e7\u00e3o do kubernetes enfatiza o uso de arquivos com as informa\u00e7\u00f5es sobre a imagem para cria\u00e7\u00e3o do POD, podemos usar tanto o JSON quanto o yaml, nesse caso, por recomenda\u00e7\u00e3o da kubernetes devemos usar conforme as melhores praticas o yaml. Abra uma pasta e crie um arquivo o_primeiro_pod.yaml , pode nomea\u013ao como quiser, mas tem que ter o final .yaml. apiVersion : v1 kind : Pod metadata : name : o-primeiro-pod spec : containers : - name : container-nginx image : nginx Com o arquivo j\u00e1 com as informa\u00e7\u00f5es abra o terminal na pasta que voc\u00ea inseriu o arquivo, e execute kubectl apply com a flag -f para passa o nome do arquivo que queremos executar. kubectl apply -f ./o_primeiro_pod.yaml E criamos nosso primeiro POD de modo declarativo. Servi\u00e7os \u00b6 Os PODs possuem uma comunica\u00e7\u00e3o interna, e que \u00e9 gerenciando de forma autonoma por um recurso chamado service ou svc , documenta\u00e7\u00e3o do service , \u00e9 importante entender o funcionamento do svc pois ele vai ser usado dentro do nosso cluster para definir comunica\u00e7\u00e3o entre os PODs, entre suas tarefas est\u00e3o: Abstra\u00e7\u00e3o para expor aplica\u00e7\u00f5es em um POD ou mais. Proveem IPs fixos para comunica\u00e7\u00e3o Proveem DNS em um POD ou mais Fazem o balanceamento de carga E service possui quatro tipos internos que s\u00e3o: ClusterIP: Exp\u00f5e o servi\u00e7o em um IP interno no cluster. Esse tipo torna o servi\u00e7o acess\u00edvel apenas de dentro do cluster. NodePort: Exp\u00f5e o servi\u00e7o na mesma porta de cada n\u00f3 selecionado no cluster usando NAT. Torna um servi\u00e7o acess\u00edvel de fora do cluster usando : . Superconjunto de ClusterIP. Load Balance: Cria um balanceador de carga externo na nuvem atual (se compat\u00edvel) e atribui um IP externo fixo ao servi\u00e7o. Superconjunto de NodePort. ExternalName: Mapeia o servi\u00e7o para o conte\u00fado do campo externalName (por exemplo, foo.bar.example.com), retornando um registro CNAME com seu valor. Nenhum proxy de qualquer tipo \u00e9 configurado. Este tipo requer v1.7 ou superior de kube-dns, ou CoreDNS vers\u00e3o 0.0.8 ou superior. ClusterIP \u00b6 Para criar um ClusterIP: POD - 1 Service Crie uma pasta Dentro da pasta crie um arquivo, vou nomealo de pod-1.yaml apiVersion : v1 kind : Pod metadata : name : pod-1 labels : app : pod-label spec : containers : - name : pod-1 image : nginx ports : - containerPort : 80 Dentro da pasta crie um arquivo, vou nomealo de svc-pod.yaml apiVersion : v1 kind : Service metadata : name : svc-pod spec : type : ClusterIP selector : app : pod-label ports : - port : 4050 targetPort : 80 Repare que no spec especificamos um ClusterIP e n\u00e3o um container, e em selector estamos dizendo para o Servi\u00e7o procura um r\u00f3tulo/labels nos PODs. Que \u00e9 o link de conex\u00e3o, \u00e9 essa informa\u00e7\u00e3o que faz o trabalho de criar a conex\u00e3o. Em ports estou dizendo para entrar a informa\u00e7\u00e3o/requis\u00e3o na porta 4050 e dizendo para sair targetPort na porta 80. E em kind, estamos passando a informa\u00e7\u00e3o de Service e n\u00e3o Pod Agora, vamos subir os servi\u00e7os e depois os PODs, sempre nessa ordem. 1 2 kubectl apply -f svc-pod.yaml kubectl apply -f pod-1.yaml Para verificar os IPs internos usamos o kubectl get pods e passamos a flag -o e wide : Execute Saida kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES o-primeiro-pod 1 /1 Running 0 5m37m 172 .17.0.3 minikube <none> <none> pod-1 1 /1 Running 0 2m26s 172 .17.0.4 minikube <none> <none> Para verificar o cluster IP precisamo usar o kubectl get svc . kubectl get svc ```bash NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 2d23h svc-pod ClusterIP 10.105.54.122 <none> 4050/TCP 9m4s ``` Com isso a comunica\u00e7\u00e3o interna do Cluster est\u00e1 com um fluxo, se houvesse mais PODs, eles conseguiriam se comunicar com o POD1 atraves do ClusterIP que definimos, por\u00e9m o POD1. NodePort \u00b6 Criando um NodePort. POD - 2 Service Dentro da pasta crie um arquivo, vou nomealo de pod-1.yaml apiVersion : v1 kind : Pod metadata : name : pod-2 labels : app : pod-label2 spec : containers : - name : pod-2 image : sposigor/app-flask-teste:1 ports : - containerPort : 5000 Dentro da pasta crie um arquivo, vou nomealo de svc-pod.yaml apiVersion : v1 kind : Service metadata : name : svc-pod2 spec : type : NodePort ports : - port : 5000 nodePort : 30000 selector : app : pod-label2 Novamente, precisamos inicializar o servi\u00e7o e em seguida o POD. Execute Saida kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 3d22h v1.23.3 192 .168.49.2 <none> Ubuntu 20 .04.2 LTS 5 .15.25-1-MANJARO docker://20.10.12 Precisamos do INTERNAL-IP e com isso passamos a porta 30000 no navegador, no meu caso 192.168.49.2:30000 . Load Balancer \u00b6 Por padr\u00e3o, os servi\u00e7os em nuvem que prestam a integra\u00e7\u00e3o do Kubernetes j\u00e1 disponibilizam o Load Balance, por tr\u00e1s dos panos ele faz tanto o trabalho de ClusterIP e se integra de forma automatica ao NodePort, ou seja, a porta externa \u00e9 disponibilizada pelo Load Balance do provedor. Vou deixar um exemplo de .yaml para Load Balance. apiVersion : v1 kind : Service metadata : name : svc-loadbalancer spec : type : LoadBalancer ports : - port : 80 nodePort : 30000 selector : app : pod-label1","title":"Pods"},{"location":"Kubernetes/pods/#o-que-e-um-pod","text":"Os pods s\u00e3o as menores unidades de computa\u00e7\u00e3o implant\u00e1veis \u200b\u200bque voc\u00ea pode criar e gerenciar no Kubernetes. Um Pod \u00e9 um grupo de um ou mais cont\u00eaineres, com armazenamento compartilhado e recursos de rede e uma especifica\u00e7\u00e3o de como executar os cont\u00eaineres. O conte\u00fado de um pod \u00e9 sempre co-localizado e co-agendado e executado em um contexto compartilhado. Um Pod modela um \"host l\u00f3gico\" espec\u00edfico do aplicativo: ele cont\u00e9m um ou mais cont\u00eaineres de aplicativos que s\u00e3o relativamente fortemente acoplados. Em contextos fora da nuvem, os aplicativos executados na mesma m\u00e1quina f\u00edsica ou virtual s\u00e3o an\u00e1logos aos aplicativos em nuvem executados no mesmo host l\u00f3gico. Al\u00e9m dos cont\u00eaineres de aplicativos, um Pod pode conter cont\u00eaineres init que s\u00e3o executados durante a inicializa\u00e7\u00e3o do Pod. Voc\u00ea tamb\u00e9m pode injetar cont\u00eaineres ef\u00eameros para depura\u00e7\u00e3o se seu cluster oferecer isso. O contexto compartilhado de um Pod \u00e9 um conjunto de namespaces do Linux, cgroups e potencialmente outras facetas de isolamento - as mesmas coisas que isolam um cont\u00eainer do Docker. Dentro do contexto de um Pod, os aplicativos individuais podem ter mais subisolamentos aplicados. Em termos de conceitos do Docker, um Pod \u00e9 semelhante a um grupo de cont\u00eaineres do Docker com namespaces compartilhados e volumes de sistema de arquivos compartilhados, ou seja, n\u00e3o vamos manipular diretamente os containers e sim os PODs, que vai se encarregar de criar os containers. Os PODs possuem endere\u00e7os IPs, isso significa que independente de quantos containers estejam no POD, existe somente um endere\u00e7o IP que \u00e9 o do POD, e as portas que s\u00e3o liberadas pelos containers, s\u00e3o sempre acessadas a partir do mesmo endere\u00e7o, mesmo que seja de containers diferentes. Lembrando que os essa estrutura de endere\u00e7amento de IP n\u00e3o permite que dois containers possuam a mesma porta. Os PODs podem se comunicar entre s\u00ed, assim como os containers que agoram compartilha o mesmo IP do POD que est\u00e3o alocado, possuem o mesmo IP. possibilitando a comunica\u00e7\u00e3o via localhost.","title":"O que \u00e9 um POD?"},{"location":"Kubernetes/pods/#primeiro-pod","text":"Para inicialziar um POD, precisamos ir na documenta\u00e7\u00e3o da API e l\u00e1 vamos encontrar esse comando kubectl run , com isso j\u00e1 podemos ir no terminal para iniciar nosso primeiro POD. kubectl run testando --image = nginx Para verificar os PODs que est\u00e3o ativos kubectl get pods , na documenta\u00e7\u00e3o temos as flags que podemos aplicar, por exemplo o --watch que visualizar os pods, da cria\u00e7\u00e3o at\u00e9 efetivar a ativa\u00e7\u00e3o. Se precisamos de mais informa\u00e7\u00f5es do POD basta usar kubectl describe pod e passa o nome do POD, que no nosso caso \u00e9 testando : kubectl describe pod testando Com isso, temos informa\u00e7\u00e3o do IP do POD, como ele est\u00e1 e os eventos que sucederam ap\u00f3s sua cria\u00e7\u00e3o. Que tal deletamos esse POD com kubectl delete pod . Essa \u00e9 a cria\u00e7\u00e3o simples de um POD, por\u00e9m n\u00e3o \u00e9 a mais recomendada, a documenta\u00e7\u00e3o do kubernetes enfatiza o uso de arquivos com as informa\u00e7\u00f5es sobre a imagem para cria\u00e7\u00e3o do POD, podemos usar tanto o JSON quanto o yaml, nesse caso, por recomenda\u00e7\u00e3o da kubernetes devemos usar conforme as melhores praticas o yaml. Abra uma pasta e crie um arquivo o_primeiro_pod.yaml , pode nomea\u013ao como quiser, mas tem que ter o final .yaml. apiVersion : v1 kind : Pod metadata : name : o-primeiro-pod spec : containers : - name : container-nginx image : nginx Com o arquivo j\u00e1 com as informa\u00e7\u00f5es abra o terminal na pasta que voc\u00ea inseriu o arquivo, e execute kubectl apply com a flag -f para passa o nome do arquivo que queremos executar. kubectl apply -f ./o_primeiro_pod.yaml E criamos nosso primeiro POD de modo declarativo.","title":"Primeiro POD"},{"location":"Kubernetes/pods/#servicos","text":"Os PODs possuem uma comunica\u00e7\u00e3o interna, e que \u00e9 gerenciando de forma autonoma por um recurso chamado service ou svc , documenta\u00e7\u00e3o do service , \u00e9 importante entender o funcionamento do svc pois ele vai ser usado dentro do nosso cluster para definir comunica\u00e7\u00e3o entre os PODs, entre suas tarefas est\u00e3o: Abstra\u00e7\u00e3o para expor aplica\u00e7\u00f5es em um POD ou mais. Proveem IPs fixos para comunica\u00e7\u00e3o Proveem DNS em um POD ou mais Fazem o balanceamento de carga E service possui quatro tipos internos que s\u00e3o: ClusterIP: Exp\u00f5e o servi\u00e7o em um IP interno no cluster. Esse tipo torna o servi\u00e7o acess\u00edvel apenas de dentro do cluster. NodePort: Exp\u00f5e o servi\u00e7o na mesma porta de cada n\u00f3 selecionado no cluster usando NAT. Torna um servi\u00e7o acess\u00edvel de fora do cluster usando : . Superconjunto de ClusterIP. Load Balance: Cria um balanceador de carga externo na nuvem atual (se compat\u00edvel) e atribui um IP externo fixo ao servi\u00e7o. Superconjunto de NodePort. ExternalName: Mapeia o servi\u00e7o para o conte\u00fado do campo externalName (por exemplo, foo.bar.example.com), retornando um registro CNAME com seu valor. Nenhum proxy de qualquer tipo \u00e9 configurado. Este tipo requer v1.7 ou superior de kube-dns, ou CoreDNS vers\u00e3o 0.0.8 ou superior.","title":"Servi\u00e7os"},{"location":"Kubernetes/pods/#clusterip","text":"Para criar um ClusterIP: POD - 1 Service Crie uma pasta Dentro da pasta crie um arquivo, vou nomealo de pod-1.yaml apiVersion : v1 kind : Pod metadata : name : pod-1 labels : app : pod-label spec : containers : - name : pod-1 image : nginx ports : - containerPort : 80 Dentro da pasta crie um arquivo, vou nomealo de svc-pod.yaml apiVersion : v1 kind : Service metadata : name : svc-pod spec : type : ClusterIP selector : app : pod-label ports : - port : 4050 targetPort : 80 Repare que no spec especificamos um ClusterIP e n\u00e3o um container, e em selector estamos dizendo para o Servi\u00e7o procura um r\u00f3tulo/labels nos PODs. Que \u00e9 o link de conex\u00e3o, \u00e9 essa informa\u00e7\u00e3o que faz o trabalho de criar a conex\u00e3o. Em ports estou dizendo para entrar a informa\u00e7\u00e3o/requis\u00e3o na porta 4050 e dizendo para sair targetPort na porta 80. E em kind, estamos passando a informa\u00e7\u00e3o de Service e n\u00e3o Pod Agora, vamos subir os servi\u00e7os e depois os PODs, sempre nessa ordem. 1 2 kubectl apply -f svc-pod.yaml kubectl apply -f pod-1.yaml Para verificar os IPs internos usamos o kubectl get pods e passamos a flag -o e wide : Execute Saida kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES o-primeiro-pod 1 /1 Running 0 5m37m 172 .17.0.3 minikube <none> <none> pod-1 1 /1 Running 0 2m26s 172 .17.0.4 minikube <none> <none> Para verificar o cluster IP precisamo usar o kubectl get svc . kubectl get svc ```bash NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 2d23h svc-pod ClusterIP 10.105.54.122 <none> 4050/TCP 9m4s ``` Com isso a comunica\u00e7\u00e3o interna do Cluster est\u00e1 com um fluxo, se houvesse mais PODs, eles conseguiriam se comunicar com o POD1 atraves do ClusterIP que definimos, por\u00e9m o POD1.","title":"ClusterIP"},{"location":"Kubernetes/pods/#nodeport","text":"Criando um NodePort. POD - 2 Service Dentro da pasta crie um arquivo, vou nomealo de pod-1.yaml apiVersion : v1 kind : Pod metadata : name : pod-2 labels : app : pod-label2 spec : containers : - name : pod-2 image : sposigor/app-flask-teste:1 ports : - containerPort : 5000 Dentro da pasta crie um arquivo, vou nomealo de svc-pod.yaml apiVersion : v1 kind : Service metadata : name : svc-pod2 spec : type : NodePort ports : - port : 5000 nodePort : 30000 selector : app : pod-label2 Novamente, precisamos inicializar o servi\u00e7o e em seguida o POD. Execute Saida kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 3d22h v1.23.3 192 .168.49.2 <none> Ubuntu 20 .04.2 LTS 5 .15.25-1-MANJARO docker://20.10.12 Precisamos do INTERNAL-IP e com isso passamos a porta 30000 no navegador, no meu caso 192.168.49.2:30000 .","title":"NodePort"},{"location":"Kubernetes/pods/#load-balancer","text":"Por padr\u00e3o, os servi\u00e7os em nuvem que prestam a integra\u00e7\u00e3o do Kubernetes j\u00e1 disponibilizam o Load Balance, por tr\u00e1s dos panos ele faz tanto o trabalho de ClusterIP e se integra de forma automatica ao NodePort, ou seja, a porta externa \u00e9 disponibilizada pelo Load Balance do provedor. Vou deixar um exemplo de .yaml para Load Balance. apiVersion : v1 kind : Service metadata : name : svc-loadbalancer spec : type : LoadBalancer ports : - port : 80 nodePort : 30000 selector : app : pod-label1","title":"Load Balancer"},{"location":"Kubernetes/projeto/","text":"Projeto \u00b6 Que tal um projeto integrando toda a explica\u00e7\u00e3o at\u00e9 aqui? Vamos usar o docker samples . Crie um .yaml: apiVersion : v1 kind : Service metadata : name : db labels : app : words-db spec : ports : - port : 5432 targetPort : 5432 name : db selector : app : words-db clusterIP : None --- apiVersion : apps/v1 kind : Deployment metadata : name : db labels : app : words-db spec : selector : matchLabels : app : words-db template : metadata : labels : app : words-db spec : containers : - name : db image : dockersamples/k8s-wordsmith-db ports : - containerPort : 5432 name : db --- apiVersion : v1 kind : Service metadata : name : words labels : app : words-api spec : ports : - port : 8080 targetPort : 8080 name : api selector : app : words-api clusterIP : None --- apiVersion : apps/v1 kind : Deployment metadata : name : words labels : app : words-api spec : replicas : 5 selector : matchLabels : app : words-api template : metadata : labels : app : words-api spec : containers : - name : words image : dockersamples/k8s-wordsmith-api ports : - containerPort : 8080 name : api --- apiVersion : v1 kind : Service metadata : name : web labels : app : words-web spec : ports : - port : 8081 targetPort : 80 name : web selector : app : words-web type : LoadBalancer --- apiVersion : apps/v1 kind : Deployment metadata : name : web labels : app : words-web spec : selector : matchLabels : app : words-web template : metadata : labels : app : words-web spec : containers : - name : web image : dockersamples/k8s-wordsmith-web ports : - containerPort : 80 name : words-web Em seguida: Execute Saida kubectl apply -f kube-deployment.yml Para verificar os servi\u00e7os ativos: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE db ClusterIP None <none> 5432 /TCP 17s kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 3d23h svc-pod ClusterIP 10 .105.54.122 <none> 9000 /TCP 24h svc-pod2 NodePort 10 .98.89.245 <none> 5000 :30000/TCP 61m web LoadBalancer 10 .97.254.156 <pending> 8081 :31555/TCP 17s words ClusterIP None <none> 8080 /TCP 17s No meu caso ele redirecionou o Load Balance para porta 31555 , ficando assim 192.168.49.2:31555 . Se precisar do INTERNAL-IP , basta executar kubectl get nodes -o wide .","title":"Projeto"},{"location":"Kubernetes/projeto/#projeto","text":"Que tal um projeto integrando toda a explica\u00e7\u00e3o at\u00e9 aqui? Vamos usar o docker samples . Crie um .yaml: apiVersion : v1 kind : Service metadata : name : db labels : app : words-db spec : ports : - port : 5432 targetPort : 5432 name : db selector : app : words-db clusterIP : None --- apiVersion : apps/v1 kind : Deployment metadata : name : db labels : app : words-db spec : selector : matchLabels : app : words-db template : metadata : labels : app : words-db spec : containers : - name : db image : dockersamples/k8s-wordsmith-db ports : - containerPort : 5432 name : db --- apiVersion : v1 kind : Service metadata : name : words labels : app : words-api spec : ports : - port : 8080 targetPort : 8080 name : api selector : app : words-api clusterIP : None --- apiVersion : apps/v1 kind : Deployment metadata : name : words labels : app : words-api spec : replicas : 5 selector : matchLabels : app : words-api template : metadata : labels : app : words-api spec : containers : - name : words image : dockersamples/k8s-wordsmith-api ports : - containerPort : 8080 name : api --- apiVersion : v1 kind : Service metadata : name : web labels : app : words-web spec : ports : - port : 8081 targetPort : 80 name : web selector : app : words-web type : LoadBalancer --- apiVersion : apps/v1 kind : Deployment metadata : name : web labels : app : words-web spec : selector : matchLabels : app : words-web template : metadata : labels : app : words-web spec : containers : - name : web image : dockersamples/k8s-wordsmith-web ports : - containerPort : 80 name : words-web Em seguida: Execute Saida kubectl apply -f kube-deployment.yml Para verificar os servi\u00e7os ativos: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE db ClusterIP None <none> 5432 /TCP 17s kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 3d23h svc-pod ClusterIP 10 .105.54.122 <none> 9000 /TCP 24h svc-pod2 NodePort 10 .98.89.245 <none> 5000 :30000/TCP 61m web LoadBalancer 10 .97.254.156 <pending> 8081 :31555/TCP 17s words ClusterIP None <none> 8080 /TCP 17s No meu caso ele redirecionou o Load Balance para porta 31555 , ficando assim 192.168.49.2:31555 . Se precisar do INTERNAL-IP , basta executar kubectl get nodes -o wide .","title":"Projeto"},{"location":"Kubernetes/replicaset/","text":"ReplicaSet \u00b6 A finalidade de um ReplicaSet \u00e9 manter um conjunto est\u00e1vel de pods de r\u00e9plica em execu\u00e7\u00e3o a qualquer momento. Como tal, \u00e9 frequentemente usado para garantir a disponibilidade de um n\u00famero especificado de Pods id\u00eanticos. \u00b6 Como funciona os ReplicaSet \u00b6 Um ReplicaSet \u00e9 definido com campos, incluindo um seletor que especifica como identificar os pods que ele pode adquirir, um n\u00famero de r\u00e9plicas indicando quantos pods ele deve manter e um modelo de pod especificando os dados dos novos pods que ele deve criar para atender ao n\u00famero de crit\u00e9rios de r\u00e9plicas. Um ReplicaSet cumpre sua finalidade criando e excluindo Pods conforme necess\u00e1rio para atingir o n\u00famero desejado. Quando um ReplicaSet precisa criar novos pods, ele usa seu modelo de pod. Um ReplicaSet \u00e9 vinculado a seus pods por meio do campo metadata.ownerReferences dos pods , que especifica de qual recurso o objeto atual pertence. Todos os Pods adquiridos por um ReplicaSet t\u00eam as informa\u00e7\u00f5es de identifica\u00e7\u00e3o de seu pr\u00f3prio ReplicaSet no campo ownerReferences. \u00c9 por meio desse link que o ReplicaSet conhece o estado dos Pods que est\u00e1 mantendo e planeja de acordo. Um ReplicaSet identifica novos pods a serem adquiridos usando seu seletor. Se houver um pod que n\u00e3o tenha OwnerReference ou OwnerReference n\u00e3o for um Controlador e corresponder ao seletor de um ReplicaSet, ele ser\u00e1 imediatamente adquirido pelo referido ReplicaSet. Quando usar um ReplicaSet? \u00b6 Um ReplicaSet garante que um n\u00famero especificado de r\u00e9plicas de pod esteja em execu\u00e7\u00e3o a qualquer momento. No entanto, uma implanta\u00e7\u00e3o \u00e9 um conceito de n\u00edvel superior que gerencia ReplicaSets e fornece atualiza\u00e7\u00f5es declarativas para Pods junto com muitos outros recursos \u00fateis. Portanto, recomendamos o uso de implanta\u00e7\u00f5es em vez de usar diretamente ReplicaSets, a menos que voc\u00ea exija uma orquestra\u00e7\u00e3o de atualiza\u00e7\u00e3o personalizada ou n\u00e3o exija nenhuma atualiza\u00e7\u00e3o. Na verdade, isso significa que talvez voc\u00ea nunca precise manipular objetos ReplicaSet: use um Deployment e defina seu aplicativo na se\u00e7\u00e3o spec. Configurando o replicaset \u00b6 Vamos usar o exemplo disponivel na documenta\u00e7\u00e3o: apiVersion : apps/v1 kind : ReplicaSet metadata : name : frontend labels : app : guestbook tier : frontend spec : # modify replicas according to your case replicas : 3 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : php-redis image : gcr.io/google_samples/gb-frontend:v3 Antes de continuar, precisamos entender melhor cada parte dessa configura\u00e7\u00e3o que foi feita. Parte 1 Parte 2 kind : ReplicaSet metadata : name : frontend labels : app : guestbook tier : frontend Estamos passando para o kind o ReplicaSet para o kubernetes entender que \u00e9 um servi\u00e7o do tipo ReplicaSet Em metadata mais especificamente no labels foi inserido o tier que \u00e9 um nome para um conjuto, ent\u00e3o imagine que tem varios PODs e ao inves de criar um matchLabels para cada app, criamos um tier e inserimos essa informa\u00e7\u00e3o em todos os PODs desse conjuto, assim precisamos criar somente um ReplicaSet para todos esses PODs. spec : # modify replicas according to your case replicas : 3 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : php-redis image : gcr.io/google_samples/gb-frontend:v3 replicas \u00e9 a quantidade de PODs ideal para aplica\u00e7\u00e3o, mas tamb\u00e9m \u00e9 a quantidade de PODs que sobem. Teremos um numero ideal de PODs a disponiveis e a quantidade de PODs que est\u00e3o efetivamente disponiveis e nesse momento o ReplicaSet faz sua magica, que \u00e9 a substitui\u00e7\u00e3o automatica dos PODs que efetuamente cairem. selector estamos falando para ele procurar todos os PODs, do tier frontend para realizar o processo de monitorar, replicar e substituir. template \u00e9 o padr\u00e3o que vai conter as informa\u00e7\u00f5es do POD. Com isso, basta criar o arquivo.yaml e executar. kubectl apply -f replicas.yaml Ele vai executar o comando e vamos verificar: kubectl get rs Saida: NAME DESIRED CURRENT READY AGE frontend 3 3 3 3s Temos o nome do ReplicaSet que criamos, a quantidade de Desired que \u00e9 um numero desejado que foi informado em replicas o Current que \u00e9 a informa\u00e7\u00e3o sobre os PODs que est\u00e3o normais e o Ready que est\u00e1 informando que est\u00e3o todos ativos. Testando o ReplicaSet \u00b6 Com a mesma aplica\u00e7\u00e3o, veremos na pratica como ele funciona, primeiro abra dois terminas e deixe um do lado do outro. No primeiro vamos digitar o kubectl get rs e passar o parametro --watch para conseguimos verificar emn tempo real. kubectl get rs --watch No outro terminal, faremos primeiro um kubectl get pod e em seguida kubectl delete pod e passe um dos PODs disponiveis. get pod delete kubectl get pod NAME READY STATUS RESTARTS AGE frontend-hvvnp 1 /1 Running 0 6m53s frontend-t4l87 1 /1 Running 0 6m53s frontend-xqfsq 1 /1 Running 0 6m53s Vou usar o frontend-hvvnp para o delete kubectl delete pod frontend-hvvnp Ap\u00f3s concluir o procedimento, perceba que o outro terminal teve uma altera\u00e7\u00e3o: NAME DESIRED CURRENT READY AGE frontend 3 3 3 8m18s frontend 3 2 2 8m50s frontend 3 3 2 8m50s frontend 3 3 3 8m51s Aqui estamos vendo que o valor de Current e Ready tiveram mudan\u00e7a e em seguida retornaram ao valor original. Com isso nosso teste est\u00e1 concluido. Balaceamento de Carga \u00b6 O ReplicaSet tamb\u00e9m se integra no balaceamento de carga, uma vez que o servi\u00e7o pode ser distribuido igualmente pelas replicas disponiveis. Para verificar mais informa\u00e7\u00f5es consultar a Documenta\u00e7\u00e3o","title":"ReplicaSet"},{"location":"Kubernetes/replicaset/#replicaset","text":"A finalidade de um ReplicaSet \u00e9 manter um conjunto est\u00e1vel de pods de r\u00e9plica em execu\u00e7\u00e3o a qualquer momento. Como tal, \u00e9 frequentemente usado para garantir a disponibilidade de um n\u00famero especificado de Pods id\u00eanticos.","title":"ReplicaSet"},{"location":"Kubernetes/replicaset/#_1","text":"","title":""},{"location":"Kubernetes/replicaset/#como-funciona-os-replicaset","text":"Um ReplicaSet \u00e9 definido com campos, incluindo um seletor que especifica como identificar os pods que ele pode adquirir, um n\u00famero de r\u00e9plicas indicando quantos pods ele deve manter e um modelo de pod especificando os dados dos novos pods que ele deve criar para atender ao n\u00famero de crit\u00e9rios de r\u00e9plicas. Um ReplicaSet cumpre sua finalidade criando e excluindo Pods conforme necess\u00e1rio para atingir o n\u00famero desejado. Quando um ReplicaSet precisa criar novos pods, ele usa seu modelo de pod. Um ReplicaSet \u00e9 vinculado a seus pods por meio do campo metadata.ownerReferences dos pods , que especifica de qual recurso o objeto atual pertence. Todos os Pods adquiridos por um ReplicaSet t\u00eam as informa\u00e7\u00f5es de identifica\u00e7\u00e3o de seu pr\u00f3prio ReplicaSet no campo ownerReferences. \u00c9 por meio desse link que o ReplicaSet conhece o estado dos Pods que est\u00e1 mantendo e planeja de acordo. Um ReplicaSet identifica novos pods a serem adquiridos usando seu seletor. Se houver um pod que n\u00e3o tenha OwnerReference ou OwnerReference n\u00e3o for um Controlador e corresponder ao seletor de um ReplicaSet, ele ser\u00e1 imediatamente adquirido pelo referido ReplicaSet.","title":"Como funciona os ReplicaSet"},{"location":"Kubernetes/replicaset/#quando-usar-um-replicaset","text":"Um ReplicaSet garante que um n\u00famero especificado de r\u00e9plicas de pod esteja em execu\u00e7\u00e3o a qualquer momento. No entanto, uma implanta\u00e7\u00e3o \u00e9 um conceito de n\u00edvel superior que gerencia ReplicaSets e fornece atualiza\u00e7\u00f5es declarativas para Pods junto com muitos outros recursos \u00fateis. Portanto, recomendamos o uso de implanta\u00e7\u00f5es em vez de usar diretamente ReplicaSets, a menos que voc\u00ea exija uma orquestra\u00e7\u00e3o de atualiza\u00e7\u00e3o personalizada ou n\u00e3o exija nenhuma atualiza\u00e7\u00e3o. Na verdade, isso significa que talvez voc\u00ea nunca precise manipular objetos ReplicaSet: use um Deployment e defina seu aplicativo na se\u00e7\u00e3o spec.","title":"Quando usar um ReplicaSet?"},{"location":"Kubernetes/replicaset/#configurando-o-replicaset","text":"Vamos usar o exemplo disponivel na documenta\u00e7\u00e3o: apiVersion : apps/v1 kind : ReplicaSet metadata : name : frontend labels : app : guestbook tier : frontend spec : # modify replicas according to your case replicas : 3 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : php-redis image : gcr.io/google_samples/gb-frontend:v3 Antes de continuar, precisamos entender melhor cada parte dessa configura\u00e7\u00e3o que foi feita. Parte 1 Parte 2 kind : ReplicaSet metadata : name : frontend labels : app : guestbook tier : frontend Estamos passando para o kind o ReplicaSet para o kubernetes entender que \u00e9 um servi\u00e7o do tipo ReplicaSet Em metadata mais especificamente no labels foi inserido o tier que \u00e9 um nome para um conjuto, ent\u00e3o imagine que tem varios PODs e ao inves de criar um matchLabels para cada app, criamos um tier e inserimos essa informa\u00e7\u00e3o em todos os PODs desse conjuto, assim precisamos criar somente um ReplicaSet para todos esses PODs. spec : # modify replicas according to your case replicas : 3 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : php-redis image : gcr.io/google_samples/gb-frontend:v3 replicas \u00e9 a quantidade de PODs ideal para aplica\u00e7\u00e3o, mas tamb\u00e9m \u00e9 a quantidade de PODs que sobem. Teremos um numero ideal de PODs a disponiveis e a quantidade de PODs que est\u00e3o efetivamente disponiveis e nesse momento o ReplicaSet faz sua magica, que \u00e9 a substitui\u00e7\u00e3o automatica dos PODs que efetuamente cairem. selector estamos falando para ele procurar todos os PODs, do tier frontend para realizar o processo de monitorar, replicar e substituir. template \u00e9 o padr\u00e3o que vai conter as informa\u00e7\u00f5es do POD. Com isso, basta criar o arquivo.yaml e executar. kubectl apply -f replicas.yaml Ele vai executar o comando e vamos verificar: kubectl get rs Saida: NAME DESIRED CURRENT READY AGE frontend 3 3 3 3s Temos o nome do ReplicaSet que criamos, a quantidade de Desired que \u00e9 um numero desejado que foi informado em replicas o Current que \u00e9 a informa\u00e7\u00e3o sobre os PODs que est\u00e3o normais e o Ready que est\u00e1 informando que est\u00e3o todos ativos.","title":"Configurando o replicaset"},{"location":"Kubernetes/replicaset/#testando-o-replicaset","text":"Com a mesma aplica\u00e7\u00e3o, veremos na pratica como ele funciona, primeiro abra dois terminas e deixe um do lado do outro. No primeiro vamos digitar o kubectl get rs e passar o parametro --watch para conseguimos verificar emn tempo real. kubectl get rs --watch No outro terminal, faremos primeiro um kubectl get pod e em seguida kubectl delete pod e passe um dos PODs disponiveis. get pod delete kubectl get pod NAME READY STATUS RESTARTS AGE frontend-hvvnp 1 /1 Running 0 6m53s frontend-t4l87 1 /1 Running 0 6m53s frontend-xqfsq 1 /1 Running 0 6m53s Vou usar o frontend-hvvnp para o delete kubectl delete pod frontend-hvvnp Ap\u00f3s concluir o procedimento, perceba que o outro terminal teve uma altera\u00e7\u00e3o: NAME DESIRED CURRENT READY AGE frontend 3 3 3 8m18s frontend 3 2 2 8m50s frontend 3 3 2 8m50s frontend 3 3 3 8m51s Aqui estamos vendo que o valor de Current e Ready tiveram mudan\u00e7a e em seguida retornaram ao valor original. Com isso nosso teste est\u00e1 concluido.","title":"Testando o ReplicaSet"},{"location":"Kubernetes/replicaset/#balaceamento-de-carga","text":"O ReplicaSet tamb\u00e9m se integra no balaceamento de carga, uma vez que o servi\u00e7o pode ser distribuido igualmente pelas replicas disponiveis. Para verificar mais informa\u00e7\u00f5es consultar a Documenta\u00e7\u00e3o","title":"Balaceamento de Carga"},{"location":"docker/compose/","text":"Compose o que \u00e9? \u00b6 O docker compose \u00e9 uma ferramenta desenvolvida para ajudar a definir e compartilhar aplicativos de v\u00e1rios cont\u00eaineres. A grande vantagem de usar o Compose \u00e9 que voc\u00ea pode definir todos os aplicativos em um arquivo, mant\u00ea-la na raiz do reposit\u00f3rio do seu projeto (agora com controle de vers\u00e3o) e permitir que outra pessoa contribua facilmente para o seu projeto. Algu\u00e9m precisaria apenas clonar seu reposit\u00f3rio e iniciar o aplicativo de composi\u00e7\u00e3o. Na verdade, voc\u00ea pode ver alguns projetos no GitHub/GitLab fazendo exatamente isso agora. O arquivo de defini\u00e7\u00e3o do Docker Compose \u00e9 o local onde \u00e9 especificado todo o ambiente (rede, volume e servi\u00e7os), ele \u00e9 escrito seguindo o formato YAML. Esse arquivo por padr\u00e3o tem como nome docker-compose.yml. O que \u00e9 o YAML? \u00b6 YAML \u00e9 uma linguagem de serializa\u00e7\u00e3o de dados que \u00e9 frequentemente usada para gravar arquivos de configura\u00e7\u00e3o. Dependendo de quem voc\u00ea pergunta, YAML significa ainda outra linguagem de marca\u00e7\u00e3o ou YAML n\u00e3o \u00e9 uma linguagem de marca\u00e7\u00e3o (um acr\u00f4nimo recursivo), que enfatiza que YAML \u00e9 para dados, n\u00e3o documentos. YAML \u00e9 uma linguagem de programa\u00e7\u00e3o popular porque \u00e9 leg\u00edvel e f\u00e1cil de entender. Tamb\u00e9m pode ser usado em conjunto com outras linguagens de programa\u00e7\u00e3o. Instalando o Docker-Compose \u00b6 Windows Mac Linux O docker compose \u00e9 instalado junto ao docker docker-compose O docker compose \u00e9 instalado junto ao docker docker-compose No linux, basta seguir a documenta\u00e7\u00e3o Se houve instala\u00e7\u00e3o via Snap ou Flatpak, o docker compose precisa usar a mesma via Ap\u00f3s a instala\u00e7\u00e3o docker-compose Anatomia do docker-compose.yml \u00b6 O padr\u00e3o YAML utiliza a indenta\u00e7\u00e3o como separador dos blocos de c\u00f3digos das defini\u00e7\u00f5es, por conta disso o uso da indenta\u00e7\u00e3o \u00e9 um fator muito importante, ou seja, caso n\u00e3o a utilize corretamente, o docker-compose falhar\u00e1 em sua execu\u00e7\u00e3o. Cada linha desse arquivo pode ser definida com uma chave valor ou uma lista. Nesse caso, vamos usar a imagem que criamos: Crie uma pasta Crie um arquivo chamado docker-compose.yml na pasta Copiei e cole: version : \"1\" services : flask_web : image : sposigor/app-flask-teste:1 ports : - 5000:5000 Abra o terminal na pasta, que est\u00e1 o arquivo docker-compose.yml e execute: docker-compose up -d Ilustra\u00e7\u00e3o usando o docker-compose up V\u00e1 em localhost:5000 para verificar. Ai est\u00e1 a m\u00e1gica do compose, com um simples documento yml, podemos configurar nosso cont\u00eaineres para rodar com um simples comando e para finalizar: docker-compose down Vou deixar a documenta\u00e7\u00e3o da docker para todas as possibilidades da estrutura docker github \u00e9 importante ler, n\u00e3o fique preocupado com todas as especifica\u00e7\u00f5es, geralmente com algumas \u00e9 o suficiente para subir o cont\u00eainer a partir do .yml. Pare de fazer confus\u00e3o com o Docker Compose Vamos brincar um pouco com o compose \u00b6 Agora que voc\u00ea entendeu um pouco sobre o compose, vou deixar alguns exemplos de arquivos docker-compose.yml. MySQL PostgresSQL PostgresSQL + pgadmin version : \"1\" services : db : image : mysql:8.0-oracle restart : always environment : MYSQL_DATABASE : 'db' MYSQL_USER : 'user' MYSQL_PASSWORD : 'password' MYSQL_ROOT_PASSWORD : 'password' ports : - '3306:3306' expose : - '3306' volumes : - my-db:/var/lib/mysql volumes : my-db : Use um editor sql de sua preferencia e fa\u00e7a a conex\u00e3o. version : '1' services : postgres-compose : image : postgres:14.-alpine restart : always environment : - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres ports : - '5432:5432' volumes : - db:/var/lib/postgresql/data volumes : db : driver : local Use um editor sql de sua preferencia e fa\u00e7a a conex\u00e3o. version : '1' services : postgres-compose : image : postgres:14.2-alpine environment : POSTGRES_USERNAME : \"postgres\" POSTGRES_PASSWORD : \"postgres\" ports : - \"5432:5432\" volumes : - db:/var/lib/postgresql/data networks : - postgres-compose-network pgadmin-compose : image : dpage/pgadmin4 environment : PGADMIN_DEFAULT_EMAIL : \"qualquer@email.com\" PGADMIN_DEFAULT_PASSWORD : \"postgres\" ports : - \"1515:80\" depends_on : - postgres-compose networks : - postgres-compose-network volumes : db : driver : local networks : postgres-compose-network : driver : bridge V\u00e1 no localhost:1515 login: qualquer@email.com Senha: postgres Pronto mais uma conex\u00e3o bem sucedida Al\u00e9m disso, recomendo verificar esse reposit\u00f3rio da docker com m\u00faltiplos exemplos awesome-compose Tente bastante, repita cada passo, crie suas varia\u00e7\u00f5es.","title":"Docker Compose"},{"location":"docker/compose/#compose-o-que-e","text":"O docker compose \u00e9 uma ferramenta desenvolvida para ajudar a definir e compartilhar aplicativos de v\u00e1rios cont\u00eaineres. A grande vantagem de usar o Compose \u00e9 que voc\u00ea pode definir todos os aplicativos em um arquivo, mant\u00ea-la na raiz do reposit\u00f3rio do seu projeto (agora com controle de vers\u00e3o) e permitir que outra pessoa contribua facilmente para o seu projeto. Algu\u00e9m precisaria apenas clonar seu reposit\u00f3rio e iniciar o aplicativo de composi\u00e7\u00e3o. Na verdade, voc\u00ea pode ver alguns projetos no GitHub/GitLab fazendo exatamente isso agora. O arquivo de defini\u00e7\u00e3o do Docker Compose \u00e9 o local onde \u00e9 especificado todo o ambiente (rede, volume e servi\u00e7os), ele \u00e9 escrito seguindo o formato YAML. Esse arquivo por padr\u00e3o tem como nome docker-compose.yml.","title":"Compose o que \u00e9?"},{"location":"docker/compose/#o-que-e-o-yaml","text":"YAML \u00e9 uma linguagem de serializa\u00e7\u00e3o de dados que \u00e9 frequentemente usada para gravar arquivos de configura\u00e7\u00e3o. Dependendo de quem voc\u00ea pergunta, YAML significa ainda outra linguagem de marca\u00e7\u00e3o ou YAML n\u00e3o \u00e9 uma linguagem de marca\u00e7\u00e3o (um acr\u00f4nimo recursivo), que enfatiza que YAML \u00e9 para dados, n\u00e3o documentos. YAML \u00e9 uma linguagem de programa\u00e7\u00e3o popular porque \u00e9 leg\u00edvel e f\u00e1cil de entender. Tamb\u00e9m pode ser usado em conjunto com outras linguagens de programa\u00e7\u00e3o.","title":"O que \u00e9 o YAML?"},{"location":"docker/compose/#instalando-o-docker-compose","text":"Windows Mac Linux O docker compose \u00e9 instalado junto ao docker docker-compose O docker compose \u00e9 instalado junto ao docker docker-compose No linux, basta seguir a documenta\u00e7\u00e3o Se houve instala\u00e7\u00e3o via Snap ou Flatpak, o docker compose precisa usar a mesma via Ap\u00f3s a instala\u00e7\u00e3o docker-compose","title":"Instalando o Docker-Compose"},{"location":"docker/compose/#anatomia-do-docker-composeyml","text":"O padr\u00e3o YAML utiliza a indenta\u00e7\u00e3o como separador dos blocos de c\u00f3digos das defini\u00e7\u00f5es, por conta disso o uso da indenta\u00e7\u00e3o \u00e9 um fator muito importante, ou seja, caso n\u00e3o a utilize corretamente, o docker-compose falhar\u00e1 em sua execu\u00e7\u00e3o. Cada linha desse arquivo pode ser definida com uma chave valor ou uma lista. Nesse caso, vamos usar a imagem que criamos: Crie uma pasta Crie um arquivo chamado docker-compose.yml na pasta Copiei e cole: version : \"1\" services : flask_web : image : sposigor/app-flask-teste:1 ports : - 5000:5000 Abra o terminal na pasta, que est\u00e1 o arquivo docker-compose.yml e execute: docker-compose up -d Ilustra\u00e7\u00e3o usando o docker-compose up V\u00e1 em localhost:5000 para verificar. Ai est\u00e1 a m\u00e1gica do compose, com um simples documento yml, podemos configurar nosso cont\u00eaineres para rodar com um simples comando e para finalizar: docker-compose down Vou deixar a documenta\u00e7\u00e3o da docker para todas as possibilidades da estrutura docker github \u00e9 importante ler, n\u00e3o fique preocupado com todas as especifica\u00e7\u00f5es, geralmente com algumas \u00e9 o suficiente para subir o cont\u00eainer a partir do .yml. Pare de fazer confus\u00e3o com o Docker Compose","title":"Anatomia do docker-compose.yml"},{"location":"docker/compose/#vamos-brincar-um-pouco-com-o-compose","text":"Agora que voc\u00ea entendeu um pouco sobre o compose, vou deixar alguns exemplos de arquivos docker-compose.yml. MySQL PostgresSQL PostgresSQL + pgadmin version : \"1\" services : db : image : mysql:8.0-oracle restart : always environment : MYSQL_DATABASE : 'db' MYSQL_USER : 'user' MYSQL_PASSWORD : 'password' MYSQL_ROOT_PASSWORD : 'password' ports : - '3306:3306' expose : - '3306' volumes : - my-db:/var/lib/mysql volumes : my-db : Use um editor sql de sua preferencia e fa\u00e7a a conex\u00e3o. version : '1' services : postgres-compose : image : postgres:14.-alpine restart : always environment : - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres ports : - '5432:5432' volumes : - db:/var/lib/postgresql/data volumes : db : driver : local Use um editor sql de sua preferencia e fa\u00e7a a conex\u00e3o. version : '1' services : postgres-compose : image : postgres:14.2-alpine environment : POSTGRES_USERNAME : \"postgres\" POSTGRES_PASSWORD : \"postgres\" ports : - \"5432:5432\" volumes : - db:/var/lib/postgresql/data networks : - postgres-compose-network pgadmin-compose : image : dpage/pgadmin4 environment : PGADMIN_DEFAULT_EMAIL : \"qualquer@email.com\" PGADMIN_DEFAULT_PASSWORD : \"postgres\" ports : - \"1515:80\" depends_on : - postgres-compose networks : - postgres-compose-network volumes : db : driver : local networks : postgres-compose-network : driver : bridge V\u00e1 no localhost:1515 login: qualquer@email.com Senha: postgres Pronto mais uma conex\u00e3o bem sucedida Al\u00e9m disso, recomendo verificar esse reposit\u00f3rio da docker com m\u00faltiplos exemplos awesome-compose Tente bastante, repita cada passo, crie suas varia\u00e7\u00f5es.","title":"Vamos brincar um pouco com o compose"},{"location":"docker/docker_hub/","text":"O que \u00e9 o Docker Hub? \ud83e\udd14 \u00b6 Para exemplificar, o Docker Hub \u00e9 um reposit\u00f3rio que possui imagens para o docker Para acessar Docker Hub Na p\u00e1gina inicial, pesquisaremos o hello-world. Pesquise e chegaremos nessa p\u00e1gina hello-world L\u00e1 encontraremos uma refer\u00eancia e informa\u00e7\u00f5es importantes Tamb\u00e9m repare que existe uma op\u00e7\u00e3o de fazer um pull, ou seja, baixa a imagem localmente Existe um selo oficial das imagens que est\u00e3o no docker hub: Quando for necess\u00e1rio pesquise a imagem. Ubuntu: um SO \u2b50 \u00b6 Realizaremos um exerc\u00edcio Fa\u00e7a o pull da imagem do Ubuntu docker pull ubuntu Rode a imagem no docker docker run ubuntu Ap\u00f3s ter realizado a etapa 1 e 2, porque a interface do Ubuntu n\u00e3o apareceu? Agora temos mais perguntas que respostas. Porque o Cont\u00eainer n\u00e3o rodou que nem o hello-world Se rodou o que o docker run fez ent\u00e3o?","title":"Docker Hub"},{"location":"docker/docker_hub/#o-que-e-o-docker-hub","text":"Para exemplificar, o Docker Hub \u00e9 um reposit\u00f3rio que possui imagens para o docker Para acessar Docker Hub Na p\u00e1gina inicial, pesquisaremos o hello-world. Pesquise e chegaremos nessa p\u00e1gina hello-world L\u00e1 encontraremos uma refer\u00eancia e informa\u00e7\u00f5es importantes Tamb\u00e9m repare que existe uma op\u00e7\u00e3o de fazer um pull, ou seja, baixa a imagem localmente Existe um selo oficial das imagens que est\u00e3o no docker hub: Quando for necess\u00e1rio pesquise a imagem.","title":"O que \u00e9 o Docker Hub? \ud83e\udd14"},{"location":"docker/docker_hub/#ubuntu-um-so","text":"Realizaremos um exerc\u00edcio Fa\u00e7a o pull da imagem do Ubuntu docker pull ubuntu Rode a imagem no docker docker run ubuntu Ap\u00f3s ter realizado a etapa 1 e 2, porque a interface do Ubuntu n\u00e3o apareceu? Agora temos mais perguntas que respostas. Porque o Cont\u00eainer n\u00e3o rodou que nem o hello-world Se rodou o que o docker run fez ent\u00e3o?","title":"Ubuntu: um SO \u2b50"},{"location":"docker/fluxo_containers/","text":"Fluxo de Cria\u00e7\u00e3o do Cont\u00eainer \u00b6 Vamos retorna para nossas perguntas Porque o Cont\u00eainer n\u00e3o rodou que nem o hello-world Se rodou o que o docker run fez ent\u00e3o? Para tentar responder vamos explorar mais alguns comandos do docker. Execute a fun\u00e7\u00e3o ps do docker para observar a lista dos cont\u00eaineres que est\u00e3o rodando docker ps Mesmo ap\u00f3s ter dado o comando docker run ubuntu, o docker ps somente retorna o cabe\u00e7alho da listagem, ou seja, n\u00e3o h\u00e1 nenhum cont\u00eainer em execu\u00e7\u00e3o. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Bem, se o docker ps n\u00e3o mostrou, tem um comando mais verboso para explicitar a listagem: docker container ls O docker cont\u00eainer ls lista os cont\u00eaineres como o ps. Por\u00e9m, o resultado foi o mesmo, para resolver, devemos acrescentar o verbo -a. docker ps docker container ls docker ps -a docker container ls -a Ap\u00f3s a execu\u00e7\u00e3o a sa\u00edda, vai ser parecida com essa: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 65adb2cb2cd5 ubuntu \"bash\" 4 seconds ago Exited ( 0 ) 4 seconds ago competent_thompson f690741c0bc7 ubuntu \"bash\" 47 seconds ago Exited ( 0 ) 47 seconds ago infallible_williams 6914d474838d hello-world \"/hello\" 2 hours ago Exited ( 0 ) 2 hours ago upbeat_lichterman Ent\u00e3o vamos entender o significado de cada cabe\u00e7alho. CONTAINER ID : \u00c9 o ID do cont\u00eainer, identifica\u00e7\u00e3o. IMAGE : \u00c9 a imagem do cont\u00eainer. COMMAND : \u00c9 o comando que est\u00e1 setado para executar CREATED : \u00c9 o tempo que passou ap\u00f3s a cria\u00e7\u00e3o STATUS : \u00c9 a lista de status created, restarting, running, removing, paused, exited. PORTS : \u00c9 a porta da rede que o docker est\u00e1 rodando, exemplo 8080 NAMES : S\u00e3o os nomes dos cont\u00eaineres Al\u00e9m disso, \u00e9 poss\u00edvel visualizar mais op\u00e7\u00f5es com o comando docker ps e docker container ls . Voltando para as quest\u00f5es \u00b6 Ent\u00e3o conforme a sa\u00edda do comando, o ubuntu subiu a imagem, executou o comando bash e finalizou. Certo, que tal, alterarmos o comando padr\u00e3o? Vamos pedir aquela ajuda do --help docker run --help Agora que retornou algumas op\u00e7\u00f5es para o comando, temos tamb\u00e9m a sintaxe do comando que \u00e9: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] Interessante, nessa sintaxe est\u00e1 dizendo que podemos executar comandos da imagem, no caso do ubuntu, podemos usar o terminal. docker run ubuntu sleep 2m Pedir para o ubuntu rodar um sleep de 2 minutos para verificamos o status desse novo cont\u00eainer. Ap\u00f3s enviar o comando ele travou o terminal, significa que est\u00e1 rodando. Abra uma nova aba do terminal e execute um docker ps -a para observar a sa\u00edda CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bb1bf42f3fc3 ubuntu \"sleep 2m\" 18 seconds ago Up 17 seconds frosty_ishizaka 65adb2cb2cd5 ubuntu \"bash\" 25 minutes ago Exited ( 0 ) 25 minutes ago competent_thompson f690741c0bc7 ubuntu \"bash\" 25 minutes ago Exited ( 0 ) 25 minutes ago infallible_williams 6914d474838d hello-world \"/hello\" 3 hours ago Exited ( 0 ) 3 hours ago upbeat_lichterman Op\u00e1, ai est\u00e1, o comando mudou, e o cont\u00eainer est\u00e1 com um status diferente Up , est\u00e1 indicando que est\u00e1 rodando. Comandos \u00dateis \u00b6 Certo, agora entendemos um pouco do fluxo que o docker faz para rodar uma imagem. Agora vamos aprender mais alguns comandos para auxiliar nessa jornada. Volte no terminal e execute: docker run ubuntu sleep 5m Ele vai congelar o terminal, abra uma nova aba ou terminal e execute o docker ps para visualizar o cont\u00eainer que est\u00e1 ativo e agora vamos parar esse cont\u00eainer que est\u00e1 em execu\u00e7\u00e3o. Execute agora o docker stop , mas passe o CONTAINER ID no seu terminal. docker stop 19bf7322160d Ap\u00f3s a execu\u00e7\u00e3o do comando ele vai parar o cont\u00eainer que estava ativo, para visualizar docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 19bf7322160d ubuntu \"sleep 5m\" 41 seconds ago Exited ( 137 ) 7 seconds ago wizardly_shtern bb1bf42f3fc3 ubuntu \"sleep 2m\" 15 minutes ago Exited ( 0 ) 13 minutes ago frosty_ishizaka 65adb2cb2cd5 ubuntu \"bash\" 40 minutes ago Exited ( 0 ) 40 minutes ago competent_thompson f690741c0bc7 ubuntu \"bash\" 40 minutes ago Exited ( 0 ) 40 minutes ago infallible_williams 6914d474838d hello-world \"/hello\" 3 hours ago Exited ( 0 ) 3 hours ago upbeat_lichterman Bem o stop para a execu\u00e7\u00e3o, que tal tentamos o docker start , basta passar o CONTAINER ID para iniciar o cont\u00eainer novamente: docker start 19bf7322160d Ele retorna o CONTAINER ID, basta executar o docker ps para validar que est\u00e1 rodando novamente, com isso voc\u00ea pode pausar com stop e reiniciar um cont\u00eainer com start. Mas, ainda n\u00e3o interagimos com o terminal do ubuntu, para conseguimos acessar o terminal do cont\u00eainer, vamos usar o comando docker exec , mas antes de executar vamos verificar o --help? docker exec --help Ele vai retorna a sintaxe e as op\u00e7\u00f5es, mas vamos verificar a sintaxe primeiro. docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Ele est\u00e1 nos dizendo que al\u00e9m das op\u00e7\u00f5es que nos forneceu, podemos d\u00e1 um comando no cont\u00eainer e que precisamos do CONTAINER ID, vamos tentar? Veja se tem algum cont\u00eainer com a imagem do ubuntu ativa, se n\u00e3o, ative usando o start. Agora verifique no ps e vamos l\u00e1. docker exec -it 19bf7322160d bash O que estou falando para o docker? Que a gente quer executar um comando interativo -i e alocar um pseudo-terminal -t no CONTAINER ID e executa o bash. Agora no seu terminal, deve est\u00e1 vendo a linha de comando do cont\u00eainer que voc\u00ea ativou. Nesse momento voc\u00ea deve est\u00e1 vendo algo +- assim: Legal n\u00e9, agora temos vamos falar do docker pause , podemos pausa um cont\u00eainer ativo: docker pause 19bf7322160d E para sair da pausa, vamos usar o docker unpause docker unpause 19bf7322160d Que tal removemos o cont\u00eainer que est\u00e1 inativo? Para isso vamos usar o docker rm docker rm 19bf7322160d Com isso voc\u00ea pode fazer a remo\u00e7\u00e3o dos cont\u00eaineres. Agora que aprendeu uns comandos novos \ud83d\ude42 \u00b6 Por \u00faltimo, que tal fazemos aquele: docker run ubuntu Contudo, sabemos agora que o que acontece quando o fazemos, vamos adicionar uma op\u00e7\u00e3o nesse comando, vamos inserir as op\u00e7\u00f5es -i e -t no comando para rodar o ubuntu e observar o que acontece. docker run -it ubuntu bash E agora nosso comando run, gerou o resultado que est\u00e1vamos esperando. Voc\u00ea est\u00e1 no terminal do cont\u00eainer. Portas \u00b6 Vamos acessar uma imagem um pouco diferente agora, j\u00e1 vimos um hello world e um SO, vamos ver um site est\u00e1tico imagem , voc\u00ea j\u00e1 deve saber o que \u00e9 preciso para rodar essa imagem, repita os passos: docker pull dockersamples/static-site Antes de usamos o docker run, vamos passar o comando -d, para ele n\u00e3o congelar no terminal enquanto usamos: docker run -d dockersamples/static-site Apos finalizar a execu\u00e7\u00e3o do comando, fa\u00e7a um docker ps para observar o resultado: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 26040d6c5529 dockersamples/static-site \"/bin/sh -c 'cd /usr\u2026\" 9 seconds ago Up 8 seconds 80 /tcp, 443 /tcp nervous_goldberg Perceba que temos agora uma informa\u00e7\u00e3o em PORTS , ele est\u00e1 dizendo que est\u00e1 dispon\u00edvel na porta 80, vamos verificar? No seu navegador: localhost:80 O navegador retorna um erro, vamos tentar a porta 443 e o resultado \u00e9 o mesmo, porque n\u00e3o conseguimos ver? Por causa da estrutura de isolamento dos cont\u00eaineres, n\u00e3o devemos esquecer que o isolamento est\u00e1 presente. Para resolver isso, remover o cont\u00eainer que acabamos de criar e passa a op\u00e7\u00e3o --force. docker rm 26040d6c5529 --force Ap\u00f3s a execu\u00e7\u00e3o, vamos passa mais uma op\u00e7\u00e3o para o comando run o -P que vai explicitar as portas e redirecionar automaticamente as portas do cont\u00eainer para as portas da nossa maquina: docker run -d -P dockersamples/static-site Use o docker ps para observar a sa\u00edda, e em PORTS a sa\u00edda est\u00e1 um pouco diferente, para visualizamos com mais clareza, vamos usar o docker port . docker port ca557128fd50 E a sa\u00edda vai ser \u00b1 assim: 443 /tcp -> 0 .0.0.0:49153 443 /tcp -> :::49153 80 /tcp -> 0 .0.0.0:49154 80 /tcp -> :::49154 A porta 443 foi redirecionada para a porta 49153 do nosso sistema e a 80 para 49154, por padr\u00e3o a porta 443 e 80 s\u00e3o sempre liberadas no cont\u00eaineres, vamos verificar? No navegador: localhost:49154 E a resposta do navegador vai ser essa aqui: Al\u00e9m do -P tamb\u00e9m tem o -p, que permite que fa\u00e7amos o mapeamento espec\u00edfico da porta, vamos tentar? Antes vou remover o cont\u00eainer atual: docker rm ca557128fd50 --force E vamos usar o comando: docker -d -p 8080 :80 dockersamples/static-site Explicando o que fiz, passei a porta 8080 da minha maquina, para refletir a porta 80 do cont\u00eainer.","title":"Fluxo"},{"location":"docker/fluxo_containers/#fluxo-de-criacao-do-conteiner","text":"Vamos retorna para nossas perguntas Porque o Cont\u00eainer n\u00e3o rodou que nem o hello-world Se rodou o que o docker run fez ent\u00e3o? Para tentar responder vamos explorar mais alguns comandos do docker. Execute a fun\u00e7\u00e3o ps do docker para observar a lista dos cont\u00eaineres que est\u00e3o rodando docker ps Mesmo ap\u00f3s ter dado o comando docker run ubuntu, o docker ps somente retorna o cabe\u00e7alho da listagem, ou seja, n\u00e3o h\u00e1 nenhum cont\u00eainer em execu\u00e7\u00e3o. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Bem, se o docker ps n\u00e3o mostrou, tem um comando mais verboso para explicitar a listagem: docker container ls O docker cont\u00eainer ls lista os cont\u00eaineres como o ps. Por\u00e9m, o resultado foi o mesmo, para resolver, devemos acrescentar o verbo -a. docker ps docker container ls docker ps -a docker container ls -a Ap\u00f3s a execu\u00e7\u00e3o a sa\u00edda, vai ser parecida com essa: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 65adb2cb2cd5 ubuntu \"bash\" 4 seconds ago Exited ( 0 ) 4 seconds ago competent_thompson f690741c0bc7 ubuntu \"bash\" 47 seconds ago Exited ( 0 ) 47 seconds ago infallible_williams 6914d474838d hello-world \"/hello\" 2 hours ago Exited ( 0 ) 2 hours ago upbeat_lichterman Ent\u00e3o vamos entender o significado de cada cabe\u00e7alho. CONTAINER ID : \u00c9 o ID do cont\u00eainer, identifica\u00e7\u00e3o. IMAGE : \u00c9 a imagem do cont\u00eainer. COMMAND : \u00c9 o comando que est\u00e1 setado para executar CREATED : \u00c9 o tempo que passou ap\u00f3s a cria\u00e7\u00e3o STATUS : \u00c9 a lista de status created, restarting, running, removing, paused, exited. PORTS : \u00c9 a porta da rede que o docker est\u00e1 rodando, exemplo 8080 NAMES : S\u00e3o os nomes dos cont\u00eaineres Al\u00e9m disso, \u00e9 poss\u00edvel visualizar mais op\u00e7\u00f5es com o comando docker ps e docker container ls .","title":"Fluxo de Cria\u00e7\u00e3o do Cont\u00eainer"},{"location":"docker/fluxo_containers/#voltando-para-as-questoes","text":"Ent\u00e3o conforme a sa\u00edda do comando, o ubuntu subiu a imagem, executou o comando bash e finalizou. Certo, que tal, alterarmos o comando padr\u00e3o? Vamos pedir aquela ajuda do --help docker run --help Agora que retornou algumas op\u00e7\u00f5es para o comando, temos tamb\u00e9m a sintaxe do comando que \u00e9: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] Interessante, nessa sintaxe est\u00e1 dizendo que podemos executar comandos da imagem, no caso do ubuntu, podemos usar o terminal. docker run ubuntu sleep 2m Pedir para o ubuntu rodar um sleep de 2 minutos para verificamos o status desse novo cont\u00eainer. Ap\u00f3s enviar o comando ele travou o terminal, significa que est\u00e1 rodando. Abra uma nova aba do terminal e execute um docker ps -a para observar a sa\u00edda CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bb1bf42f3fc3 ubuntu \"sleep 2m\" 18 seconds ago Up 17 seconds frosty_ishizaka 65adb2cb2cd5 ubuntu \"bash\" 25 minutes ago Exited ( 0 ) 25 minutes ago competent_thompson f690741c0bc7 ubuntu \"bash\" 25 minutes ago Exited ( 0 ) 25 minutes ago infallible_williams 6914d474838d hello-world \"/hello\" 3 hours ago Exited ( 0 ) 3 hours ago upbeat_lichterman Op\u00e1, ai est\u00e1, o comando mudou, e o cont\u00eainer est\u00e1 com um status diferente Up , est\u00e1 indicando que est\u00e1 rodando.","title":"Voltando para as quest\u00f5es"},{"location":"docker/fluxo_containers/#comandos-uteis","text":"Certo, agora entendemos um pouco do fluxo que o docker faz para rodar uma imagem. Agora vamos aprender mais alguns comandos para auxiliar nessa jornada. Volte no terminal e execute: docker run ubuntu sleep 5m Ele vai congelar o terminal, abra uma nova aba ou terminal e execute o docker ps para visualizar o cont\u00eainer que est\u00e1 ativo e agora vamos parar esse cont\u00eainer que est\u00e1 em execu\u00e7\u00e3o. Execute agora o docker stop , mas passe o CONTAINER ID no seu terminal. docker stop 19bf7322160d Ap\u00f3s a execu\u00e7\u00e3o do comando ele vai parar o cont\u00eainer que estava ativo, para visualizar docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 19bf7322160d ubuntu \"sleep 5m\" 41 seconds ago Exited ( 137 ) 7 seconds ago wizardly_shtern bb1bf42f3fc3 ubuntu \"sleep 2m\" 15 minutes ago Exited ( 0 ) 13 minutes ago frosty_ishizaka 65adb2cb2cd5 ubuntu \"bash\" 40 minutes ago Exited ( 0 ) 40 minutes ago competent_thompson f690741c0bc7 ubuntu \"bash\" 40 minutes ago Exited ( 0 ) 40 minutes ago infallible_williams 6914d474838d hello-world \"/hello\" 3 hours ago Exited ( 0 ) 3 hours ago upbeat_lichterman Bem o stop para a execu\u00e7\u00e3o, que tal tentamos o docker start , basta passar o CONTAINER ID para iniciar o cont\u00eainer novamente: docker start 19bf7322160d Ele retorna o CONTAINER ID, basta executar o docker ps para validar que est\u00e1 rodando novamente, com isso voc\u00ea pode pausar com stop e reiniciar um cont\u00eainer com start. Mas, ainda n\u00e3o interagimos com o terminal do ubuntu, para conseguimos acessar o terminal do cont\u00eainer, vamos usar o comando docker exec , mas antes de executar vamos verificar o --help? docker exec --help Ele vai retorna a sintaxe e as op\u00e7\u00f5es, mas vamos verificar a sintaxe primeiro. docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Ele est\u00e1 nos dizendo que al\u00e9m das op\u00e7\u00f5es que nos forneceu, podemos d\u00e1 um comando no cont\u00eainer e que precisamos do CONTAINER ID, vamos tentar? Veja se tem algum cont\u00eainer com a imagem do ubuntu ativa, se n\u00e3o, ative usando o start. Agora verifique no ps e vamos l\u00e1. docker exec -it 19bf7322160d bash O que estou falando para o docker? Que a gente quer executar um comando interativo -i e alocar um pseudo-terminal -t no CONTAINER ID e executa o bash. Agora no seu terminal, deve est\u00e1 vendo a linha de comando do cont\u00eainer que voc\u00ea ativou. Nesse momento voc\u00ea deve est\u00e1 vendo algo +- assim: Legal n\u00e9, agora temos vamos falar do docker pause , podemos pausa um cont\u00eainer ativo: docker pause 19bf7322160d E para sair da pausa, vamos usar o docker unpause docker unpause 19bf7322160d Que tal removemos o cont\u00eainer que est\u00e1 inativo? Para isso vamos usar o docker rm docker rm 19bf7322160d Com isso voc\u00ea pode fazer a remo\u00e7\u00e3o dos cont\u00eaineres.","title":"Comandos \u00dateis"},{"location":"docker/fluxo_containers/#agora-que-aprendeu-uns-comandos-novos","text":"Por \u00faltimo, que tal fazemos aquele: docker run ubuntu Contudo, sabemos agora que o que acontece quando o fazemos, vamos adicionar uma op\u00e7\u00e3o nesse comando, vamos inserir as op\u00e7\u00f5es -i e -t no comando para rodar o ubuntu e observar o que acontece. docker run -it ubuntu bash E agora nosso comando run, gerou o resultado que est\u00e1vamos esperando. Voc\u00ea est\u00e1 no terminal do cont\u00eainer.","title":"Agora que aprendeu uns comandos novos \ud83d\ude42"},{"location":"docker/fluxo_containers/#portas","text":"Vamos acessar uma imagem um pouco diferente agora, j\u00e1 vimos um hello world e um SO, vamos ver um site est\u00e1tico imagem , voc\u00ea j\u00e1 deve saber o que \u00e9 preciso para rodar essa imagem, repita os passos: docker pull dockersamples/static-site Antes de usamos o docker run, vamos passar o comando -d, para ele n\u00e3o congelar no terminal enquanto usamos: docker run -d dockersamples/static-site Apos finalizar a execu\u00e7\u00e3o do comando, fa\u00e7a um docker ps para observar o resultado: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 26040d6c5529 dockersamples/static-site \"/bin/sh -c 'cd /usr\u2026\" 9 seconds ago Up 8 seconds 80 /tcp, 443 /tcp nervous_goldberg Perceba que temos agora uma informa\u00e7\u00e3o em PORTS , ele est\u00e1 dizendo que est\u00e1 dispon\u00edvel na porta 80, vamos verificar? No seu navegador: localhost:80 O navegador retorna um erro, vamos tentar a porta 443 e o resultado \u00e9 o mesmo, porque n\u00e3o conseguimos ver? Por causa da estrutura de isolamento dos cont\u00eaineres, n\u00e3o devemos esquecer que o isolamento est\u00e1 presente. Para resolver isso, remover o cont\u00eainer que acabamos de criar e passa a op\u00e7\u00e3o --force. docker rm 26040d6c5529 --force Ap\u00f3s a execu\u00e7\u00e3o, vamos passa mais uma op\u00e7\u00e3o para o comando run o -P que vai explicitar as portas e redirecionar automaticamente as portas do cont\u00eainer para as portas da nossa maquina: docker run -d -P dockersamples/static-site Use o docker ps para observar a sa\u00edda, e em PORTS a sa\u00edda est\u00e1 um pouco diferente, para visualizamos com mais clareza, vamos usar o docker port . docker port ca557128fd50 E a sa\u00edda vai ser \u00b1 assim: 443 /tcp -> 0 .0.0.0:49153 443 /tcp -> :::49153 80 /tcp -> 0 .0.0.0:49154 80 /tcp -> :::49154 A porta 443 foi redirecionada para a porta 49153 do nosso sistema e a 80 para 49154, por padr\u00e3o a porta 443 e 80 s\u00e3o sempre liberadas no cont\u00eaineres, vamos verificar? No navegador: localhost:49154 E a resposta do navegador vai ser essa aqui: Al\u00e9m do -P tamb\u00e9m tem o -p, que permite que fa\u00e7amos o mapeamento espec\u00edfico da porta, vamos tentar? Antes vou remover o cont\u00eainer atual: docker rm ca557128fd50 --force E vamos usar o comando: docker -d -p 8080 :80 dockersamples/static-site Explicando o que fiz, passei a porta 8080 da minha maquina, para refletir a porta 80 do cont\u00eainer.","title":"Portas"},{"location":"docker/imagem/","text":"O que \u00e9 a imagem \u00b6 Pense nela como um conjunto de instru\u00e7\u00f5es para criar um cont\u00eainer no docker, como um modelo. A imagem do docker, cont\u00e9m c\u00f3digo de aplicativo, bibliotecas, ferramentas, depend\u00eancias e outros arquivos necess\u00e1rios para executar um aplicativo. Quando um usu\u00e1rio executa uma imagem, ela pode se tornar uma ou v\u00e1rias inst\u00e2ncias de um cont\u00eainer. As imagens do Docker t\u00eam v\u00e1rias camadas, cada uma se originando da camada anterior, mas difere dela. As camadas aceleram as compila\u00e7\u00f5es do Docker enquanto aumentam a capacidade de reutiliza\u00e7\u00e3o e diminuem o uso do disco. Camadas de imagem tamb\u00e9m s\u00e3o arquivos somente (leitura). Depois que um cont\u00eainer \u00e9 criado, uma camada grav\u00e1vel \u00e9 adicionada sobre as imagens imut\u00e1veis, permitindo que o usu\u00e1rio fa\u00e7a altera\u00e7\u00f5es. Sabemos sobre o docker hub e o sobre fluxo dos cont\u00eaineres, voc\u00ea deve estar se perguntando se \u00e9 poss\u00edvel ter nossa pr\u00f3pria imagem e para responder, vamos come\u00e7ar com o comando docker images . docker images Vai ter um retorno das imagens dispon\u00edveis localmente REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 54c9d81cbb44 4 weeks ago 72 .8MB postgres latest da2cb49d7a8d 4 weeks ago 374MB dpage/pgadmin4 latest e52ca07eba62 7 weeks ago 272MB hello-world latest feb5d9fea6a5 5 months ago 13 .3kB dockersamples/static-site latest f589ccde7957 5 years ago 190MB Vamos entender o cabe\u00e7alho: REPOSITORY : \u00c9 o reposit\u00f3rio da imagem TAG : \u00c9 a tag, geralmente associada a vers\u00e3o IMAGE ID : \u00c9 o ID da imagem, n\u00e3o confunda com o ID do cont\u00eainer CREATED : Data da cria\u00e7\u00e3o da imagem SIZE : Tamanho da imagem Copie o IMAGE ID de uma das suas imagens locais e vamos usar o comando docker inspect . docker inspect 54c9d81cbb44 A sa\u00edda retorna um JSON array por padr\u00e3o, mostrando informa\u00e7\u00f5es sobre a imagem, antes de continuamos, vamos observar o docker history . docker history 54c9d81cbb44 E ela vai retorna uma lista, o que \u00e9 essa lista? S\u00e3o a camadas que quando unidas(amarrado) formam a imagem do docker que usamos. E \u00e9 poss\u00edvel usar essas camadas em outras imagens docker, o que significa isso? Quando fazemos um pull de uma imagem, ele, na verdade, est\u00e1 baixando essas camadas, e o modelo(imagem) \u00e9 o respons\u00e1vel por unificar elas para formar o resultado, ent\u00e3o se voc\u00ea baixa uma imagem nova ela aproveitar as camadas que j\u00e1 est\u00e3o dispon\u00edveis, que fazem parte do seu modelo(imagem), e baixam somente as camadas que faltam. Essa reutiliza\u00e7\u00e3o otimiza o espa\u00e7o e evitar que ocorra duplica\u00e7\u00e3o de camadas. Criando nossa primeira imagem \u00b6 Para continuar a explica\u00e7\u00e3o da cria\u00e7\u00e3o da vamos apresentar uma nova categoria de arquivo, o dockerfile . Esse processo de cria\u00e7\u00e3o de imagem \u00e9 um pouco confuso, mas leia a documenta\u00e7\u00e3o do dockerfile para facilitar na compreens\u00e3o. Nesse processo vou subir uma aplica\u00e7\u00e3o flask. Fa\u00e7a uma pasta para esse projeto. Criar um arquivo chamado app.py na pasta. Crie uma pasta chamada templates na pasta do projeto. E por \u00faltimo um index.html na pasta templates. app.py index.html from flask import Flask , render_template import os app = Flask ( __name__ ) @app . route ( '/' ) def home (): return render_template ( 'index.html' ) if __name__ == \"__main__\" : port = int ( os . environ . get ( 'PORT' , 5000 )) app . run ( debug = True , host = '0.0.0.0' , port = port ) <!DOCTYPE html> < html lang = \"\" > < head > < meta charset = \"UTF-8\" > < title > Flask no Docker </ title > </ head > < body > < h1 > Eu amo Docker </ h1 > </ body > </ html > Se quiser mudar fazer altera\u00e7\u00e3o no html fique a vontade. Dentro do arquivo requirements.txt, insira: click == 8 .0.3 colorama == 0 .4.4 Flask == 2 .0.2 itsdangerous == 2 .0.1 Jinja2 == 3 .0.3 MarkupSafe == 2 .0.1 Werkzeug == 2 .0.2 gunicorn == 20 .1.0 S\u00f3 mais uma coisa, por padr\u00e3o o flask usa a porta 5000, ent\u00e3o vamos guarda essa informa\u00e7\u00e3o para depois. Dockerfile \u00b6 Crie um arquivo chamado Dockerfile na pasta do projeto. Insira no Dockerfile: # Iniciando a image do Python FROM python:3.10-alpine3.15 # Copiando o requirements para o /app da imagem COPY requirements.txt /app/requirements.txt # Expor a porta 5000 da aplica\u00e7\u00e3o flask EXPOSE 5000 # Mudando para pasta de trabalho WORKDIR /app # instalando as libs do requirements RUN pip install -r requirements.txt # Copiando o restante do conteudo no /app da imagem COPY . /app # Configurando o enrtypoint para rodar o python ENTRYPOINT [ \"python\" ] CMD [ \"app.py\" ] Antes de continuar com o processo, vou listar os comandos que usei no dockerfile para compreender esse processo que estamos fazendo. FROM python:3.10-alpine3.15 O comando FROM fala para o docker que estou buscando uma imagem, a imagem do python , e ele fornece essa estrutura do python para usamos, como base nesse processo. Lembrando que j\u00e1 existe milhares de imagens no docker hub. WORKDIR /app Aqui estamos dizendo ao docker qual pasta usar para continuar o processo de cria\u00e7\u00e3o da imagem. O comando WORKDIR \u00e9 o local de trabalho que ele deve usar. COPY requirements.txt /app/requirements.txt RUN pip install -r requirements.txt O comando COPY, diz para copiar o conte\u00fado do nosso diret\u00f3rio, no caso o requirements.txt para o diret\u00f3rio da imagem e em seguida executar o pip para instalar o requirements.txt. COPY . /app Continuando com a c\u00f3pia, agora copiamos o restante dos arquivos em nosso diret\u00f3rio de trabalho local para o diret\u00f3rio na imagem docker. ENTRYPOINT [ \"python\" ] ENTRYPOINT este \u00e9 o comando que executa a aplica\u00e7\u00e3o no cont\u00eainer. CMD [ \"app.py\" ] CMD anexa a lista de par\u00e2metros ao par\u00e2metro EntryPoint para executar o comando que executa o aplicativo. J\u00e1 revisamos o arquivo Dockerfile, est\u00e1 na hora de subir efetivamente a imagem, vamos usar um comando chamado docker build . Abra o terminal a partir da pasta que deve est\u00e1 com essa estrutura: app_web_flask \u251c\u2500\u2500 app.py \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 templates \u251c\u2500\u2500 index.html Com o terminal, aberto a partir da pasta vamos executar esse comando docker build -t univesp-github/app-flask-teste:1 . Antes de executar vamos compreender, toda essa instru\u00e7\u00e3o, o -t \u00e9 para nomear a imagem. O :1 para a vers\u00e3o da nossa imagem e o ponto no final para dizer que queremos que seja feito o build a partir do diret\u00f3rio atual. Para alterar o nome da imagem basta substituir o univesp-github/app-flask-teste . Na execu\u00e7\u00e3o ele roda o Dockerfiler, baixa as camadas do python e depois criar uma camada com a estrutura que criamos, para verificar, primeiro vamos dar um docker imagens : REPOSITORY TAG IMAGE ID CREATED SIZE univesp-github/app-flask-teste 1 0c1acf96e7e1 About a minute ago 126MB python 3 .10-slim-buster 41471b406cc5 4 days ago 115MB ubuntu latest 54c9d81cbb44 4 weeks ago 72 .8MB postgres latest da2cb49d7a8d 4 weeks ago 374MB dpage/pgadmin4 latest e52ca07eba62 7 weeks ago 272MB hello-world latest feb5d9fea6a5 5 months ago 13 .3kB dockersamples/static-site latest f589ccde7957 5 years ago 190MB E l\u00e1 est\u00e1 nossa imagem rec\u00e9m-criada, lembrando que a porta que o flask redireciona \u00e9 a porta 5000, para manter o padr\u00e3o vamos direcionar para porta 5000 da nossa maquina. docker run -d -p 5000 :5000 univesp-github/app-flask-teste:1 Agora no navegador, acessar a porta 5000. localhost:5000 Espero que tenha entendido o processo de cria\u00e7\u00e3o de imagem, lembrando que a documenta\u00e7\u00e3o dockerfile , as vari\u00e1veis que podemos inserir no Dockerfiler podem nos ajudar na constru\u00e7\u00e3o de imagens mais completas. Subindo a imagem para o Docker Hub \u00b6 Esse processo vai disponibilizar a imagem que fizemos, no reposit\u00f3rio do docker hub. Fa\u00e7a login em docker hub Fa\u00e7a login no terminal, para isso execute o comando docker login -u ap\u00f3s o -u inserir o seu nome de usu\u00e1rio. O terminal vai solicitar sua senha. Exemplo: docker login -u sposigor Logado no terminal, vamos realizar o docker push . Antes de continuar com o push, devemos corrigir o nome da imagem para o docker hub aceitar. Vamos usar o docker tag . Nome atual da imagem : univesp-github/app-flask-teste Nome alterado da imagem : seu usuario/app-flask-teste Com esse ajuste, vamos subir a imagem no reposit\u00f3rio atrelado ao usu\u00e1rio, se voc\u00ea logou no docker hub, deve ter observado o reposit\u00f3rio da sua conta. docker tag univesp-github/app-flask-teste:1 sposigor/app-flask-teste:1 N\u00e3o esque\u00e7a da vers\u00e3o, assim que a execu\u00e7\u00e3o finalizar, verifique as imagens docker images , observe a altera\u00e7\u00e3o e vamos fazer o push dessa imagem. docker push sposigor/app-flask-teste:1 V\u00e1 novamente no reposit\u00f3rio e veja sua imagem dispon\u00edvel. Para visualizar o resultado do meu push app-flask-teste .","title":"Imagem"},{"location":"docker/imagem/#o-que-e-a-imagem","text":"Pense nela como um conjunto de instru\u00e7\u00f5es para criar um cont\u00eainer no docker, como um modelo. A imagem do docker, cont\u00e9m c\u00f3digo de aplicativo, bibliotecas, ferramentas, depend\u00eancias e outros arquivos necess\u00e1rios para executar um aplicativo. Quando um usu\u00e1rio executa uma imagem, ela pode se tornar uma ou v\u00e1rias inst\u00e2ncias de um cont\u00eainer. As imagens do Docker t\u00eam v\u00e1rias camadas, cada uma se originando da camada anterior, mas difere dela. As camadas aceleram as compila\u00e7\u00f5es do Docker enquanto aumentam a capacidade de reutiliza\u00e7\u00e3o e diminuem o uso do disco. Camadas de imagem tamb\u00e9m s\u00e3o arquivos somente (leitura). Depois que um cont\u00eainer \u00e9 criado, uma camada grav\u00e1vel \u00e9 adicionada sobre as imagens imut\u00e1veis, permitindo que o usu\u00e1rio fa\u00e7a altera\u00e7\u00f5es. Sabemos sobre o docker hub e o sobre fluxo dos cont\u00eaineres, voc\u00ea deve estar se perguntando se \u00e9 poss\u00edvel ter nossa pr\u00f3pria imagem e para responder, vamos come\u00e7ar com o comando docker images . docker images Vai ter um retorno das imagens dispon\u00edveis localmente REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 54c9d81cbb44 4 weeks ago 72 .8MB postgres latest da2cb49d7a8d 4 weeks ago 374MB dpage/pgadmin4 latest e52ca07eba62 7 weeks ago 272MB hello-world latest feb5d9fea6a5 5 months ago 13 .3kB dockersamples/static-site latest f589ccde7957 5 years ago 190MB Vamos entender o cabe\u00e7alho: REPOSITORY : \u00c9 o reposit\u00f3rio da imagem TAG : \u00c9 a tag, geralmente associada a vers\u00e3o IMAGE ID : \u00c9 o ID da imagem, n\u00e3o confunda com o ID do cont\u00eainer CREATED : Data da cria\u00e7\u00e3o da imagem SIZE : Tamanho da imagem Copie o IMAGE ID de uma das suas imagens locais e vamos usar o comando docker inspect . docker inspect 54c9d81cbb44 A sa\u00edda retorna um JSON array por padr\u00e3o, mostrando informa\u00e7\u00f5es sobre a imagem, antes de continuamos, vamos observar o docker history . docker history 54c9d81cbb44 E ela vai retorna uma lista, o que \u00e9 essa lista? S\u00e3o a camadas que quando unidas(amarrado) formam a imagem do docker que usamos. E \u00e9 poss\u00edvel usar essas camadas em outras imagens docker, o que significa isso? Quando fazemos um pull de uma imagem, ele, na verdade, est\u00e1 baixando essas camadas, e o modelo(imagem) \u00e9 o respons\u00e1vel por unificar elas para formar o resultado, ent\u00e3o se voc\u00ea baixa uma imagem nova ela aproveitar as camadas que j\u00e1 est\u00e3o dispon\u00edveis, que fazem parte do seu modelo(imagem), e baixam somente as camadas que faltam. Essa reutiliza\u00e7\u00e3o otimiza o espa\u00e7o e evitar que ocorra duplica\u00e7\u00e3o de camadas.","title":"O que \u00e9 a imagem"},{"location":"docker/imagem/#criando-nossa-primeira-imagem","text":"Para continuar a explica\u00e7\u00e3o da cria\u00e7\u00e3o da vamos apresentar uma nova categoria de arquivo, o dockerfile . Esse processo de cria\u00e7\u00e3o de imagem \u00e9 um pouco confuso, mas leia a documenta\u00e7\u00e3o do dockerfile para facilitar na compreens\u00e3o. Nesse processo vou subir uma aplica\u00e7\u00e3o flask. Fa\u00e7a uma pasta para esse projeto. Criar um arquivo chamado app.py na pasta. Crie uma pasta chamada templates na pasta do projeto. E por \u00faltimo um index.html na pasta templates. app.py index.html from flask import Flask , render_template import os app = Flask ( __name__ ) @app . route ( '/' ) def home (): return render_template ( 'index.html' ) if __name__ == \"__main__\" : port = int ( os . environ . get ( 'PORT' , 5000 )) app . run ( debug = True , host = '0.0.0.0' , port = port ) <!DOCTYPE html> < html lang = \"\" > < head > < meta charset = \"UTF-8\" > < title > Flask no Docker </ title > </ head > < body > < h1 > Eu amo Docker </ h1 > </ body > </ html > Se quiser mudar fazer altera\u00e7\u00e3o no html fique a vontade. Dentro do arquivo requirements.txt, insira: click == 8 .0.3 colorama == 0 .4.4 Flask == 2 .0.2 itsdangerous == 2 .0.1 Jinja2 == 3 .0.3 MarkupSafe == 2 .0.1 Werkzeug == 2 .0.2 gunicorn == 20 .1.0 S\u00f3 mais uma coisa, por padr\u00e3o o flask usa a porta 5000, ent\u00e3o vamos guarda essa informa\u00e7\u00e3o para depois.","title":"Criando nossa primeira imagem"},{"location":"docker/imagem/#dockerfile","text":"Crie um arquivo chamado Dockerfile na pasta do projeto. Insira no Dockerfile: # Iniciando a image do Python FROM python:3.10-alpine3.15 # Copiando o requirements para o /app da imagem COPY requirements.txt /app/requirements.txt # Expor a porta 5000 da aplica\u00e7\u00e3o flask EXPOSE 5000 # Mudando para pasta de trabalho WORKDIR /app # instalando as libs do requirements RUN pip install -r requirements.txt # Copiando o restante do conteudo no /app da imagem COPY . /app # Configurando o enrtypoint para rodar o python ENTRYPOINT [ \"python\" ] CMD [ \"app.py\" ] Antes de continuar com o processo, vou listar os comandos que usei no dockerfile para compreender esse processo que estamos fazendo. FROM python:3.10-alpine3.15 O comando FROM fala para o docker que estou buscando uma imagem, a imagem do python , e ele fornece essa estrutura do python para usamos, como base nesse processo. Lembrando que j\u00e1 existe milhares de imagens no docker hub. WORKDIR /app Aqui estamos dizendo ao docker qual pasta usar para continuar o processo de cria\u00e7\u00e3o da imagem. O comando WORKDIR \u00e9 o local de trabalho que ele deve usar. COPY requirements.txt /app/requirements.txt RUN pip install -r requirements.txt O comando COPY, diz para copiar o conte\u00fado do nosso diret\u00f3rio, no caso o requirements.txt para o diret\u00f3rio da imagem e em seguida executar o pip para instalar o requirements.txt. COPY . /app Continuando com a c\u00f3pia, agora copiamos o restante dos arquivos em nosso diret\u00f3rio de trabalho local para o diret\u00f3rio na imagem docker. ENTRYPOINT [ \"python\" ] ENTRYPOINT este \u00e9 o comando que executa a aplica\u00e7\u00e3o no cont\u00eainer. CMD [ \"app.py\" ] CMD anexa a lista de par\u00e2metros ao par\u00e2metro EntryPoint para executar o comando que executa o aplicativo. J\u00e1 revisamos o arquivo Dockerfile, est\u00e1 na hora de subir efetivamente a imagem, vamos usar um comando chamado docker build . Abra o terminal a partir da pasta que deve est\u00e1 com essa estrutura: app_web_flask \u251c\u2500\u2500 app.py \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 templates \u251c\u2500\u2500 index.html Com o terminal, aberto a partir da pasta vamos executar esse comando docker build -t univesp-github/app-flask-teste:1 . Antes de executar vamos compreender, toda essa instru\u00e7\u00e3o, o -t \u00e9 para nomear a imagem. O :1 para a vers\u00e3o da nossa imagem e o ponto no final para dizer que queremos que seja feito o build a partir do diret\u00f3rio atual. Para alterar o nome da imagem basta substituir o univesp-github/app-flask-teste . Na execu\u00e7\u00e3o ele roda o Dockerfiler, baixa as camadas do python e depois criar uma camada com a estrutura que criamos, para verificar, primeiro vamos dar um docker imagens : REPOSITORY TAG IMAGE ID CREATED SIZE univesp-github/app-flask-teste 1 0c1acf96e7e1 About a minute ago 126MB python 3 .10-slim-buster 41471b406cc5 4 days ago 115MB ubuntu latest 54c9d81cbb44 4 weeks ago 72 .8MB postgres latest da2cb49d7a8d 4 weeks ago 374MB dpage/pgadmin4 latest e52ca07eba62 7 weeks ago 272MB hello-world latest feb5d9fea6a5 5 months ago 13 .3kB dockersamples/static-site latest f589ccde7957 5 years ago 190MB E l\u00e1 est\u00e1 nossa imagem rec\u00e9m-criada, lembrando que a porta que o flask redireciona \u00e9 a porta 5000, para manter o padr\u00e3o vamos direcionar para porta 5000 da nossa maquina. docker run -d -p 5000 :5000 univesp-github/app-flask-teste:1 Agora no navegador, acessar a porta 5000. localhost:5000 Espero que tenha entendido o processo de cria\u00e7\u00e3o de imagem, lembrando que a documenta\u00e7\u00e3o dockerfile , as vari\u00e1veis que podemos inserir no Dockerfiler podem nos ajudar na constru\u00e7\u00e3o de imagens mais completas.","title":"Dockerfile"},{"location":"docker/imagem/#subindo-a-imagem-para-o-docker-hub","text":"Esse processo vai disponibilizar a imagem que fizemos, no reposit\u00f3rio do docker hub. Fa\u00e7a login em docker hub Fa\u00e7a login no terminal, para isso execute o comando docker login -u ap\u00f3s o -u inserir o seu nome de usu\u00e1rio. O terminal vai solicitar sua senha. Exemplo: docker login -u sposigor Logado no terminal, vamos realizar o docker push . Antes de continuar com o push, devemos corrigir o nome da imagem para o docker hub aceitar. Vamos usar o docker tag . Nome atual da imagem : univesp-github/app-flask-teste Nome alterado da imagem : seu usuario/app-flask-teste Com esse ajuste, vamos subir a imagem no reposit\u00f3rio atrelado ao usu\u00e1rio, se voc\u00ea logou no docker hub, deve ter observado o reposit\u00f3rio da sua conta. docker tag univesp-github/app-flask-teste:1 sposigor/app-flask-teste:1 N\u00e3o esque\u00e7a da vers\u00e3o, assim que a execu\u00e7\u00e3o finalizar, verifique as imagens docker images , observe a altera\u00e7\u00e3o e vamos fazer o push dessa imagem. docker push sposigor/app-flask-teste:1 V\u00e1 novamente no reposit\u00f3rio e veja sua imagem dispon\u00edvel. Para visualizar o resultado do meu push app-flask-teste .","title":"Subindo a imagem para o Docker Hub"},{"location":"docker/instala%C3%A7%C3%A3o/","text":"Para Instalar \u2728 \u00b6 Vamos iniciar com a instala\u00e7\u00e3o do docker. Windows Linux Mac A documenta\u00e7\u00e3o: Instala\u00e7\u00e3o no Windows Vou deixa aqui um passo a passo que vai ajudar nesse processo de instala\u00e7\u00e3o no windows. Lembre-se de que o Docker para Windows possui restri\u00e7\u00f5es acerca de utiliza\u00e7\u00e3o e instabilidades, sendo prefer\u00edvel usar a vers\u00e3o para Linux A documenta\u00e7\u00e3o para verificar as distros: Distros Selecione sua distribui\u00e7\u00e3o e continue com a instala\u00e7\u00e3o Tamb\u00e9m \u00e9 possivel usar o repositorio oficial da sua distro e op\u00e7\u00f5es como o Snap e Flatpak non-root user: para rodar o docker no Linux, temos que chamar o sudo todas \u00e0s vezes, para contornar isso aqui documenta\u00e7\u00e3o Ou basta usar o comando: sudo usermod -aG docker $USER A documenta\u00e7\u00e3o: Instala\u00e7\u00e3o no MAC Lembre-se de que o Docker para Mac possui restri\u00e7\u00f5es acerca de utiliza\u00e7\u00e3o e instabilidades, sendo prefer\u00edvel usar a vers\u00e3o para Linux Hello World \ud83e\ude90 \u00b6 Ap\u00f3s a instala\u00e7\u00e3o abra seu terminal e execute: Windows Linux Mac docker run hello-world docker run hello-world Se o erro do docker daemon ocorrer basta acessar a documenta\u00e7\u00e3o Para rodar manualmente o backend, abra o terminal e use o comando: sudo dockerd \u00c9 necess\u00e1rio manter o terminal aberto rodando o backend.] Para finalizar o processo no terminal, ctrl + c docker run hello-world Entendendo o Hello World \u00b6 Se tudo ocorreu bem no hello world na saida vai ser assim: docker run hello-world 126 \u2718 Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 2db29710123e: Pull complete Digest: sha256:97a379f4f88575512824f3b352bc03cd75e239179eea0fecc38e597b2209f49a Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Essa saida nos d\u00e1 informa\u00e7\u00f5es importantes, ent\u00e3o vamos analisar por partes: Unable to find image 'hello-world:latest' locally Esse trecho fala sobre uma imagem que n\u00e3o foi encontrada localmente, o proximo trecho: latest: Pulling from library/hello-world 2db29710123e: Pull complete Digest: sha256:97a379f4f88575512824f3b352bc03cd75e239179eea0fecc38e597b2209f49a Status: Downloaded newer image for hello-world:latest J\u00e1 nesse trecho o docker est\u00e1 baixando uma imagem de algum lugar e ap\u00f3s isso ele executa o hello world normalmente. Afinal o que aconteceu? \u00b6 Para exemplificar o docker n\u00e3o encontrou a imagem local e baixou de um reposit\u00f3rio, mas qual? Espiar a resposta O reposit\u00f3rio com as imagens \u00e9 o Docker Hub.","title":"Instala\u00e7\u00e3o"},{"location":"docker/instala%C3%A7%C3%A3o/#para-instalar","text":"Vamos iniciar com a instala\u00e7\u00e3o do docker. Windows Linux Mac A documenta\u00e7\u00e3o: Instala\u00e7\u00e3o no Windows Vou deixa aqui um passo a passo que vai ajudar nesse processo de instala\u00e7\u00e3o no windows. Lembre-se de que o Docker para Windows possui restri\u00e7\u00f5es acerca de utiliza\u00e7\u00e3o e instabilidades, sendo prefer\u00edvel usar a vers\u00e3o para Linux A documenta\u00e7\u00e3o para verificar as distros: Distros Selecione sua distribui\u00e7\u00e3o e continue com a instala\u00e7\u00e3o Tamb\u00e9m \u00e9 possivel usar o repositorio oficial da sua distro e op\u00e7\u00f5es como o Snap e Flatpak non-root user: para rodar o docker no Linux, temos que chamar o sudo todas \u00e0s vezes, para contornar isso aqui documenta\u00e7\u00e3o Ou basta usar o comando: sudo usermod -aG docker $USER A documenta\u00e7\u00e3o: Instala\u00e7\u00e3o no MAC Lembre-se de que o Docker para Mac possui restri\u00e7\u00f5es acerca de utiliza\u00e7\u00e3o e instabilidades, sendo prefer\u00edvel usar a vers\u00e3o para Linux","title":"Para Instalar \u2728"},{"location":"docker/instala%C3%A7%C3%A3o/#hello-world","text":"Ap\u00f3s a instala\u00e7\u00e3o abra seu terminal e execute: Windows Linux Mac docker run hello-world docker run hello-world Se o erro do docker daemon ocorrer basta acessar a documenta\u00e7\u00e3o Para rodar manualmente o backend, abra o terminal e use o comando: sudo dockerd \u00c9 necess\u00e1rio manter o terminal aberto rodando o backend.] Para finalizar o processo no terminal, ctrl + c docker run hello-world","title":"Hello World \ud83e\ude90"},{"location":"docker/instala%C3%A7%C3%A3o/#entendendo-o-hello-world","text":"Se tudo ocorreu bem no hello world na saida vai ser assim: docker run hello-world 126 \u2718 Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 2db29710123e: Pull complete Digest: sha256:97a379f4f88575512824f3b352bc03cd75e239179eea0fecc38e597b2209f49a Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Essa saida nos d\u00e1 informa\u00e7\u00f5es importantes, ent\u00e3o vamos analisar por partes: Unable to find image 'hello-world:latest' locally Esse trecho fala sobre uma imagem que n\u00e3o foi encontrada localmente, o proximo trecho: latest: Pulling from library/hello-world 2db29710123e: Pull complete Digest: sha256:97a379f4f88575512824f3b352bc03cd75e239179eea0fecc38e597b2209f49a Status: Downloaded newer image for hello-world:latest J\u00e1 nesse trecho o docker est\u00e1 baixando uma imagem de algum lugar e ap\u00f3s isso ele executa o hello world normalmente.","title":"Entendendo o Hello World"},{"location":"docker/instala%C3%A7%C3%A3o/#afinal-o-que-aconteceu","text":"Para exemplificar o docker n\u00e3o encontrou a imagem local e baixou de um reposit\u00f3rio, mas qual? Espiar a resposta O reposit\u00f3rio com as imagens \u00e9 o Docker Hub.","title":"Afinal o que aconteceu?"},{"location":"docker/redes/","text":"Rede Bridge - Ponte \u00b6 A rede, uma rede de ponte \u00e9 um dispositivo de camada de link que encaminha o tr\u00e1fego entre os segmentos de rede. Uma ponte pode ser um dispositivo de hardware ou um dispositivo de software executado no kernel de uma m\u00e1quina host. No Docker, uma rede de ponte usa uma ponte de software que permite que os cont\u00eaineres conectados \u00e0 mesma rede de ponte se comuniquem, enquanto fornece isolamento de cont\u00eaineres que n\u00e3o est\u00e3o conectados a essa rede de ponte. O driver de ponte do Docker instala automaticamente as regras na m\u00e1quina host para que os cont\u00eaineres em diferentes redes de ponte n\u00e3o possam se comunicar diretamente entre si. As redes de ponte se aplicam a cont\u00eaineres em execu\u00e7\u00e3o no mesmo host do daemon do Docker. Para comunica\u00e7\u00e3o entre cont\u00eaineres em execu\u00e7\u00e3o em diferentes hosts do daemon do Docker, voc\u00ea pode gerenciar o roteamento no n\u00edvel do sistema operacional ou usar uma rede de sobreposi\u00e7\u00e3o. Quando voc\u00ea inicia o Docker, uma rede de ponte padr\u00e3o (tamb\u00e9m chamada Bridge) \u00e9 criada automaticamente e os cont\u00eaineres rec\u00e9m-iniciados se conectam a ela, a menos que especificado de outra forma. Voc\u00ea tamb\u00e9m pode criar redes de ponte personalizadas definidas pelo usu\u00e1rio. As redes de ponte definidas pelo usu\u00e1rio s\u00e3o superiores \u00e0 rede de ponte padr\u00e3o. Realizaremos um teste: execute o docker run -it ubuntu bash , deixei o terminal aberto e abra uma nova aba no terminal Nesse terminal execute docker ps , copie o id do cont\u00eainer e vamos usar o docker inspect docker inspect 8c9e5c315c0f A sa\u00edda vai ser um JSON array e l\u00e1 no final vai esta as informa\u00e7\u00f5es sobre a rede: \"Networks\" : { \"bridge\" : { \"IPAMConfig\" : null , \"Links\" : null , \"Aliases\" : null , \"NetworkID\" : \"f2265fd0f92380df4ff74210e72ae304c76b3720cfb07bb134d97aa6ed215d70\" , \"EndpointID\" : \"86e57b1d3e2f1b3019246d6b50da7d88744c6bdc4acdb62f8b69236d1407185b\" , \"Gateway\" : \"172.17.0.1\" , \"IPAddress\" : \"172.17.0.2\" , \"IPPrefixLen\" : 16 , \"IPv6Gateway\" : \"\" , \"GlobalIPv6Address\" : \"\" , \"GlobalIPv6PrefixLen\" : 0 , \"MacAddress\" : \"02:42:ac:11:00:02\" , \"DriverOpts\" : null } Entre todas essas configura\u00e7\u00f5es, temos o NetworkID que \u00e9 a rede do docker0 , ou seja, por padr\u00e3o o docker mant\u00e9m os mesmo para todos os containers NetworkID . Execute o docker network ls para verificar a saida: NETWORK ID NAME DRIVER SCOPE f2265fd0f923 bridge bridge local 95dc365e7cbf host host local 093d70e2475c none null local NETWORK ID : Id da rede NAME : Nome da rede DRIVER : Driver da rede SCOPE : \u00c9 o alcance, com as op\u00e7\u00f5es de local ou global Para mais informa\u00e7\u00f5es docker network . Testando a rede Bridge \u00b6 Para isso, vamos usar dois cont\u00eaineres ubuntu: Ubuntu 1 Ubuntu 2 docker run -it ubuntu bash Quando o bash iniciar: apt-get update E vamos instalar o ping apt-get install iputils-ping -y docker run -it ubuntu bash Assim que o bash iniciar deixei esse ubuntu de lado Precisamos do IP dos cont\u00eaineres, para isso vamos usar docker ps e em seguida o docker inspect Geralmente o Gateway \u00e9 172.17.0.1 e os cont\u00eaineres continuar a numera\u00e7\u00e3o. Fa\u00e7a uma anota\u00e7\u00e3o de qual \u00e9 o cont\u00eainer/IP para usar. Use no ubuntu 1 o comando ping e passe o IP do ubuntu 2. No meu caso a sa\u00edda ficou assim: root@8c9e5c315c0f:/# ping 172 .17.0.2 PING 172 .17.0.2 ( 172 .17.0.2 ) 56 ( 84 ) bytes of data. 64 bytes from 172 .17.0.2: icmp_seq = 1 ttl = 64 time = 0 .313 ms 64 bytes from 172 .17.0.2: icmp_seq = 2 ttl = 64 time = 0 .147 ms 64 bytes from 172 .17.0.2: icmp_seq = 3 ttl = 64 time = 0 .123 ms 64 bytes from 172 .17.0.2: icmp_seq = 4 ttl = 64 time = 0 .151 ms Para finalizar a execu\u00e7\u00e3o basta aperta ctrl + c, e acabamos de usar a rede do docker para realizar a comunica\u00e7\u00e3o entre dois cont\u00eaineres. Melhorando a comunica\u00e7\u00e3o \u00b6 Bem, deve ter percebido que ficar procurando IP dos cont\u00eaineres e anotando cada IP n\u00e3o \u00e9 uma boa ideia, al\u00e9m de que os cont\u00eaineres podem perder a conex\u00e3o ou cair, pensando nesse problema o docker criou uma coluna NAME no cont\u00eaineres e com ela que vamos resolver todo esse problema. Vamos passa a flag --name durante o docker run para nomear nossos cont\u00eaineres e vamos criar nossa rede, \u00e9 um processo bem simples: Usaremos o docker network create para criar uma rede, vamos usar a flag --driver para passar o driver bridge. docker network create --driver bridge rede-teste E vamos passar nossa rede na cria\u00e7\u00e3o do cont\u00eainer: Pong Ping docker run -it --name pong --network rede-teste ubuntu sleep 30m docker run -it --name ubuntu1 --network rede-teste ubuntu bash Quando o bash iniciar: apt-get update E vamos instalar o ping apt-get install iputils-ping -y Execute o ping, mas agora vamos passar o nome do cont\u00eainer e n\u00e3o o IP ping pong A diferen\u00e7a entre o padr\u00e3o do docker bridge network e o user-defined bridges \u00e9 que quando criamos nossa pr\u00f3pria rede bridge, no processo \u00e9 provido uma solu\u00e7\u00e3o DNS autom\u00e1tica nos cont\u00eaineres, possibilitando esse teste que fizemos, sem ocorrer a necessidade de mais configura\u00e7\u00f5es. Quando executamos o docker network ls , verificamos que temos mais duas categorias de drivers de redes dispon\u00edveis, none e host . A rede com o driver none basicamente n\u00e3o criar conex\u00f5es no cont\u00eainer, removendo a interface de rede: docker run -d --network none ubuntu sleep 30m De um docker inspect nesse cont\u00eainer e o resultado: \"Networks\" : { \"none\" : { \"IPAMConfig\" : null , \"Links\" : null , \"Aliases\" : null , \"NetworkID\" : \"093d70e2475c6ddd793426f4f6d3aa5560fd946ae182219aff56b2f406f1d290\" , \"EndpointID\" : \"59be8bf2afca5f2a103aa48fe86a85a676dd8ffe48a4d1101d128dcb5e9ad8b1\" , \"Gateway\" : \"\" , \"IPAddress\" : \"\" , \"IPPrefixLen\" : 0 , \"IPv6Gateway\" : \"\" , \"GlobalIPv6Address\" : \"\" , \"GlobalIPv6PrefixLen\" : 0 , \"MacAddress\" : \"\" , \"DriverOpts\" : null } } Os campos est\u00e3o sem informa\u00e7\u00e3o de IP ou est\u00e3o com null. Isolando completamente o cont\u00eainer. Enquanto o driver host , elimina o isolamento proporcionado pela estrutura do docker, ou seja, acessa a rede diretamente da host machine: docker run -d --network host sposigor/app-flask-teste:1 Se lembra que no projeto o flask define a porta 5000, e para acessar no navegador era preciso passa a flag -p para redirecionar? Com o driver host da rede, basta acessar diretamente, copie e cole no navegador: localhost:5000 Para lembrar ainda \u00e9 poss\u00edvel ter problemas de portas, ou seja, se houve mais alguma aplica\u00e7\u00e3o usando a porta 5000 vai ocorrer um erro.","title":"Rede"},{"location":"docker/redes/#rede-bridge-ponte","text":"A rede, uma rede de ponte \u00e9 um dispositivo de camada de link que encaminha o tr\u00e1fego entre os segmentos de rede. Uma ponte pode ser um dispositivo de hardware ou um dispositivo de software executado no kernel de uma m\u00e1quina host. No Docker, uma rede de ponte usa uma ponte de software que permite que os cont\u00eaineres conectados \u00e0 mesma rede de ponte se comuniquem, enquanto fornece isolamento de cont\u00eaineres que n\u00e3o est\u00e3o conectados a essa rede de ponte. O driver de ponte do Docker instala automaticamente as regras na m\u00e1quina host para que os cont\u00eaineres em diferentes redes de ponte n\u00e3o possam se comunicar diretamente entre si. As redes de ponte se aplicam a cont\u00eaineres em execu\u00e7\u00e3o no mesmo host do daemon do Docker. Para comunica\u00e7\u00e3o entre cont\u00eaineres em execu\u00e7\u00e3o em diferentes hosts do daemon do Docker, voc\u00ea pode gerenciar o roteamento no n\u00edvel do sistema operacional ou usar uma rede de sobreposi\u00e7\u00e3o. Quando voc\u00ea inicia o Docker, uma rede de ponte padr\u00e3o (tamb\u00e9m chamada Bridge) \u00e9 criada automaticamente e os cont\u00eaineres rec\u00e9m-iniciados se conectam a ela, a menos que especificado de outra forma. Voc\u00ea tamb\u00e9m pode criar redes de ponte personalizadas definidas pelo usu\u00e1rio. As redes de ponte definidas pelo usu\u00e1rio s\u00e3o superiores \u00e0 rede de ponte padr\u00e3o. Realizaremos um teste: execute o docker run -it ubuntu bash , deixei o terminal aberto e abra uma nova aba no terminal Nesse terminal execute docker ps , copie o id do cont\u00eainer e vamos usar o docker inspect docker inspect 8c9e5c315c0f A sa\u00edda vai ser um JSON array e l\u00e1 no final vai esta as informa\u00e7\u00f5es sobre a rede: \"Networks\" : { \"bridge\" : { \"IPAMConfig\" : null , \"Links\" : null , \"Aliases\" : null , \"NetworkID\" : \"f2265fd0f92380df4ff74210e72ae304c76b3720cfb07bb134d97aa6ed215d70\" , \"EndpointID\" : \"86e57b1d3e2f1b3019246d6b50da7d88744c6bdc4acdb62f8b69236d1407185b\" , \"Gateway\" : \"172.17.0.1\" , \"IPAddress\" : \"172.17.0.2\" , \"IPPrefixLen\" : 16 , \"IPv6Gateway\" : \"\" , \"GlobalIPv6Address\" : \"\" , \"GlobalIPv6PrefixLen\" : 0 , \"MacAddress\" : \"02:42:ac:11:00:02\" , \"DriverOpts\" : null } Entre todas essas configura\u00e7\u00f5es, temos o NetworkID que \u00e9 a rede do docker0 , ou seja, por padr\u00e3o o docker mant\u00e9m os mesmo para todos os containers NetworkID . Execute o docker network ls para verificar a saida: NETWORK ID NAME DRIVER SCOPE f2265fd0f923 bridge bridge local 95dc365e7cbf host host local 093d70e2475c none null local NETWORK ID : Id da rede NAME : Nome da rede DRIVER : Driver da rede SCOPE : \u00c9 o alcance, com as op\u00e7\u00f5es de local ou global Para mais informa\u00e7\u00f5es docker network .","title":"Rede Bridge - Ponte"},{"location":"docker/redes/#testando-a-rede-bridge","text":"Para isso, vamos usar dois cont\u00eaineres ubuntu: Ubuntu 1 Ubuntu 2 docker run -it ubuntu bash Quando o bash iniciar: apt-get update E vamos instalar o ping apt-get install iputils-ping -y docker run -it ubuntu bash Assim que o bash iniciar deixei esse ubuntu de lado Precisamos do IP dos cont\u00eaineres, para isso vamos usar docker ps e em seguida o docker inspect Geralmente o Gateway \u00e9 172.17.0.1 e os cont\u00eaineres continuar a numera\u00e7\u00e3o. Fa\u00e7a uma anota\u00e7\u00e3o de qual \u00e9 o cont\u00eainer/IP para usar. Use no ubuntu 1 o comando ping e passe o IP do ubuntu 2. No meu caso a sa\u00edda ficou assim: root@8c9e5c315c0f:/# ping 172 .17.0.2 PING 172 .17.0.2 ( 172 .17.0.2 ) 56 ( 84 ) bytes of data. 64 bytes from 172 .17.0.2: icmp_seq = 1 ttl = 64 time = 0 .313 ms 64 bytes from 172 .17.0.2: icmp_seq = 2 ttl = 64 time = 0 .147 ms 64 bytes from 172 .17.0.2: icmp_seq = 3 ttl = 64 time = 0 .123 ms 64 bytes from 172 .17.0.2: icmp_seq = 4 ttl = 64 time = 0 .151 ms Para finalizar a execu\u00e7\u00e3o basta aperta ctrl + c, e acabamos de usar a rede do docker para realizar a comunica\u00e7\u00e3o entre dois cont\u00eaineres.","title":"Testando a rede Bridge"},{"location":"docker/redes/#melhorando-a-comunicacao","text":"Bem, deve ter percebido que ficar procurando IP dos cont\u00eaineres e anotando cada IP n\u00e3o \u00e9 uma boa ideia, al\u00e9m de que os cont\u00eaineres podem perder a conex\u00e3o ou cair, pensando nesse problema o docker criou uma coluna NAME no cont\u00eaineres e com ela que vamos resolver todo esse problema. Vamos passa a flag --name durante o docker run para nomear nossos cont\u00eaineres e vamos criar nossa rede, \u00e9 um processo bem simples: Usaremos o docker network create para criar uma rede, vamos usar a flag --driver para passar o driver bridge. docker network create --driver bridge rede-teste E vamos passar nossa rede na cria\u00e7\u00e3o do cont\u00eainer: Pong Ping docker run -it --name pong --network rede-teste ubuntu sleep 30m docker run -it --name ubuntu1 --network rede-teste ubuntu bash Quando o bash iniciar: apt-get update E vamos instalar o ping apt-get install iputils-ping -y Execute o ping, mas agora vamos passar o nome do cont\u00eainer e n\u00e3o o IP ping pong A diferen\u00e7a entre o padr\u00e3o do docker bridge network e o user-defined bridges \u00e9 que quando criamos nossa pr\u00f3pria rede bridge, no processo \u00e9 provido uma solu\u00e7\u00e3o DNS autom\u00e1tica nos cont\u00eaineres, possibilitando esse teste que fizemos, sem ocorrer a necessidade de mais configura\u00e7\u00f5es. Quando executamos o docker network ls , verificamos que temos mais duas categorias de drivers de redes dispon\u00edveis, none e host . A rede com o driver none basicamente n\u00e3o criar conex\u00f5es no cont\u00eainer, removendo a interface de rede: docker run -d --network none ubuntu sleep 30m De um docker inspect nesse cont\u00eainer e o resultado: \"Networks\" : { \"none\" : { \"IPAMConfig\" : null , \"Links\" : null , \"Aliases\" : null , \"NetworkID\" : \"093d70e2475c6ddd793426f4f6d3aa5560fd946ae182219aff56b2f406f1d290\" , \"EndpointID\" : \"59be8bf2afca5f2a103aa48fe86a85a676dd8ffe48a4d1101d128dcb5e9ad8b1\" , \"Gateway\" : \"\" , \"IPAddress\" : \"\" , \"IPPrefixLen\" : 0 , \"IPv6Gateway\" : \"\" , \"GlobalIPv6Address\" : \"\" , \"GlobalIPv6PrefixLen\" : 0 , \"MacAddress\" : \"\" , \"DriverOpts\" : null } } Os campos est\u00e3o sem informa\u00e7\u00e3o de IP ou est\u00e3o com null. Isolando completamente o cont\u00eainer. Enquanto o driver host , elimina o isolamento proporcionado pela estrutura do docker, ou seja, acessa a rede diretamente da host machine: docker run -d --network host sposigor/app-flask-teste:1 Se lembra que no projeto o flask define a porta 5000, e para acessar no navegador era preciso passa a flag -p para redirecionar? Com o driver host da rede, basta acessar diretamente, copie e cole no navegador: localhost:5000 Para lembrar ainda \u00e9 poss\u00edvel ter problemas de portas, ou seja, se houve mais alguma aplica\u00e7\u00e3o usando a porta 5000 vai ocorrer um erro.","title":"Melhorando a comunica\u00e7\u00e3o"},{"location":"docker/volumes/","text":"Gerenciamento de Dados \u00b6 As refer\u00eancias ao espa\u00e7o em disco em imagens e cont\u00eaineres do Docker podem ser confusas. \u00c9 importante distinguir entre armazenamento local e armazenamento virtual. Armazenamento local refere-se ao espa\u00e7o em disco que a camada grav\u00e1vel de um cont\u00eainer usa, enquanto o armazenamento virtual \u00e9 o espa\u00e7o em disco usado para o cont\u00eainer e a camada grav\u00e1vel. A camada somente leitura de uma imagem podem ser compartilhadas entre qualquer cont\u00eainer iniciado a partir da mesma imagem. Que tal verificar essa informa\u00e7\u00e3o? Antes de continuamos, vou limpar minhas imagens e meus cont\u00eaineres que est\u00e3o armazenados, como pode ver: Imagens Cont\u00eaineres REPOSITORY TAG IMAGE ID CREATED SIZE sposigor/app-flask-teste 1 c704ddb7c517 2 hours ago 60 .3MB univesp-github/app-flask-teste 1 c704ddb7c517 2 hours ago 60 .3MB python 3 .10-alpine a0e2910f7263 4 days ago 48 .6MB python 3 .10-slim-buster 41471b406cc5 4 days ago 115MB python 3 .10 e4bdbb3cacf1 4 days ago 917MB ubuntu latest 54c9d81cbb44 4 weeks ago 72 .8MB postgres latest da2cb49d7a8d 4 weeks ago 374MB dpage/pgadmin4 latest e52ca07eba62 7 weeks ago 272MB hello-world latest feb5d9fea6a5 5 months ago 13 .3kB dockersamples/static-site latest f589ccde7957 5 years ago 190MB CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 46cab0670fc8 univesp-github/app-flask-teste:1 \"python app.py\" 2 hours ago Exited ( 0 ) 2 hours ago elegant_brown b50c4ac105c3 f32698e186b5 \"python app.py\" 2 hours ago Exited ( 0 ) 2 hours ago boring_knuth 40ea6243bbc3 64160aa2a8dc \"python app.py\" 2 hours ago Exited ( 0 ) 2 hours ago unruffled_roentgen 7ef49050e8df 1647b4c47b68 \"python app.py\" 2 hours ago Exited ( 0 ) 2 hours ago nostalgic_dirac 19bf7322160d ubuntu \"sleep 5m\" 20 hours ago Exited ( 0 ) 19 hours ago wizardly_shtern bb1bf42f3fc3 ubuntu \"sleep 2m\" 20 hours ago Exited ( 0 ) 20 hours ago frosty_ishizaka 65adb2cb2cd5 ubuntu \"bash\" 20 hours ago Exited ( 0 ) 20 hours ago competent_thompson f690741c0bc7 ubuntu \"bash\" 20 hours ago Exited ( 0 ) 20 hours ago infallible_williams 6914d474838d hello-world \"/hello\" 23 hours ago Exited ( 0 ) 20 hours ago upbeat_lichterman Vou usar um comando um pouco diferente o docker system prune docker system prune -af Ele remove todas as imagens, cont\u00eaineres, volumes e o que mais tiver armazenado no docker, passei o par\u00e2metro -a de all e -f de force. Vou lista mais alguns comandos de remo\u00e7\u00e3o que podem ser uteis: docker container rm : Remove somente com o CONTAINER ID. docker container prune : Remove todos os containeres docker image rm : Remove somente a imagem com o IMAGE ID docker image prune : Remove todas as imagens docker volume rm : Remove o volume com o VOLUME NAME docker volume prune : Remove todos os volumes docker network rm : Remove o network com o NETWORK ID docker network prune : Remove todos os network Para come\u00e7ar a entender melhor o volume, armazenamento local e armazenamento virtual, recorremos ao nosso querido ubuntu, dessa vez, vamos chamar ele duas vezes: Primeira Segunda docker run -it ubuntu bash Assim que subir o bash execute o apt-get update Escreva exit para finalizar docker run -it ubuntu bash Nesse caso apenas e escreva exit para finalizar Com o comando docker ps -as a sa\u00edda: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE a4928a7ff9a6 ubuntu \"bash\" 47 seconds ago Exited ( 0 ) 44 seconds ago funny_thompson 5B ( virtual 72 .8MB ) 0516c2a0ac00 ubuntu \"bash\" 4 minutes ago Exited ( 130 ) About a minute ago fervent_turing 34MB ( virtual 107MB ) Temos uma nova coluna que \u00e9 a SIZE e ela possui duas informa\u00e7\u00f5es o armazenamento local e armazenamento virtual, usando o ubuntu como exemplo, em um apenas subimos a imagem sendo o primeiro na lista e o segundo fizemos o apt-get update. Armazenamento Local : \u00c9 a camada da qual temos acesso a gravar. Armazenamento Virtual : \u00c9 o tamanho da imagem sem qualquer grava\u00e7\u00e3o. Quando fazemos grava\u00e7\u00f5es, esses dados reflete tanto no armazenamento local quanto no armazenamento virtual. Persist\u00eancia de dados \u00b6 Podemos querer que os dados da nossa aplica\u00e7\u00e3o sejam persistentes, porque assim garantimos que ela esteja distribu\u00edda e dispon\u00edvel se precisarmos consult\u00e1-la. Por\u00e9m, se escrevermos os dados nos cont\u00eaineres, por padr\u00e3o eles n\u00e3o ficar\u00e3o armazenados nesta camada, criada para ser descart\u00e1vel. Existem tr\u00eas possibilidades para contornar esta situa\u00e7\u00e3o com o Docker. Bind Mounts \u00b6 Os bind mounts est\u00e3o dispon\u00edveis no Docker desde seus primeiros dias para a persist\u00eancia de dados. Os bind mounts montar\u00e3o um arquivo ou diret\u00f3rio em seu cont\u00eainer a partir de sua m\u00e1quina host, que voc\u00ea pode referenciar atrav\u00e9s de seu caminho absoluto. Para usar o bind mounts, o arquivo ou diret\u00f3rio n\u00e3o precisa j\u00e1 existir em seu host do Docker. Se n\u00e3o existir, ser\u00e1 criado sob demanda. As montagens de liga\u00e7\u00e3o dependem do sistema de arquivos da m\u00e1quina host ter uma estrutura de diret\u00f3rio espec\u00edfica dispon\u00edvel. Voc\u00ea deve criar explicitamente um caminho para o arquivo ou pasta para colocar o armazenamento. Outra informa\u00e7\u00e3o importante sobre bind mounts \u00e9 que elas d\u00e3o acesso a arquivos confidenciais. Conforme os documentos do Docker, voc\u00ea pode alterar o sistema de arquivos do host atrav\u00e9s de processos executados em um cont\u00eainer. Isso inclui criar, modificar e excluir arquivos e diret\u00f3rios do sistema, que podem ter implica\u00e7\u00f5es de seguran\u00e7a bastante graves. Pode at\u00e9 impactar processos n\u00e3o-Docker. Para come\u00e7ar a usar o bind mounts, precisamos criar uma pasta na nossa maquina, vou cria na \u00e1rea de trabalho e nome\u00e1-la de bind. Copie a localiza\u00e7\u00e3o que vamos usar. -v --mount docker run -v /home/sposigor/bind:/teste -it ubuntu bash docker run -it --mount type = bind,src = /home/sposigor/bind,dst = /teste ubuntu bash A docker recomenda usar --mount e n\u00e3o o -v Se tiver d\u00favidas, na documenta\u00e7\u00e3o do docker run tem mais informa\u00e7\u00f5es. Para realizar o teste: Fa\u00e7a o docker run, seja em -v ou --mount. No bash do cont\u00eainer, de um ls Veja a pasta teste criada e vamos entrar nela com o cd teste/ Na pasta touch teste1.txt e escreva exit para sair V\u00e1 na sua pasta que voc\u00ea criou na m\u00e1quina local e abra ela e o arquivo teste1.txt vai est\u00e1 l\u00e1. Repita o processo de 1 \u00e1 3, com um novo cont\u00eainer e o arquivo criado no cont\u00eainer anterior persiste l\u00e1. Volumes \u00b6 Os volumes s\u00e3o um \u00f3timo mecanismo para adicionar uma camada de persist\u00eancia de dados em seus cont\u00eaineres do Docker, especialmente para uma situa\u00e7\u00e3o em que voc\u00ea precisa persistir dados ap\u00f3s desligar seus cont\u00eaineres. Volumes, portanto \u00e9 a solu\u00e7\u00e3o mais recomendada pelo docker para usar em ambientes de produ\u00e7\u00e3o e afins. O docker volume ls observa os volumes dispon\u00edveis: DRIVER VOLUME NAME DRIVER : \u00c9 o volume do drive VOLUME NAME : \u00c9 o nome do volume Criar um volume novo docker volume create e o nome do volume docker volume create testando Vamos colocar em produ\u00e7\u00e3o: -v --mount docker run -v testando:/teste -it ubuntu bash docker run -t -i --mount type = bind,src = testando,dst = /teste ubuntu bash Fa\u00e7a o docker run, seja em -v ou --mount. No bash do container, de um ls Veja a pasta teste que foi criada e vamos entrar nela com o cd teste/ Dentro da pasta touch teste1.txt e escreva exit para sair Repita o processo de 1 \u00e1 3, com um novo container e o arquivo criado no container anterios persiste l\u00e1. Deve ter surgido uma duvida, aonde est\u00e1 esse arquivo que criei? Para isso vamos para o terminal: No terminal sudo su cd /var/lib/docker/volumes ls para verificar os arquivos O volume criado vai est\u00e1 aqui, cd testando ls vai ter uma pasta _data cd _data e em seguida ls Encontrou o arquivo que foi criado Os volumes do Docker s\u00e3o completamente manipulados pelo pr\u00f3prio Docker e, portanto, independentes da estrutura de diret\u00f3rios e do sistema operacional da m\u00e1quina host. Quando voc\u00ea usa um volume, um novo diret\u00f3rio \u00e9 criado no diret\u00f3rio de armazenamento do Docker na m\u00e1quina host e o Docker gerencia o conte\u00fado desse diret\u00f3rio. Por ultimo podemos passar diretamente do docker run o volume que ele se encarrega de criar -v --mount docker run -v volume_novo:/teste -it ubuntu bash docker run -t -i --mount type = bind,src = volume_novo,dst = /teste ubuntu bash docker volume ls e a sa\u00edda: DRIVER VOLUME NAME local testando local volume_novo tmpfs mounts \u00b6 O tmpfs mounts \u00e9 para espa\u00e7os de armazenamento pequenos e ef\u00eameros que s\u00f3 podem ser usados \u200b\u200bpor um \u00fanico cont\u00eainer, existe o sistema de arquivos tmpfs. Ele \u00e9 apoiado apenas pelo armazenamento RAM no sistema host. Essa funcionalidade s\u00f3 est\u00e1 dispon\u00edvel se voc\u00ea estiver executando o Docker no Linux. Seguindo a ordem para criar um tmpfs \u00e9 mais simples, j\u00e1 possui a pr\u00f3pria flag --tmpfs: docker run -it --tmpfs = /app ubuntu bash Perceba que a pasta app que criamos usando o --tmpfs, est\u00e1 com um fundo verde, assim como o tmp, ent\u00e3o significa que ela \u00e9 tempor\u00e1ria, quando o cont\u00eainer for finalizado, os dados ali s\u00e3o apagados. Ent\u00e3o qual \u00e9 a utilidade de usar o tmpfs? \u00c9 que ela armazena os arquivos na mem\u00f3ria do host e n\u00e3o na camada de escrita do cont\u00eaineres r/w, ou seja, se existem dados sens\u00edveis para serem armazenados no processo e por quest\u00f5es de segura n\u00e3o deseja escrever na camada de escrita do cont\u00eainer r/w, o tmpfs \u00e9 recomendando nessa situa\u00e7\u00e3o. Para mais uma aplica\u00e7\u00e3o podemos usar como par\u00e2metro do --mount: docker run -it --mount type = tmpfs,dst = /teste ubuntu bash","title":"Gerenciamento de Dados"},{"location":"docker/volumes/#gerenciamento-de-dados","text":"As refer\u00eancias ao espa\u00e7o em disco em imagens e cont\u00eaineres do Docker podem ser confusas. \u00c9 importante distinguir entre armazenamento local e armazenamento virtual. Armazenamento local refere-se ao espa\u00e7o em disco que a camada grav\u00e1vel de um cont\u00eainer usa, enquanto o armazenamento virtual \u00e9 o espa\u00e7o em disco usado para o cont\u00eainer e a camada grav\u00e1vel. A camada somente leitura de uma imagem podem ser compartilhadas entre qualquer cont\u00eainer iniciado a partir da mesma imagem. Que tal verificar essa informa\u00e7\u00e3o? Antes de continuamos, vou limpar minhas imagens e meus cont\u00eaineres que est\u00e3o armazenados, como pode ver: Imagens Cont\u00eaineres REPOSITORY TAG IMAGE ID CREATED SIZE sposigor/app-flask-teste 1 c704ddb7c517 2 hours ago 60 .3MB univesp-github/app-flask-teste 1 c704ddb7c517 2 hours ago 60 .3MB python 3 .10-alpine a0e2910f7263 4 days ago 48 .6MB python 3 .10-slim-buster 41471b406cc5 4 days ago 115MB python 3 .10 e4bdbb3cacf1 4 days ago 917MB ubuntu latest 54c9d81cbb44 4 weeks ago 72 .8MB postgres latest da2cb49d7a8d 4 weeks ago 374MB dpage/pgadmin4 latest e52ca07eba62 7 weeks ago 272MB hello-world latest feb5d9fea6a5 5 months ago 13 .3kB dockersamples/static-site latest f589ccde7957 5 years ago 190MB CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 46cab0670fc8 univesp-github/app-flask-teste:1 \"python app.py\" 2 hours ago Exited ( 0 ) 2 hours ago elegant_brown b50c4ac105c3 f32698e186b5 \"python app.py\" 2 hours ago Exited ( 0 ) 2 hours ago boring_knuth 40ea6243bbc3 64160aa2a8dc \"python app.py\" 2 hours ago Exited ( 0 ) 2 hours ago unruffled_roentgen 7ef49050e8df 1647b4c47b68 \"python app.py\" 2 hours ago Exited ( 0 ) 2 hours ago nostalgic_dirac 19bf7322160d ubuntu \"sleep 5m\" 20 hours ago Exited ( 0 ) 19 hours ago wizardly_shtern bb1bf42f3fc3 ubuntu \"sleep 2m\" 20 hours ago Exited ( 0 ) 20 hours ago frosty_ishizaka 65adb2cb2cd5 ubuntu \"bash\" 20 hours ago Exited ( 0 ) 20 hours ago competent_thompson f690741c0bc7 ubuntu \"bash\" 20 hours ago Exited ( 0 ) 20 hours ago infallible_williams 6914d474838d hello-world \"/hello\" 23 hours ago Exited ( 0 ) 20 hours ago upbeat_lichterman Vou usar um comando um pouco diferente o docker system prune docker system prune -af Ele remove todas as imagens, cont\u00eaineres, volumes e o que mais tiver armazenado no docker, passei o par\u00e2metro -a de all e -f de force. Vou lista mais alguns comandos de remo\u00e7\u00e3o que podem ser uteis: docker container rm : Remove somente com o CONTAINER ID. docker container prune : Remove todos os containeres docker image rm : Remove somente a imagem com o IMAGE ID docker image prune : Remove todas as imagens docker volume rm : Remove o volume com o VOLUME NAME docker volume prune : Remove todos os volumes docker network rm : Remove o network com o NETWORK ID docker network prune : Remove todos os network Para come\u00e7ar a entender melhor o volume, armazenamento local e armazenamento virtual, recorremos ao nosso querido ubuntu, dessa vez, vamos chamar ele duas vezes: Primeira Segunda docker run -it ubuntu bash Assim que subir o bash execute o apt-get update Escreva exit para finalizar docker run -it ubuntu bash Nesse caso apenas e escreva exit para finalizar Com o comando docker ps -as a sa\u00edda: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE a4928a7ff9a6 ubuntu \"bash\" 47 seconds ago Exited ( 0 ) 44 seconds ago funny_thompson 5B ( virtual 72 .8MB ) 0516c2a0ac00 ubuntu \"bash\" 4 minutes ago Exited ( 130 ) About a minute ago fervent_turing 34MB ( virtual 107MB ) Temos uma nova coluna que \u00e9 a SIZE e ela possui duas informa\u00e7\u00f5es o armazenamento local e armazenamento virtual, usando o ubuntu como exemplo, em um apenas subimos a imagem sendo o primeiro na lista e o segundo fizemos o apt-get update. Armazenamento Local : \u00c9 a camada da qual temos acesso a gravar. Armazenamento Virtual : \u00c9 o tamanho da imagem sem qualquer grava\u00e7\u00e3o. Quando fazemos grava\u00e7\u00f5es, esses dados reflete tanto no armazenamento local quanto no armazenamento virtual.","title":"Gerenciamento de Dados"},{"location":"docker/volumes/#persistencia-de-dados","text":"Podemos querer que os dados da nossa aplica\u00e7\u00e3o sejam persistentes, porque assim garantimos que ela esteja distribu\u00edda e dispon\u00edvel se precisarmos consult\u00e1-la. Por\u00e9m, se escrevermos os dados nos cont\u00eaineres, por padr\u00e3o eles n\u00e3o ficar\u00e3o armazenados nesta camada, criada para ser descart\u00e1vel. Existem tr\u00eas possibilidades para contornar esta situa\u00e7\u00e3o com o Docker.","title":"Persist\u00eancia de dados"},{"location":"docker/volumes/#bind-mounts","text":"Os bind mounts est\u00e3o dispon\u00edveis no Docker desde seus primeiros dias para a persist\u00eancia de dados. Os bind mounts montar\u00e3o um arquivo ou diret\u00f3rio em seu cont\u00eainer a partir de sua m\u00e1quina host, que voc\u00ea pode referenciar atrav\u00e9s de seu caminho absoluto. Para usar o bind mounts, o arquivo ou diret\u00f3rio n\u00e3o precisa j\u00e1 existir em seu host do Docker. Se n\u00e3o existir, ser\u00e1 criado sob demanda. As montagens de liga\u00e7\u00e3o dependem do sistema de arquivos da m\u00e1quina host ter uma estrutura de diret\u00f3rio espec\u00edfica dispon\u00edvel. Voc\u00ea deve criar explicitamente um caminho para o arquivo ou pasta para colocar o armazenamento. Outra informa\u00e7\u00e3o importante sobre bind mounts \u00e9 que elas d\u00e3o acesso a arquivos confidenciais. Conforme os documentos do Docker, voc\u00ea pode alterar o sistema de arquivos do host atrav\u00e9s de processos executados em um cont\u00eainer. Isso inclui criar, modificar e excluir arquivos e diret\u00f3rios do sistema, que podem ter implica\u00e7\u00f5es de seguran\u00e7a bastante graves. Pode at\u00e9 impactar processos n\u00e3o-Docker. Para come\u00e7ar a usar o bind mounts, precisamos criar uma pasta na nossa maquina, vou cria na \u00e1rea de trabalho e nome\u00e1-la de bind. Copie a localiza\u00e7\u00e3o que vamos usar. -v --mount docker run -v /home/sposigor/bind:/teste -it ubuntu bash docker run -it --mount type = bind,src = /home/sposigor/bind,dst = /teste ubuntu bash A docker recomenda usar --mount e n\u00e3o o -v Se tiver d\u00favidas, na documenta\u00e7\u00e3o do docker run tem mais informa\u00e7\u00f5es. Para realizar o teste: Fa\u00e7a o docker run, seja em -v ou --mount. No bash do cont\u00eainer, de um ls Veja a pasta teste criada e vamos entrar nela com o cd teste/ Na pasta touch teste1.txt e escreva exit para sair V\u00e1 na sua pasta que voc\u00ea criou na m\u00e1quina local e abra ela e o arquivo teste1.txt vai est\u00e1 l\u00e1. Repita o processo de 1 \u00e1 3, com um novo cont\u00eainer e o arquivo criado no cont\u00eainer anterior persiste l\u00e1.","title":"Bind Mounts"},{"location":"docker/volumes/#volumes","text":"Os volumes s\u00e3o um \u00f3timo mecanismo para adicionar uma camada de persist\u00eancia de dados em seus cont\u00eaineres do Docker, especialmente para uma situa\u00e7\u00e3o em que voc\u00ea precisa persistir dados ap\u00f3s desligar seus cont\u00eaineres. Volumes, portanto \u00e9 a solu\u00e7\u00e3o mais recomendada pelo docker para usar em ambientes de produ\u00e7\u00e3o e afins. O docker volume ls observa os volumes dispon\u00edveis: DRIVER VOLUME NAME DRIVER : \u00c9 o volume do drive VOLUME NAME : \u00c9 o nome do volume Criar um volume novo docker volume create e o nome do volume docker volume create testando Vamos colocar em produ\u00e7\u00e3o: -v --mount docker run -v testando:/teste -it ubuntu bash docker run -t -i --mount type = bind,src = testando,dst = /teste ubuntu bash Fa\u00e7a o docker run, seja em -v ou --mount. No bash do container, de um ls Veja a pasta teste que foi criada e vamos entrar nela com o cd teste/ Dentro da pasta touch teste1.txt e escreva exit para sair Repita o processo de 1 \u00e1 3, com um novo container e o arquivo criado no container anterios persiste l\u00e1. Deve ter surgido uma duvida, aonde est\u00e1 esse arquivo que criei? Para isso vamos para o terminal: No terminal sudo su cd /var/lib/docker/volumes ls para verificar os arquivos O volume criado vai est\u00e1 aqui, cd testando ls vai ter uma pasta _data cd _data e em seguida ls Encontrou o arquivo que foi criado Os volumes do Docker s\u00e3o completamente manipulados pelo pr\u00f3prio Docker e, portanto, independentes da estrutura de diret\u00f3rios e do sistema operacional da m\u00e1quina host. Quando voc\u00ea usa um volume, um novo diret\u00f3rio \u00e9 criado no diret\u00f3rio de armazenamento do Docker na m\u00e1quina host e o Docker gerencia o conte\u00fado desse diret\u00f3rio. Por ultimo podemos passar diretamente do docker run o volume que ele se encarrega de criar -v --mount docker run -v volume_novo:/teste -it ubuntu bash docker run -t -i --mount type = bind,src = volume_novo,dst = /teste ubuntu bash docker volume ls e a sa\u00edda: DRIVER VOLUME NAME local testando local volume_novo","title":"Volumes"},{"location":"docker/volumes/#tmpfs-mounts","text":"O tmpfs mounts \u00e9 para espa\u00e7os de armazenamento pequenos e ef\u00eameros que s\u00f3 podem ser usados \u200b\u200bpor um \u00fanico cont\u00eainer, existe o sistema de arquivos tmpfs. Ele \u00e9 apoiado apenas pelo armazenamento RAM no sistema host. Essa funcionalidade s\u00f3 est\u00e1 dispon\u00edvel se voc\u00ea estiver executando o Docker no Linux. Seguindo a ordem para criar um tmpfs \u00e9 mais simples, j\u00e1 possui a pr\u00f3pria flag --tmpfs: docker run -it --tmpfs = /app ubuntu bash Perceba que a pasta app que criamos usando o --tmpfs, est\u00e1 com um fundo verde, assim como o tmp, ent\u00e3o significa que ela \u00e9 tempor\u00e1ria, quando o cont\u00eainer for finalizado, os dados ali s\u00e3o apagados. Ent\u00e3o qual \u00e9 a utilidade de usar o tmpfs? \u00c9 que ela armazena os arquivos na mem\u00f3ria do host e n\u00e3o na camada de escrita do cont\u00eaineres r/w, ou seja, se existem dados sens\u00edveis para serem armazenados no processo e por quest\u00f5es de segura n\u00e3o deseja escrever na camada de escrita do cont\u00eainer r/w, o tmpfs \u00e9 recomendando nessa situa\u00e7\u00e3o. Para mais uma aplica\u00e7\u00e3o podemos usar como par\u00e2metro do --mount: docker run -it --mount type = tmpfs,dst = /teste ubuntu bash","title":"tmpfs mounts"},{"location":"docker%20swarm/docker%20stack/","text":"Docker Stack \u00b6 A Docker Stack \u00e9 um comando do Docker para gerenciar as pilhas do Docker. Podemos usar este comando para implantar uma nova pilha ou atualizar uma existente, listar pilhas, listar as tarefas na pilha, remover uma ou mais pilhas e listar os servi\u00e7os na pilha. Devemos habilitar o modo swarm para executar este comando, pois s\u00f3 podemos implantar pilhas no modo swarm do Docker e ele est\u00e1 inclu\u00eddo no mecanismo do Docker, para n\u00e3o precisarmos instalar nenhum pacote adicional, apenas precisamos habilit\u00e1-lo, pois est\u00e1 desabilitado por predefini\u00e7\u00e3o. A proposta aqui \u00e9 roda uma infraestrutura composto por v\u00e1rios servi\u00e7os e usando o docker swarm. No docker hub. tem uma imagem bem popular para exemplificar todo esse processo, aqui voc\u00ea acessa o material referente a essa aplica\u00e7\u00e3o. Antes de continuamos, vou reiniciar a p\u00e1gina do docker play e selecionar a configura\u00e7\u00e3o de 3 manager and 2 workes. Vamos criar um docker-compose.yml: Copie o Conte\u00fado abaixo: version : \"3\" services : redis : image : redis:alpine networks : - frontend deploy : replicas : 1 restart_policy : condition : on-failure db : image : postgres:9.4 volumes : - db-data:/var/lib/postgresql/data networks : - backend deploy : placement : constraints : [ node.role == manager ] environment : POSTGRES_HOST_AUTH_METHOD : trust vote : image : dockersamples/examplevotingapp_vote:before ports : - 5000:80 networks : - frontend depends_on : - redis deploy : replicas : 2 restart_policy : condition : on-failure result : image : dockersamples/examplevotingapp_result:before ports : - 5001:80 networks : - backend depends_on : - db deploy : replicas : 1 restart_policy : condition : on-failure worker : image : dockersamples/examplevotingapp_worker networks : - frontend - backend depends_on : - db - redis deploy : mode : replicated replicas : 1 labels : [ APP=VOTING ] restart_policy : condition : on-failure placement : constraints : [ node.role == worker ] visualizer : image : dockersamples/visualizer:stable ports : - 8080:8080 stop_grace_period : 1m30s volumes : - \"/var/run/docker.sock:/var/run/docker.sock\" deploy : placement : constraints : [ node.role == manager ] networks : frontend : backend : volumes : db-data : V\u00e1 no terminal e digite: cat > docker-compose.yml De um enter, depois um shift + insert para colar o conteudo copiado. De um enter para abrir um espa\u00e7o no final e ctrl + d, para sair da edi\u00e7\u00e3o do arquivo. Agora com o .yml certinho, vamos usar o docker stack deploy para subir toda essa aplica\u00e7\u00e3o. Execute Saida docker stack deploy --compose-file docker-compose.yml vote Creating network vote_backend Creating network vote_frontend Creating network vote_default Creating service vote_db Creating service vote_vote Creating service vote_result Creating service vote_worker Creating service vote_visualizer Creating service vote_redis Vamos verificar agora com o docker stack ls . docker stack ls docker service ls docker stack ls Saida: NAME SERVICES ORCHESTRATOR vote 6 Swarm docker service ls Saida ID NAME MODE REPLICAS IMAGE PORTS a71bfc34hnw6 vote_db replicated 1 /1 postgres:9.4 3c11wqgvyvhe vote_redis replicated 1 /1 redis:alpine 5bghvabs9joq vote_result replicated 1 /1 dockersamples/examplevotingapp_result:before *:5001->80/tcp 5pn9qep3hqji vote_visualizer replicated 1 /1 dockersamples/visualizer:stable *:8080->8080/tcp u8pgdms926le vote_vote replicated 2 /2 dockersamples/examplevotingapp_vote:before *:5000->80/tcp vwlu9rb7yycs vote_worker replicated 1 /1 dockersamples/examplevotingapp_worker:latest E com isso subimos completamente a aplica\u00e7\u00e3o, se por acaso voc\u00ea executar muito r\u00e1pido as instru\u00e7\u00f5es, talvez veja as r\u00e9plicas subindo. Com isso vamos abrir as portas para verificar o resultado. 8080 : Para visualizar o dashboard 5000 : Para interagir com a vota\u00e7\u00e3o 5001 : Para visualizar o resultado da vota\u00e7\u00e3o No dashboard \u00e9 poss\u00edvel ver quais n\u00f3s, est\u00e3o rodando os cont\u00eaineres. Com isso finalizamos esse conte\u00fado sobre docker swarm, espero que tenha gostado e aprendido.","title":"Docker Stack"},{"location":"docker%20swarm/docker%20stack/#docker-stack","text":"A Docker Stack \u00e9 um comando do Docker para gerenciar as pilhas do Docker. Podemos usar este comando para implantar uma nova pilha ou atualizar uma existente, listar pilhas, listar as tarefas na pilha, remover uma ou mais pilhas e listar os servi\u00e7os na pilha. Devemos habilitar o modo swarm para executar este comando, pois s\u00f3 podemos implantar pilhas no modo swarm do Docker e ele est\u00e1 inclu\u00eddo no mecanismo do Docker, para n\u00e3o precisarmos instalar nenhum pacote adicional, apenas precisamos habilit\u00e1-lo, pois est\u00e1 desabilitado por predefini\u00e7\u00e3o. A proposta aqui \u00e9 roda uma infraestrutura composto por v\u00e1rios servi\u00e7os e usando o docker swarm. No docker hub. tem uma imagem bem popular para exemplificar todo esse processo, aqui voc\u00ea acessa o material referente a essa aplica\u00e7\u00e3o. Antes de continuamos, vou reiniciar a p\u00e1gina do docker play e selecionar a configura\u00e7\u00e3o de 3 manager and 2 workes. Vamos criar um docker-compose.yml: Copie o Conte\u00fado abaixo: version : \"3\" services : redis : image : redis:alpine networks : - frontend deploy : replicas : 1 restart_policy : condition : on-failure db : image : postgres:9.4 volumes : - db-data:/var/lib/postgresql/data networks : - backend deploy : placement : constraints : [ node.role == manager ] environment : POSTGRES_HOST_AUTH_METHOD : trust vote : image : dockersamples/examplevotingapp_vote:before ports : - 5000:80 networks : - frontend depends_on : - redis deploy : replicas : 2 restart_policy : condition : on-failure result : image : dockersamples/examplevotingapp_result:before ports : - 5001:80 networks : - backend depends_on : - db deploy : replicas : 1 restart_policy : condition : on-failure worker : image : dockersamples/examplevotingapp_worker networks : - frontend - backend depends_on : - db - redis deploy : mode : replicated replicas : 1 labels : [ APP=VOTING ] restart_policy : condition : on-failure placement : constraints : [ node.role == worker ] visualizer : image : dockersamples/visualizer:stable ports : - 8080:8080 stop_grace_period : 1m30s volumes : - \"/var/run/docker.sock:/var/run/docker.sock\" deploy : placement : constraints : [ node.role == manager ] networks : frontend : backend : volumes : db-data : V\u00e1 no terminal e digite: cat > docker-compose.yml De um enter, depois um shift + insert para colar o conteudo copiado. De um enter para abrir um espa\u00e7o no final e ctrl + d, para sair da edi\u00e7\u00e3o do arquivo. Agora com o .yml certinho, vamos usar o docker stack deploy para subir toda essa aplica\u00e7\u00e3o. Execute Saida docker stack deploy --compose-file docker-compose.yml vote Creating network vote_backend Creating network vote_frontend Creating network vote_default Creating service vote_db Creating service vote_vote Creating service vote_result Creating service vote_worker Creating service vote_visualizer Creating service vote_redis Vamos verificar agora com o docker stack ls . docker stack ls docker service ls docker stack ls Saida: NAME SERVICES ORCHESTRATOR vote 6 Swarm docker service ls Saida ID NAME MODE REPLICAS IMAGE PORTS a71bfc34hnw6 vote_db replicated 1 /1 postgres:9.4 3c11wqgvyvhe vote_redis replicated 1 /1 redis:alpine 5bghvabs9joq vote_result replicated 1 /1 dockersamples/examplevotingapp_result:before *:5001->80/tcp 5pn9qep3hqji vote_visualizer replicated 1 /1 dockersamples/visualizer:stable *:8080->8080/tcp u8pgdms926le vote_vote replicated 2 /2 dockersamples/examplevotingapp_vote:before *:5000->80/tcp vwlu9rb7yycs vote_worker replicated 1 /1 dockersamples/examplevotingapp_worker:latest E com isso subimos completamente a aplica\u00e7\u00e3o, se por acaso voc\u00ea executar muito r\u00e1pido as instru\u00e7\u00f5es, talvez veja as r\u00e9plicas subindo. Com isso vamos abrir as portas para verificar o resultado. 8080 : Para visualizar o dashboard 5000 : Para interagir com a vota\u00e7\u00e3o 5001 : Para visualizar o resultado da vota\u00e7\u00e3o No dashboard \u00e9 poss\u00edvel ver quais n\u00f3s, est\u00e3o rodando os cont\u00eaineres. Com isso finalizamos esse conte\u00fado sobre docker swarm, espero que tenha gostado e aprendido.","title":"Docker Stack"},{"location":"docker%20swarm/ingress/","text":"Ingress \u00b6 Que tal damos uma olhada na rede? Execute Execute em outro n\u00f3 docker network ls Saida: NETWORK ID NAME DRIVER SCOPE b61db738d8e1 bridge bridge local a1dc5572a15b docker_gwbridge bridge local 0310735b3f9c host host local k6lk5evd8ifb ingress overlay swarm 8e6d45cb021f none null local docker network ls Saida: NETWORK ID NAME DRIVER SCOPE 97f34fa9fd5d bridge bridge local 97e970ad94f3 docker_gwbridge bridge local e8c6e013648a host host local k6lk5evd8ifb ingress overlay swarm 20f0f809422d none null local Com isso podemos percebe uma coisa muito interessante que \u00e9 que independente do N\u00f3, seja ele gerente ou trabalhador, a rede com nome de ingress \u00e9 a mesma em qualquer n\u00f3. O que significa que todos as maquinas dentro do swarm tem essa rede em comum, vamos nos lembrar do routing mesh ent\u00e3o \u00e9 basicamente isso que estamos vendo, o driver overlay \u00e9 o responsavel por fazer boa parte da m\u00e1gica acontecer. Resolvendo todas as requisi\u00e7\u00f5es antes de chegarem nos n\u00f3s. E ela tem mais uma caracter\u00edstica importante, \u00e9 que est\u00e1 criptografada, garantindo seguran\u00e7a para o swarm. Service Discovery \u00b6 O Docker Engine possui um servidor DNS embutido nele, usado por cont\u00eaineres quando o Docker n\u00e3o est\u00e1 sendo executado no modo Swarm e para tarefas, quando est\u00e1. Ele fornece resolu\u00e7\u00e3o de nomes para todos os cont\u00eaineres que est\u00e3o no host em redes bridge, overlay. Cada cont\u00eainer encaminha suas consultas para o Docker Engine, que por sua vez verifica se o cont\u00eainer ou servi\u00e7o est\u00e1 na mesma rede que o cont\u00eainer que enviou a solicita\u00e7\u00e3o em primeiro lugar. Se for, ele pesquisa o endere\u00e7o IP (ou IP virtual) que corresponde a um cont\u00eainer, uma tarefa ou um nome de servi\u00e7o em seu armazenamento de chave-valor interno e o retorna ao cont\u00eainer que enviou a solicita\u00e7\u00e3o. Bem legal, hein? Como eu disse antes, o Docker Engine s\u00f3 retornar\u00e1 um endere\u00e7o IP se o recurso correspondente estiver na mesma rede que o cont\u00eainer que gerou a solicita\u00e7\u00e3o. O que tamb\u00e9m \u00e9 legal nisso \u00e9 que os Docker Hosts armazenam apenas as entradas DNS que pertencem \u00e0s redes nas quais o n\u00f3 possui cont\u00eaineres ou tarefas. Isso significa que eles n\u00e3o armazenar\u00e3o informa\u00e7\u00f5es que sejam praticamente irrelevantes para eles ou que outros cont\u00eaineres n\u00e3o precisem saber. Traduzindo o texto, se lembra do teste que fizemos do ping pong? Basicamente conseguimos fazer chamadas diretas pelo nome dos servi\u00e7os de outros servi\u00e7os usando uma rede overlay, mas n\u00e3o h\u00e1, por padr\u00e3o, e sim uma que vamos criar. Criando nossa rede overlay \u00b6 Para acessar a documenta\u00e7\u00e3o overlay . Para criar uma rede network: docker network create -d overlay minha_rede_overlay Para verificar: Gerente Trabalhadores docker network ls Saida: NETWORK ID NAME DRIVER SCOPE a0d052078dbe bridge bridge local 561b12e09e46 docker_gwbridge bridge local 4881488b5453 host host local k6lk5evd8ifb ingress overlay swarm wpmvutc3phrd minha_rede_overlay overlay swarm 255b8ee96504 none null local NETWORK ID NAME DRIVER SCOPE 97f34fa9fd5d bridge bridge local 97e970ad94f3 docker_gwbridge bridge local e8c6e013648a host host local k6lk5evd8ifb ingress overlay swarm 20f0f809422d none null local Como podem ver, quando executamos o docker network ls nos gerentes conseguimos ver a rede que criamos, e nos trabalhadores n\u00e3o. Porque isso acontece? Bem, por de tr\u00e2s dos panos ali, todos os n\u00f3 j\u00e1 sabem da existencia dessa rede, por\u00e9m os n\u00f3s trabalhadores s\u00e3o v\u00e3o listar essa rede, se um servi\u00e7o usar ela explicitamente. Vamos usar uma distro chamada alpine, que \u00e9 um linux super leve que vai servi para nossa ilustra\u00e7\u00e3o do processo. docker service create --name teste_discovery_search --network minha_rede_overlay --replicas 2 alpine sleep 50m Verifique aonde o servi\u00e7o est\u00e1 rodando com o docker service ps e passe o ID do servi\u00e7o: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS wuj7ee913pop teste_discovery_search.1 alpine:latest manager3 Running Running about a minute ago 178njdwf3bjx teste_discovery_search.1 alpine:latest manager1 Running Running about a minute ago No meu caso, est\u00e1 rodando no manager3 e no manager1, para acessa o \"bash\" do alpine a gente precisa passar o parametro sh , se por acaso cair numa maquina trabalhadora, voc\u00ea vai conseguir visualizar a rede que criamos. Vamos acessar o terminal do alpine: manager1 manager3 Vamos pega o nome do servi\u00e7o na rede que criamo no manager1 docker network inspect minha_rede_overlay Saida: \"Containers\" : { \"c25892b0685a6a5c3ea0337c58ce694d4a6da01712988c9cdb5fbad4ef24a656\" : { \"Name\" : \"teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh\" , \"EndpointID\" : \"41c6b2419c382238425e7afc0882ecdf2ef2038003d5d80bfb7322037982e840\" , \"MacAddress\" : \"02:42:0a:00:01:03\" , \"IPv4Address\" : \"10.0.1.3/24\" , \"IPv6Address\" : \"\" }, Na saida JSON vai ter esse trecho, e precisamos do Name para conseguir realizar essa comunica\u00e7\u00e3o. No meu caso teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh Com o nome do servi\u00e7o vamos usar o exec para acessar o terminal do alpine docker exec -it teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh sh Vamos executar o mesmo procedimento para pegar o nome do servi\u00e7o no manager3 docker network inspect minha_rede_overlay Saida: \"Containers\" : { \"ccabd4e0efd2862300cbb27ffebf8ca6718ab6d0163633bdb0a388766ebe937a\" : { \"Name\" : \"teste_discovery_search.1.wuj7ee913popf8wsysemruslc\" , \"EndpointID\" : \"e518f6eeab368d6ffabd439a4b902dccd3d5996bcbabd1f79c1cb2eb56b9be42\" , \"MacAddress\" : \"02:42:0a:00:01:04\" , \"IPv4Address\" : \"10.0.1.4/24\" , \"IPv6Address\" : \"\" }, Na sa\u00edda JSON vai ter esse trecho, e precisamos do Name para conseguir realizar essa comunica\u00e7\u00e3o. No meu caso teste_discovery_search.1.wuj7ee913popf8wsysemruslc Com o nome do servi\u00e7o vamos usar o exec para acessar o terminal do alpine docker exec -it teste_discovery_search.1.wuj7ee913popf8wsysemruslc sh Agora que estamos com o terminal de ambos os servi\u00e7os, vamos usar o ping pong para realizar uma chamada. manager1 manager3 Use o nome do servi\u00e7o do manager3 ping teste_discovery_search.1.wuj7ee913popf8wsysemruslc Saida: PING teste_discovery_search.1.wuj7ee913popf8wsysemruslc ( 10 .0.1.4 ) : 56 data bytes 64 bytes from 10 .0.1.4: seq = 0 ttl = 64 time = 0 .083 ms 64 bytes from 10 .0.1.4: seq = 1 ttl = 64 time = 0 .107 ms 64 bytes from 10 .0.1.4: seq = 2 ttl = 64 time = 0 .087 ms 64 bytes from 10 .0.1.4: seq = 3 ttl = 64 time = 0 .134 ms 64 bytes from 10 .0.1.4: seq = 4 ttl = 64 time = 0 .156 ms 64 bytes from 10 .0.1.4: seq = 5 ttl = 64 time = 0 .075 ms Use o nome do servi\u00e7o do manager1 ping teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh Saida: PING teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh ( 10 .0.1.3 ) : 56 data bytes 64 bytes from 10 .0.1.3: seq = 0 ttl = 64 time = 0 .060 ms 64 bytes from 10 .0.1.3: seq = 1 ttl = 64 time = 0 .082 ms 64 bytes from 10 .0.1.3: seq = 2 ttl = 64 time = 0 .079 ms 64 bytes from 10 .0.1.3: seq = 3 ttl = 64 time = 0 .273 ms 64 bytes from 10 .0.1.3: seq = 4 ttl = 64 time = 0 .097 ms 64 bytes from 10 .0.1.3: seq = 5 ttl = 64 time = 0 .203 ms Ent\u00e3o nosso teste foi realizado, de m\u00e1quinas diferentes, acessamos cont\u00eaineres e com apenas o nome do servi\u00e7o, o docker se encarregou de encontrar.","title":"Ingress"},{"location":"docker%20swarm/ingress/#ingress","text":"Que tal damos uma olhada na rede? Execute Execute em outro n\u00f3 docker network ls Saida: NETWORK ID NAME DRIVER SCOPE b61db738d8e1 bridge bridge local a1dc5572a15b docker_gwbridge bridge local 0310735b3f9c host host local k6lk5evd8ifb ingress overlay swarm 8e6d45cb021f none null local docker network ls Saida: NETWORK ID NAME DRIVER SCOPE 97f34fa9fd5d bridge bridge local 97e970ad94f3 docker_gwbridge bridge local e8c6e013648a host host local k6lk5evd8ifb ingress overlay swarm 20f0f809422d none null local Com isso podemos percebe uma coisa muito interessante que \u00e9 que independente do N\u00f3, seja ele gerente ou trabalhador, a rede com nome de ingress \u00e9 a mesma em qualquer n\u00f3. O que significa que todos as maquinas dentro do swarm tem essa rede em comum, vamos nos lembrar do routing mesh ent\u00e3o \u00e9 basicamente isso que estamos vendo, o driver overlay \u00e9 o responsavel por fazer boa parte da m\u00e1gica acontecer. Resolvendo todas as requisi\u00e7\u00f5es antes de chegarem nos n\u00f3s. E ela tem mais uma caracter\u00edstica importante, \u00e9 que est\u00e1 criptografada, garantindo seguran\u00e7a para o swarm.","title":"Ingress"},{"location":"docker%20swarm/ingress/#service-discovery","text":"O Docker Engine possui um servidor DNS embutido nele, usado por cont\u00eaineres quando o Docker n\u00e3o est\u00e1 sendo executado no modo Swarm e para tarefas, quando est\u00e1. Ele fornece resolu\u00e7\u00e3o de nomes para todos os cont\u00eaineres que est\u00e3o no host em redes bridge, overlay. Cada cont\u00eainer encaminha suas consultas para o Docker Engine, que por sua vez verifica se o cont\u00eainer ou servi\u00e7o est\u00e1 na mesma rede que o cont\u00eainer que enviou a solicita\u00e7\u00e3o em primeiro lugar. Se for, ele pesquisa o endere\u00e7o IP (ou IP virtual) que corresponde a um cont\u00eainer, uma tarefa ou um nome de servi\u00e7o em seu armazenamento de chave-valor interno e o retorna ao cont\u00eainer que enviou a solicita\u00e7\u00e3o. Bem legal, hein? Como eu disse antes, o Docker Engine s\u00f3 retornar\u00e1 um endere\u00e7o IP se o recurso correspondente estiver na mesma rede que o cont\u00eainer que gerou a solicita\u00e7\u00e3o. O que tamb\u00e9m \u00e9 legal nisso \u00e9 que os Docker Hosts armazenam apenas as entradas DNS que pertencem \u00e0s redes nas quais o n\u00f3 possui cont\u00eaineres ou tarefas. Isso significa que eles n\u00e3o armazenar\u00e3o informa\u00e7\u00f5es que sejam praticamente irrelevantes para eles ou que outros cont\u00eaineres n\u00e3o precisem saber. Traduzindo o texto, se lembra do teste que fizemos do ping pong? Basicamente conseguimos fazer chamadas diretas pelo nome dos servi\u00e7os de outros servi\u00e7os usando uma rede overlay, mas n\u00e3o h\u00e1, por padr\u00e3o, e sim uma que vamos criar.","title":"Service Discovery"},{"location":"docker%20swarm/ingress/#criando-nossa-rede-overlay","text":"Para acessar a documenta\u00e7\u00e3o overlay . Para criar uma rede network: docker network create -d overlay minha_rede_overlay Para verificar: Gerente Trabalhadores docker network ls Saida: NETWORK ID NAME DRIVER SCOPE a0d052078dbe bridge bridge local 561b12e09e46 docker_gwbridge bridge local 4881488b5453 host host local k6lk5evd8ifb ingress overlay swarm wpmvutc3phrd minha_rede_overlay overlay swarm 255b8ee96504 none null local NETWORK ID NAME DRIVER SCOPE 97f34fa9fd5d bridge bridge local 97e970ad94f3 docker_gwbridge bridge local e8c6e013648a host host local k6lk5evd8ifb ingress overlay swarm 20f0f809422d none null local Como podem ver, quando executamos o docker network ls nos gerentes conseguimos ver a rede que criamos, e nos trabalhadores n\u00e3o. Porque isso acontece? Bem, por de tr\u00e2s dos panos ali, todos os n\u00f3 j\u00e1 sabem da existencia dessa rede, por\u00e9m os n\u00f3s trabalhadores s\u00e3o v\u00e3o listar essa rede, se um servi\u00e7o usar ela explicitamente. Vamos usar uma distro chamada alpine, que \u00e9 um linux super leve que vai servi para nossa ilustra\u00e7\u00e3o do processo. docker service create --name teste_discovery_search --network minha_rede_overlay --replicas 2 alpine sleep 50m Verifique aonde o servi\u00e7o est\u00e1 rodando com o docker service ps e passe o ID do servi\u00e7o: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS wuj7ee913pop teste_discovery_search.1 alpine:latest manager3 Running Running about a minute ago 178njdwf3bjx teste_discovery_search.1 alpine:latest manager1 Running Running about a minute ago No meu caso, est\u00e1 rodando no manager3 e no manager1, para acessa o \"bash\" do alpine a gente precisa passar o parametro sh , se por acaso cair numa maquina trabalhadora, voc\u00ea vai conseguir visualizar a rede que criamos. Vamos acessar o terminal do alpine: manager1 manager3 Vamos pega o nome do servi\u00e7o na rede que criamo no manager1 docker network inspect minha_rede_overlay Saida: \"Containers\" : { \"c25892b0685a6a5c3ea0337c58ce694d4a6da01712988c9cdb5fbad4ef24a656\" : { \"Name\" : \"teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh\" , \"EndpointID\" : \"41c6b2419c382238425e7afc0882ecdf2ef2038003d5d80bfb7322037982e840\" , \"MacAddress\" : \"02:42:0a:00:01:03\" , \"IPv4Address\" : \"10.0.1.3/24\" , \"IPv6Address\" : \"\" }, Na saida JSON vai ter esse trecho, e precisamos do Name para conseguir realizar essa comunica\u00e7\u00e3o. No meu caso teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh Com o nome do servi\u00e7o vamos usar o exec para acessar o terminal do alpine docker exec -it teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh sh Vamos executar o mesmo procedimento para pegar o nome do servi\u00e7o no manager3 docker network inspect minha_rede_overlay Saida: \"Containers\" : { \"ccabd4e0efd2862300cbb27ffebf8ca6718ab6d0163633bdb0a388766ebe937a\" : { \"Name\" : \"teste_discovery_search.1.wuj7ee913popf8wsysemruslc\" , \"EndpointID\" : \"e518f6eeab368d6ffabd439a4b902dccd3d5996bcbabd1f79c1cb2eb56b9be42\" , \"MacAddress\" : \"02:42:0a:00:01:04\" , \"IPv4Address\" : \"10.0.1.4/24\" , \"IPv6Address\" : \"\" }, Na sa\u00edda JSON vai ter esse trecho, e precisamos do Name para conseguir realizar essa comunica\u00e7\u00e3o. No meu caso teste_discovery_search.1.wuj7ee913popf8wsysemruslc Com o nome do servi\u00e7o vamos usar o exec para acessar o terminal do alpine docker exec -it teste_discovery_search.1.wuj7ee913popf8wsysemruslc sh Agora que estamos com o terminal de ambos os servi\u00e7os, vamos usar o ping pong para realizar uma chamada. manager1 manager3 Use o nome do servi\u00e7o do manager3 ping teste_discovery_search.1.wuj7ee913popf8wsysemruslc Saida: PING teste_discovery_search.1.wuj7ee913popf8wsysemruslc ( 10 .0.1.4 ) : 56 data bytes 64 bytes from 10 .0.1.4: seq = 0 ttl = 64 time = 0 .083 ms 64 bytes from 10 .0.1.4: seq = 1 ttl = 64 time = 0 .107 ms 64 bytes from 10 .0.1.4: seq = 2 ttl = 64 time = 0 .087 ms 64 bytes from 10 .0.1.4: seq = 3 ttl = 64 time = 0 .134 ms 64 bytes from 10 .0.1.4: seq = 4 ttl = 64 time = 0 .156 ms 64 bytes from 10 .0.1.4: seq = 5 ttl = 64 time = 0 .075 ms Use o nome do servi\u00e7o do manager1 ping teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh Saida: PING teste_discovery_search.2.178njdwf3bjx26qk81fkr8yoh ( 10 .0.1.3 ) : 56 data bytes 64 bytes from 10 .0.1.3: seq = 0 ttl = 64 time = 0 .060 ms 64 bytes from 10 .0.1.3: seq = 1 ttl = 64 time = 0 .082 ms 64 bytes from 10 .0.1.3: seq = 2 ttl = 64 time = 0 .079 ms 64 bytes from 10 .0.1.3: seq = 3 ttl = 64 time = 0 .273 ms 64 bytes from 10 .0.1.3: seq = 4 ttl = 64 time = 0 .097 ms 64 bytes from 10 .0.1.3: seq = 5 ttl = 64 time = 0 .203 ms Ent\u00e3o nosso teste foi realizado, de m\u00e1quinas diferentes, acessamos cont\u00eaineres e com apenas o nome do servi\u00e7o, o docker se encarregou de encontrar.","title":"Criando nossa rede overlay"},{"location":"docker%20swarm/iniciando%20o%20swarm/","text":"Iniciando o Swarm \u00b6 Bem, chegamos a um ponto importante aqui que \u00e9 a cria\u00e7\u00e3o de uma rede de computadores, mas criar uma rede n\u00e3o \u00e9 t\u00e3o simples, mesmo que fizesse o passo a passo, vai levar muito tempo com muitas vari\u00e1veis que podem dar errado, ent\u00e3o para conseguimos usar o docker swarm vamos usar o Play With Docker Vai facilitar demais e vamos conseguir subir imagens normalmente, ele j\u00e1 vem com o docker instalado. Aperte Start Ele vai solicitar seu login do docker. Em seguida se ele n\u00e3o redirecionar basta aperta Start novamente. Assim que ele abrir a p\u00e1gina, n\u00e3o tenha nenhuma aplica\u00e7\u00e3o no momento, basta ficar tentando at\u00e9 conseguir acessar. Quando a aplica\u00e7\u00e3o subir vamos ter essa pagina: Vamos adicionar 5 instancias, basta pressionar o +ADD NEW INSTANCE que ele vai gerar automaticamente os computadores : Primeiro Gerente \u00b6 Para iniciar o swarm, usaremos o docker swarm init . docker swarm init Escolha umas das m\u00e1quinas para roda o comando e observe a sa\u00edda: Nesse erro, ele nos diz que n\u00e3o anunciamos o IP que devemos usar, o IP que as maquinas est\u00e3o na rede, para isso vamos usar a vari\u00e1vel que ele solicita --advertise-addr e passar o IP, \u00e9 importante sempre passar essa flag e o IP de umas das m\u00e1quinas, para fixamos a m\u00e1quina que criou o swarm. docker swarm init --advertise-addr 192 .168.0.13 E iniciamos o nosso swarm: A maquina que inicializa o comando, automaticamente se torna l\u00edder, ou seja, j\u00e1 temos nosso primeiro gerente. Nesse momento, ele vai passar dois comandos para que possamos adicionar novos gerentes ou novos trabalhadores: Gerentes : docker swarm join com a flag -token manager Trabalhadores : docker swarm join mais a flag --token SWMTKN-1-317lss7q287qzea30syrpzsnb7dlzcp5xqppqomkz0dpwlf0jv-3qzxl99iwvxo1b7zr2463xgdf 192.168.0.13:2377 passando o token e o IP para os novos computadores se unirem ao swarm. Antes de continuamos, vamos imaginar que voc\u00ea tenha deixado o ambiente rodando l\u00e1 e ficou um ano sem acessar, ai um belo dia voc\u00ea volta contudo n\u00e3o lembra qual maquina \u00e9 a que iniciou o swarm ou se ela est\u00e1 no swarm, para isso vamos usar o docker info . docker info E a sa\u00edda vai ser \u00b1 essa aqui: Saida Client: Context: default Debug Mode: false Plugins: app: Docker App ( Docker Inc., v0.9.1-beta3 ) Server: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 20 .10.0 Storage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: active NodeID: tv5szgmtbqunlbhkv4jqqdjrz Is Manager: true ClusterID: qxz7jhjgyd5ldoy7o1vcutbfn Managers: 1 Nodes: 1 Default Address Pool: 10 .0.0.0/8 SubnetSize: 24 Data Path Port: 4789 Orchestration: Task History Retention Limit: 5 Raft: Snapshot Interval: 10000 Number of Old Snapshots to Retain: 0 Heartbeat Tick: 1 Election Tick: 10 Dispatcher: Heartbeat Period: 5 seconds CA Configuration: Expiry Duration: 3 months Force Rotate: 0 Autolock Managers: false Root Rotation In Progress: false Node Address: 192 .168.0.13 Manager Addresses: 192 .168.0.13:2377 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc Default Runtime: runc Init Binary: docker-init containerd version: 269548fa27e0089a8b8278fc4fc781d7f65a939b runc version: ff819c7e9184c13b7c2607fe6c30ae19403a7aff init version: de40ad0 Security Options: apparmor seccomp Profile: default Kernel Version: 4 .4.0-210-generic Operating System: Alpine Linux v3.12 ( containerized ) OSType: linux Architecture: x86_64 CPUs: 8 Total Memory: 31 .42GiB Name: node1 ID: MLNO:DP7B:WIP5:4LLV:IUGJ:B5DP:KMRL:PZ2Z:A7Y3:7XLT:Z4NJ:UULR Docker Root Dir: /var/lib/docker Debug Mode: true File Descriptors: 38 Goroutines: 155 System Time: 2022 -03-05T23:02:27.17700525Z EventsListeners: 0 Registry: https://index.docker.io/v1/ Labels: Experimental: true Insecure Registries: 127 .0.0.1 127 .0.0.0/8 Live Restore Enabled: false Product License: Community Engine E nessa lista todas temos o Swarm: active , significa que o swarm est\u00e1 ativo e o Is Manager: true que diz que a maquina \u00e9 um gerente. Primeiro Trabalhador \u00b6 Agora que temos nosso swarm, vamos inserir mais computadores, vamos usar o docker swarm join Nas outras instancias que voc\u00ea tiver, basta copia o comando com shift+insert que o docker fornece, no meu caso \u00e9: docker swarm join --token SWMTKN-1-317lss7q287qzea30syrpzsnb7dlzcp5xqppqomkz0dpwlf0jv-3qzxl99iwvxo1b7zr2463xgdf 192 .168.0.13:2377 E a saida: This node joined a swarm as a worker. Agora fa\u00e7a isso em todas as inst\u00e2ncias que voc\u00ea criou, recomendo n\u00e3o passar de 5 por quest\u00e3o de estabilidade e desempenho do servi\u00e7o. Se voc\u00ea perde o token, n\u00e3o precisa anotar nenhum comando, basta no ir ao terminal do gerente e pergunta para ele, usando o comando docker swarm join-token e passe quem voc\u00ea quer saber, worker ou maneger docker swarm join-token worker N\u00f3 - Nodes \u00b6 Precisamos ver de uma forma geral nosso swarm, para isso vamos usar o comando docker node ls , no gerente, se usar no trabalhador, vai d\u00e1 um erro. ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION tv5szgmtbqunlbhkv4jqqdjrz * node1 Ready Active Leader 20 .10.0 somcfzr1i52h02s32g8162jv5 node2 Ready Active 20 .10.0 m3ug6hoe28zz1dap3ejhqug6n node3 Ready Active 20 .10.0 le62bq94ze1eagudy8dtx12ed node4 Ready Active 20 .10.0 aedfo2gsp1ky5cbzb7vmj1eqy node5 Ready Active 20 .10.0 Explicando: ID : \u00c9 o ID do n\u00f3, e o n\u00f3 com * \u00e9 o n\u00f3 que iniciou o swarm. HOSTNAME : Nome das m\u00e1quinas STATUS : Que est\u00e1 funcionando normalmente AVAILABILITY : Que vai rodar cont\u00eaineres normalmente MANAGER STATUS : Quem s\u00e3o os gerentes e o l\u00edder, todos os, n\u00f3 sem informa\u00e7\u00e3o, vazio, s\u00e3o workes para o docker. ENGINE VERSION : Engine \u00e9 a vers\u00e3o do docker. Para exemplificar, vamos roda o comando em um trabalhador: Error response from daemon: This node is not a swarm manager. Worker nodes can ' t be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager. Ent\u00e3o todos os comandos de altera\u00e7\u00e3o ou visualiza\u00e7\u00e3o, somente em n\u00f3 gerente. Por exemplo, vamos remover um n\u00f3 do cluster, usando o docker node rm , basta passar o ID do n\u00f3 que voc\u00ea quer derrubar. Execute Saida docker node rm aedfo2gsp1ky5cbzb7vmj1eqy Error response from daemon: rpc error: code = FailedPrecondition desc = node aedfo2gsp1ky5cbzb7vmj1eqy is not down and can ' t be removed O erro est\u00e1 dizendo que o n\u00f3 que tentamos remover n\u00e3o est\u00e1 com o status down , para resolver isso precisamos ir ao n\u00f3 que queremos derrubar e usar o docker swarm leave para muda o status do n\u00f3. Execute Saida docker swarm leave Node left the swarm. Retorne para o n\u00f3 gerente: ```bash docker node ls ``` Saida ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION tv5szgmtbqunlbhkv4jqqdjrz * node1 Ready Active Leader 20 .10.0 somcfzr1i52h02s32g8162jv5 node2 Ready Active 20 .10.0 m3ug6hoe28zz1dap3ejhqug6n node3 Ready Active 20 .10.0 le62bq94ze1eagudy8dtx12ed node4 Ready Active 20 .10.0 aedfo2gsp1ky5cbzb7vmj1eqy node5 Down Active 20 .10.0 O status do n\u00f3 5 mudou para down e com isso podemos remover efetivamente o n\u00f3. docker node rm aedfo2gsp1ky5cbzb7vmj1eqy E de outro docker node ls para confirmar. Adicione o n\u00f3 novamente para continuamos, use o docker swarm join","title":"Iniciando o Swarm"},{"location":"docker%20swarm/iniciando%20o%20swarm/#iniciando-o-swarm","text":"Bem, chegamos a um ponto importante aqui que \u00e9 a cria\u00e7\u00e3o de uma rede de computadores, mas criar uma rede n\u00e3o \u00e9 t\u00e3o simples, mesmo que fizesse o passo a passo, vai levar muito tempo com muitas vari\u00e1veis que podem dar errado, ent\u00e3o para conseguimos usar o docker swarm vamos usar o Play With Docker Vai facilitar demais e vamos conseguir subir imagens normalmente, ele j\u00e1 vem com o docker instalado. Aperte Start Ele vai solicitar seu login do docker. Em seguida se ele n\u00e3o redirecionar basta aperta Start novamente. Assim que ele abrir a p\u00e1gina, n\u00e3o tenha nenhuma aplica\u00e7\u00e3o no momento, basta ficar tentando at\u00e9 conseguir acessar. Quando a aplica\u00e7\u00e3o subir vamos ter essa pagina: Vamos adicionar 5 instancias, basta pressionar o +ADD NEW INSTANCE que ele vai gerar automaticamente os computadores :","title":"Iniciando o Swarm"},{"location":"docker%20swarm/iniciando%20o%20swarm/#primeiro-gerente","text":"Para iniciar o swarm, usaremos o docker swarm init . docker swarm init Escolha umas das m\u00e1quinas para roda o comando e observe a sa\u00edda: Nesse erro, ele nos diz que n\u00e3o anunciamos o IP que devemos usar, o IP que as maquinas est\u00e3o na rede, para isso vamos usar a vari\u00e1vel que ele solicita --advertise-addr e passar o IP, \u00e9 importante sempre passar essa flag e o IP de umas das m\u00e1quinas, para fixamos a m\u00e1quina que criou o swarm. docker swarm init --advertise-addr 192 .168.0.13 E iniciamos o nosso swarm: A maquina que inicializa o comando, automaticamente se torna l\u00edder, ou seja, j\u00e1 temos nosso primeiro gerente. Nesse momento, ele vai passar dois comandos para que possamos adicionar novos gerentes ou novos trabalhadores: Gerentes : docker swarm join com a flag -token manager Trabalhadores : docker swarm join mais a flag --token SWMTKN-1-317lss7q287qzea30syrpzsnb7dlzcp5xqppqomkz0dpwlf0jv-3qzxl99iwvxo1b7zr2463xgdf 192.168.0.13:2377 passando o token e o IP para os novos computadores se unirem ao swarm. Antes de continuamos, vamos imaginar que voc\u00ea tenha deixado o ambiente rodando l\u00e1 e ficou um ano sem acessar, ai um belo dia voc\u00ea volta contudo n\u00e3o lembra qual maquina \u00e9 a que iniciou o swarm ou se ela est\u00e1 no swarm, para isso vamos usar o docker info . docker info E a sa\u00edda vai ser \u00b1 essa aqui: Saida Client: Context: default Debug Mode: false Plugins: app: Docker App ( Docker Inc., v0.9.1-beta3 ) Server: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 20 .10.0 Storage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: active NodeID: tv5szgmtbqunlbhkv4jqqdjrz Is Manager: true ClusterID: qxz7jhjgyd5ldoy7o1vcutbfn Managers: 1 Nodes: 1 Default Address Pool: 10 .0.0.0/8 SubnetSize: 24 Data Path Port: 4789 Orchestration: Task History Retention Limit: 5 Raft: Snapshot Interval: 10000 Number of Old Snapshots to Retain: 0 Heartbeat Tick: 1 Election Tick: 10 Dispatcher: Heartbeat Period: 5 seconds CA Configuration: Expiry Duration: 3 months Force Rotate: 0 Autolock Managers: false Root Rotation In Progress: false Node Address: 192 .168.0.13 Manager Addresses: 192 .168.0.13:2377 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc Default Runtime: runc Init Binary: docker-init containerd version: 269548fa27e0089a8b8278fc4fc781d7f65a939b runc version: ff819c7e9184c13b7c2607fe6c30ae19403a7aff init version: de40ad0 Security Options: apparmor seccomp Profile: default Kernel Version: 4 .4.0-210-generic Operating System: Alpine Linux v3.12 ( containerized ) OSType: linux Architecture: x86_64 CPUs: 8 Total Memory: 31 .42GiB Name: node1 ID: MLNO:DP7B:WIP5:4LLV:IUGJ:B5DP:KMRL:PZ2Z:A7Y3:7XLT:Z4NJ:UULR Docker Root Dir: /var/lib/docker Debug Mode: true File Descriptors: 38 Goroutines: 155 System Time: 2022 -03-05T23:02:27.17700525Z EventsListeners: 0 Registry: https://index.docker.io/v1/ Labels: Experimental: true Insecure Registries: 127 .0.0.1 127 .0.0.0/8 Live Restore Enabled: false Product License: Community Engine E nessa lista todas temos o Swarm: active , significa que o swarm est\u00e1 ativo e o Is Manager: true que diz que a maquina \u00e9 um gerente.","title":"Primeiro Gerente"},{"location":"docker%20swarm/iniciando%20o%20swarm/#primeiro-trabalhador","text":"Agora que temos nosso swarm, vamos inserir mais computadores, vamos usar o docker swarm join Nas outras instancias que voc\u00ea tiver, basta copia o comando com shift+insert que o docker fornece, no meu caso \u00e9: docker swarm join --token SWMTKN-1-317lss7q287qzea30syrpzsnb7dlzcp5xqppqomkz0dpwlf0jv-3qzxl99iwvxo1b7zr2463xgdf 192 .168.0.13:2377 E a saida: This node joined a swarm as a worker. Agora fa\u00e7a isso em todas as inst\u00e2ncias que voc\u00ea criou, recomendo n\u00e3o passar de 5 por quest\u00e3o de estabilidade e desempenho do servi\u00e7o. Se voc\u00ea perde o token, n\u00e3o precisa anotar nenhum comando, basta no ir ao terminal do gerente e pergunta para ele, usando o comando docker swarm join-token e passe quem voc\u00ea quer saber, worker ou maneger docker swarm join-token worker","title":"Primeiro Trabalhador"},{"location":"docker%20swarm/iniciando%20o%20swarm/#no-nodes","text":"Precisamos ver de uma forma geral nosso swarm, para isso vamos usar o comando docker node ls , no gerente, se usar no trabalhador, vai d\u00e1 um erro. ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION tv5szgmtbqunlbhkv4jqqdjrz * node1 Ready Active Leader 20 .10.0 somcfzr1i52h02s32g8162jv5 node2 Ready Active 20 .10.0 m3ug6hoe28zz1dap3ejhqug6n node3 Ready Active 20 .10.0 le62bq94ze1eagudy8dtx12ed node4 Ready Active 20 .10.0 aedfo2gsp1ky5cbzb7vmj1eqy node5 Ready Active 20 .10.0 Explicando: ID : \u00c9 o ID do n\u00f3, e o n\u00f3 com * \u00e9 o n\u00f3 que iniciou o swarm. HOSTNAME : Nome das m\u00e1quinas STATUS : Que est\u00e1 funcionando normalmente AVAILABILITY : Que vai rodar cont\u00eaineres normalmente MANAGER STATUS : Quem s\u00e3o os gerentes e o l\u00edder, todos os, n\u00f3 sem informa\u00e7\u00e3o, vazio, s\u00e3o workes para o docker. ENGINE VERSION : Engine \u00e9 a vers\u00e3o do docker. Para exemplificar, vamos roda o comando em um trabalhador: Error response from daemon: This node is not a swarm manager. Worker nodes can ' t be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager. Ent\u00e3o todos os comandos de altera\u00e7\u00e3o ou visualiza\u00e7\u00e3o, somente em n\u00f3 gerente. Por exemplo, vamos remover um n\u00f3 do cluster, usando o docker node rm , basta passar o ID do n\u00f3 que voc\u00ea quer derrubar. Execute Saida docker node rm aedfo2gsp1ky5cbzb7vmj1eqy Error response from daemon: rpc error: code = FailedPrecondition desc = node aedfo2gsp1ky5cbzb7vmj1eqy is not down and can ' t be removed O erro est\u00e1 dizendo que o n\u00f3 que tentamos remover n\u00e3o est\u00e1 com o status down , para resolver isso precisamos ir ao n\u00f3 que queremos derrubar e usar o docker swarm leave para muda o status do n\u00f3. Execute Saida docker swarm leave Node left the swarm. Retorne para o n\u00f3 gerente: ```bash docker node ls ``` Saida ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION tv5szgmtbqunlbhkv4jqqdjrz * node1 Ready Active Leader 20 .10.0 somcfzr1i52h02s32g8162jv5 node2 Ready Active 20 .10.0 m3ug6hoe28zz1dap3ejhqug6n node3 Ready Active 20 .10.0 le62bq94ze1eagudy8dtx12ed node4 Ready Active 20 .10.0 aedfo2gsp1ky5cbzb7vmj1eqy node5 Down Active 20 .10.0 O status do n\u00f3 5 mudou para down e com isso podemos remover efetivamente o n\u00f3. docker node rm aedfo2gsp1ky5cbzb7vmj1eqy E de outro docker node ls para confirmar. Adicione o n\u00f3 novamente para continuamos, use o docker swarm join","title":"N\u00f3 - Nodes"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/","text":"O que \u00e9 o Swarm? \u00b6 Imagine uma orquestra, com todas aquelas pessoas tocando instrumentos e com um maestro que transforma, direcionar e guia o ritmo da m\u00fasica. Exemplo de Orquestra E em seguida transformamos isso em c\u00f3digo e estrutura, chegamos nesse modelo: Pessoas que tocam a orquestra : S\u00e3o os computadores, para cada pessoa, imagine um computador Instrumentos : Para cada instrumento, imagine que seja um docker/trabalhador Maestro : Esse \u00e9 o cara, no docker, chamamos ele de Swarm/gerente, \u00e9 o respons\u00e1vel por dividir, gerenciar e garantir a estabilidade e tudo isso sem precisar que algu\u00e9m fique de olho. M\u00fasica : Por \u00faltimo a m\u00fasica, qual nota tocar? Quando tocar? Atento a cada nota, se houver um erro \u00e9 resolvido por ele e essa vamos chamar conta ineres, que v\u00e3o ser controlados pelo gerente. Ilustra\u00e7\u00e3o simples: Um pouco de teoria \u00b6 As caracter\u00edsticas de gerenciamento de agrupamento e orquestra\u00e7\u00e3o incorporadas no Docker Engine s\u00e3o constru\u00eddas utilizando o swarmkit. O Swarmkit \u00e9 um projeto separado que implementa a camada de orquestra\u00e7\u00e3o do Docker sendo utilizado diretamente dentro do Docker. Um swarm consiste em m\u00faltiplos hospedeiros Docker que funcionam em modo swarm e agem como gerentes (para gerir membros e delega\u00e7\u00e3o) e trabalhadores (que gerem servi\u00e7os de swarm). Um determinado anfitri\u00e3o Docker pode ser um gerente, um trabalhador, ou desempenhar ambas as fun\u00e7\u00f5es. Quando se cria um servi\u00e7o, define-se o seu estado ideal (n\u00famero de r\u00e9plicas, rede e recursos de armazenamento dispon\u00edveis, portas que o servi\u00e7o exp\u00f5e ao mundo exterior, e mais). O Docker trabalha para manter esse estado desejado. Por exemplo, se um n\u00f3 de trabalhador ficar indispon\u00edvel, Docker programa as tarefas desse n\u00f3 em outros n\u00f3s. Uma tarefa \u00e9 um cont\u00eainer em funcionamento que faz parte de um servi\u00e7o de swarm sendo gerido por um gerente de swarm, em oposi\u00e7\u00e3o a um cont\u00eainer independente. Uma das principais vantagens dos servi\u00e7os de swarm sobre os cont\u00eaineres independentes \u00e9 que se pode modificar a configura\u00e7\u00e3o de um servi\u00e7o, incluindo as redes e os volumes a que est\u00e1 ligado, sem necessidade de reiniciar manualmente o servi\u00e7o. O Docker ir\u00e1 atualizar a configura\u00e7\u00e3o, parar as tarefas do servi\u00e7o com a configura\u00e7\u00e3o desatualizada, e criar que correspondam \u00e0 configura\u00e7\u00e3o desejada. Quando o Docker funciona em modo de swarm, ainda \u00e9 poss\u00edvel executar cont\u00eaineres independentes em qualquer um dos anfitri\u00f5es Docker que participam no swarm, bem como servi\u00e7os de swarm. Uma diferen\u00e7a chave entre cont\u00eaineres independentes e servi\u00e7os de swarm \u00e9 que apenas os gerentes de swarm podem gerir um swarm, enquanto os cont\u00eaineres independentes podem ser iniciados em qualquer daemon. O Docker daemons da doca podem participar num swarm como gerentes, trabalhadores, ou ambos. Da mesma forma que se pode usar Docker Compose para definir e gerir cont\u00eaineres, pode-se definir e gerir pilhas de servi\u00e7os de Swarm. Continue a ler para obter detalhes sobre conceitos relacionados com os servi\u00e7os de swarm de Docker, incluindo n\u00f3s, servi\u00e7os, tarefas, e equil\u00edbrio de carga mais conhecido como Load balancing. A documenta\u00e7\u00e3o docker, vai definir o swarm/gerentes como manager e os trabalhadores de works. Nodes - n\u00f3 \u00b6 Um n\u00f3 \u00e9 um exemplo do motor Docker que participa no swarm. Tamb\u00e9m se pode pensar nisto como um n\u00f3 de Docker. Pode-se correr um ou mais n\u00f3s num \u00fanico computador f\u00edsico, ou em um servidor de nuvem, mas as implementa\u00e7\u00f5es de swarm de produ\u00e7\u00e3o incluem tipicamente n\u00f3s Docker distribu\u00eddos por m\u00faltiplas m\u00e1quinas f\u00edsicas e de nuvem. Para implantar a sua aplica\u00e7\u00e3o num swarm, submete uma defini\u00e7\u00e3o de servi\u00e7o a um n\u00f3 gerente. O n\u00f3 gerente despacha unidades de trabalho chamadas tarefas para n\u00f3s de trabalhadores. Os n\u00f3s gerentes tamb\u00e9m executam as fun\u00e7\u00f5es de orquestra\u00e7\u00e3o e gerenciamento de agrupamento necess\u00e1rias para manter o estado desejado do swarm. Os n\u00f3s gerentes elegem um \u00fanico l\u00edder para realizar tarefas de orquestra\u00e7\u00e3o. Esses n\u00f3s trabalhadores recebem e executam as tarefas enviadas pelos n\u00f3s gerentes. Por defeito, os n\u00f3s gerentes tamb\u00e9m executam servi\u00e7os como n\u00f3s trabalhadores, mas pode configur\u00e1-los para executar tarefas de gerenciamento exclusivamente e ser n\u00f3s apenas gerentes. Um agente corre em cada n\u00f3 de trabalhadores e informa sobre as tarefas que lhe s\u00e3o atribu\u00eddas. O n\u00f3 trabalhador notifica o n\u00f3 gerente do estado atual das suas tarefas atribu\u00eddas, para que o gerente possa manter o estado desejado de cada trabalhadores. Tarefas e servi\u00e7os \u00b6 Um servi\u00e7o \u00e9 a defini\u00e7\u00e3o das tarefas a executar nos n\u00f3s do gerente ou do trabalhador. \u00c9 a estrutura central do sistema do swarm e a raiz prim\u00e1ria da intera\u00e7\u00e3o do usu\u00e1rio com o swarm. Quando se cria um servi\u00e7o, especifica-se a imagem do cont\u00eainer a utilizar e os comandos a executar dentro de cont\u00eaineres em funcionamento. No modelo de servi\u00e7os replicados, o gerente do swarm distribui um n\u00famero espec\u00edfico de tarefas replicadas entre os n\u00f3s, com base na escala que se define no estado desejado. Para servi\u00e7os globais, o swarm executa uma tarefa para o servi\u00e7o em cada n\u00f3 dispon\u00edvel no agrupamento. Uma tarefa transporta um cont\u00eainer Docker e os comandos para correr dentro do cont\u00eainer. \u00c9 a unidade de programa\u00e7\u00e3o at\u00f4mica do swarm. Os n\u00f3s gerentes atribuem tarefas aos n\u00f3s de trabalhadores conforme o n\u00famero de r\u00e9plicas definidas na escala de servi\u00e7o. Uma vez atribu\u00edda uma tarefa a um n\u00f3, este n\u00e3o pode deslocar-se para outro n\u00f3. S\u00f3 pode funcionar com o n\u00f3 atribu\u00eddo ou falhar. Load balancing \u00b6 O gerente do swarm utiliza o equil\u00edbrio da carga de entrada para expor os servi\u00e7os que pretende disponibilizar externamente ao swarm. O gerente de swarm pode atribuir automaticamente ao servi\u00e7o em uma porta ou pode configurar uma porta para o servi\u00e7o. \u00c9 poss\u00edvel especificar qualquer porta n\u00e3o utilizada. Se n\u00e3o especificar uma porta, o gerente do swarm atribui ao servi\u00e7o uma porta no intervalo 30000-32767. Componentes externos, tais como equilibradores de carga de nuvem, podem acessar o servi\u00e7o na porta de qualquer n\u00f3 do cluster, quer o n\u00f3 esteja ou n\u00e3o a executar a tarefa para o servi\u00e7o. Todos os n\u00f3s na rota de entrada do swarm de liga\u00e7\u00f5es a uma inst\u00e2ncia de tarefa em execu\u00e7\u00e3o. O modo swarm tem um componente DNS interno que atribui automaticamente a cada servi\u00e7o no swarm uma entrada DNS. O gerente do swarm utiliza o equil\u00edbrio de carga interno para distribuir pedidos entre servi\u00e7os dentro do cluster, com base no nome DNS do servi\u00e7o. Raft - Consenso Distribu\u00eddos \u00b6 Antes de continuamos a estudar sobre swarm \u00e9 importante falar sobre um algoritmo que se chama Raft, o respons\u00e1vel pelas decis\u00f5es, \u00e9 o c\u00e9rebro dessa estrutura, para isso vou deixa aqui a Raft . \u00c9 muito importante que voc\u00ea entenda um pouco sobre o Raft, j\u00e1 que essa express\u00e3o vai se repetir mais algumas vezes, em kubernetes, por exemplo. Entender o raft \u00e9 criar melhores aplica\u00e7\u00f5es que usam docker, kubernetes e outras solu\u00e7\u00f5es que usam esse c\u00e9rebro por tr\u00e1s das decis\u00f5es. Nesse ponto, encontrei dois v\u00eddeos did\u00e1ticos sobre o assunto. Linuxtips usar a explica\u00e7\u00e3o visual do Ben Johnson para explicar sobre raft E tamb\u00e9m Algoritmos de consenso em sistemas distribu\u00eddos (teoria e pr\u00e1tica) - Edward Ribeiro que vai abordar o raft e mais alguns algoritmos de consenso.","title":"O que \u00e9 Swarm?"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#o-que-e-o-swarm","text":"Imagine uma orquestra, com todas aquelas pessoas tocando instrumentos e com um maestro que transforma, direcionar e guia o ritmo da m\u00fasica. Exemplo de Orquestra E em seguida transformamos isso em c\u00f3digo e estrutura, chegamos nesse modelo: Pessoas que tocam a orquestra : S\u00e3o os computadores, para cada pessoa, imagine um computador Instrumentos : Para cada instrumento, imagine que seja um docker/trabalhador Maestro : Esse \u00e9 o cara, no docker, chamamos ele de Swarm/gerente, \u00e9 o respons\u00e1vel por dividir, gerenciar e garantir a estabilidade e tudo isso sem precisar que algu\u00e9m fique de olho. M\u00fasica : Por \u00faltimo a m\u00fasica, qual nota tocar? Quando tocar? Atento a cada nota, se houver um erro \u00e9 resolvido por ele e essa vamos chamar conta ineres, que v\u00e3o ser controlados pelo gerente. Ilustra\u00e7\u00e3o simples:","title":"O que \u00e9 o Swarm?"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#um-pouco-de-teoria","text":"As caracter\u00edsticas de gerenciamento de agrupamento e orquestra\u00e7\u00e3o incorporadas no Docker Engine s\u00e3o constru\u00eddas utilizando o swarmkit. O Swarmkit \u00e9 um projeto separado que implementa a camada de orquestra\u00e7\u00e3o do Docker sendo utilizado diretamente dentro do Docker. Um swarm consiste em m\u00faltiplos hospedeiros Docker que funcionam em modo swarm e agem como gerentes (para gerir membros e delega\u00e7\u00e3o) e trabalhadores (que gerem servi\u00e7os de swarm). Um determinado anfitri\u00e3o Docker pode ser um gerente, um trabalhador, ou desempenhar ambas as fun\u00e7\u00f5es. Quando se cria um servi\u00e7o, define-se o seu estado ideal (n\u00famero de r\u00e9plicas, rede e recursos de armazenamento dispon\u00edveis, portas que o servi\u00e7o exp\u00f5e ao mundo exterior, e mais). O Docker trabalha para manter esse estado desejado. Por exemplo, se um n\u00f3 de trabalhador ficar indispon\u00edvel, Docker programa as tarefas desse n\u00f3 em outros n\u00f3s. Uma tarefa \u00e9 um cont\u00eainer em funcionamento que faz parte de um servi\u00e7o de swarm sendo gerido por um gerente de swarm, em oposi\u00e7\u00e3o a um cont\u00eainer independente. Uma das principais vantagens dos servi\u00e7os de swarm sobre os cont\u00eaineres independentes \u00e9 que se pode modificar a configura\u00e7\u00e3o de um servi\u00e7o, incluindo as redes e os volumes a que est\u00e1 ligado, sem necessidade de reiniciar manualmente o servi\u00e7o. O Docker ir\u00e1 atualizar a configura\u00e7\u00e3o, parar as tarefas do servi\u00e7o com a configura\u00e7\u00e3o desatualizada, e criar que correspondam \u00e0 configura\u00e7\u00e3o desejada. Quando o Docker funciona em modo de swarm, ainda \u00e9 poss\u00edvel executar cont\u00eaineres independentes em qualquer um dos anfitri\u00f5es Docker que participam no swarm, bem como servi\u00e7os de swarm. Uma diferen\u00e7a chave entre cont\u00eaineres independentes e servi\u00e7os de swarm \u00e9 que apenas os gerentes de swarm podem gerir um swarm, enquanto os cont\u00eaineres independentes podem ser iniciados em qualquer daemon. O Docker daemons da doca podem participar num swarm como gerentes, trabalhadores, ou ambos. Da mesma forma que se pode usar Docker Compose para definir e gerir cont\u00eaineres, pode-se definir e gerir pilhas de servi\u00e7os de Swarm. Continue a ler para obter detalhes sobre conceitos relacionados com os servi\u00e7os de swarm de Docker, incluindo n\u00f3s, servi\u00e7os, tarefas, e equil\u00edbrio de carga mais conhecido como Load balancing. A documenta\u00e7\u00e3o docker, vai definir o swarm/gerentes como manager e os trabalhadores de works.","title":"Um pouco de teoria"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#nodes-no","text":"Um n\u00f3 \u00e9 um exemplo do motor Docker que participa no swarm. Tamb\u00e9m se pode pensar nisto como um n\u00f3 de Docker. Pode-se correr um ou mais n\u00f3s num \u00fanico computador f\u00edsico, ou em um servidor de nuvem, mas as implementa\u00e7\u00f5es de swarm de produ\u00e7\u00e3o incluem tipicamente n\u00f3s Docker distribu\u00eddos por m\u00faltiplas m\u00e1quinas f\u00edsicas e de nuvem. Para implantar a sua aplica\u00e7\u00e3o num swarm, submete uma defini\u00e7\u00e3o de servi\u00e7o a um n\u00f3 gerente. O n\u00f3 gerente despacha unidades de trabalho chamadas tarefas para n\u00f3s de trabalhadores. Os n\u00f3s gerentes tamb\u00e9m executam as fun\u00e7\u00f5es de orquestra\u00e7\u00e3o e gerenciamento de agrupamento necess\u00e1rias para manter o estado desejado do swarm. Os n\u00f3s gerentes elegem um \u00fanico l\u00edder para realizar tarefas de orquestra\u00e7\u00e3o. Esses n\u00f3s trabalhadores recebem e executam as tarefas enviadas pelos n\u00f3s gerentes. Por defeito, os n\u00f3s gerentes tamb\u00e9m executam servi\u00e7os como n\u00f3s trabalhadores, mas pode configur\u00e1-los para executar tarefas de gerenciamento exclusivamente e ser n\u00f3s apenas gerentes. Um agente corre em cada n\u00f3 de trabalhadores e informa sobre as tarefas que lhe s\u00e3o atribu\u00eddas. O n\u00f3 trabalhador notifica o n\u00f3 gerente do estado atual das suas tarefas atribu\u00eddas, para que o gerente possa manter o estado desejado de cada trabalhadores.","title":"Nodes - n\u00f3"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#tarefas-e-servicos","text":"Um servi\u00e7o \u00e9 a defini\u00e7\u00e3o das tarefas a executar nos n\u00f3s do gerente ou do trabalhador. \u00c9 a estrutura central do sistema do swarm e a raiz prim\u00e1ria da intera\u00e7\u00e3o do usu\u00e1rio com o swarm. Quando se cria um servi\u00e7o, especifica-se a imagem do cont\u00eainer a utilizar e os comandos a executar dentro de cont\u00eaineres em funcionamento. No modelo de servi\u00e7os replicados, o gerente do swarm distribui um n\u00famero espec\u00edfico de tarefas replicadas entre os n\u00f3s, com base na escala que se define no estado desejado. Para servi\u00e7os globais, o swarm executa uma tarefa para o servi\u00e7o em cada n\u00f3 dispon\u00edvel no agrupamento. Uma tarefa transporta um cont\u00eainer Docker e os comandos para correr dentro do cont\u00eainer. \u00c9 a unidade de programa\u00e7\u00e3o at\u00f4mica do swarm. Os n\u00f3s gerentes atribuem tarefas aos n\u00f3s de trabalhadores conforme o n\u00famero de r\u00e9plicas definidas na escala de servi\u00e7o. Uma vez atribu\u00edda uma tarefa a um n\u00f3, este n\u00e3o pode deslocar-se para outro n\u00f3. S\u00f3 pode funcionar com o n\u00f3 atribu\u00eddo ou falhar.","title":"Tarefas e servi\u00e7os"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#load-balancing","text":"O gerente do swarm utiliza o equil\u00edbrio da carga de entrada para expor os servi\u00e7os que pretende disponibilizar externamente ao swarm. O gerente de swarm pode atribuir automaticamente ao servi\u00e7o em uma porta ou pode configurar uma porta para o servi\u00e7o. \u00c9 poss\u00edvel especificar qualquer porta n\u00e3o utilizada. Se n\u00e3o especificar uma porta, o gerente do swarm atribui ao servi\u00e7o uma porta no intervalo 30000-32767. Componentes externos, tais como equilibradores de carga de nuvem, podem acessar o servi\u00e7o na porta de qualquer n\u00f3 do cluster, quer o n\u00f3 esteja ou n\u00e3o a executar a tarefa para o servi\u00e7o. Todos os n\u00f3s na rota de entrada do swarm de liga\u00e7\u00f5es a uma inst\u00e2ncia de tarefa em execu\u00e7\u00e3o. O modo swarm tem um componente DNS interno que atribui automaticamente a cada servi\u00e7o no swarm uma entrada DNS. O gerente do swarm utiliza o equil\u00edbrio de carga interno para distribuir pedidos entre servi\u00e7os dentro do cluster, com base no nome DNS do servi\u00e7o.","title":"Load balancing"},{"location":"docker%20swarm/o%20que%20%C3%A9%20swarm/#raft-consenso-distribuidos","text":"Antes de continuamos a estudar sobre swarm \u00e9 importante falar sobre um algoritmo que se chama Raft, o respons\u00e1vel pelas decis\u00f5es, \u00e9 o c\u00e9rebro dessa estrutura, para isso vou deixa aqui a Raft . \u00c9 muito importante que voc\u00ea entenda um pouco sobre o Raft, j\u00e1 que essa express\u00e3o vai se repetir mais algumas vezes, em kubernetes, por exemplo. Entender o raft \u00e9 criar melhores aplica\u00e7\u00f5es que usam docker, kubernetes e outras solu\u00e7\u00f5es que usam esse c\u00e9rebro por tr\u00e1s das decis\u00f5es. Nesse ponto, encontrei dois v\u00eddeos did\u00e1ticos sobre o assunto. Linuxtips usar a explica\u00e7\u00e3o visual do Ben Johnson para explicar sobre raft E tamb\u00e9m Algoritmos de consenso em sistemas distribu\u00eddos (teoria e pr\u00e1tica) - Edward Ribeiro que vai abordar o raft e mais alguns algoritmos de consenso.","title":"Raft - Consenso Distribu\u00eddos"},{"location":"docker%20swarm/servi%C3%A7os/","text":"Servi\u00e7o \u00b6 Antes de subimos nossa primeira imagem, vamos realizar um teste, para entender melhor o fluxo que devemos seguir quando usamos o swarm: Escolha um dos n\u00f3s trabalhadores Execute o comando docker run -d -p 5000:5000 sposigor/app-flask-teste:1 Libere a porta em OPEN PORT e digite 5000 : Ele vai abrir uma nova aba com a aplica\u00e7\u00e3o. Se lembra de como funciona o swarm? Ent\u00e3o, escolha outro n\u00f3 trabalhador e execute o docker ps , fa\u00e7a o mesmo para o n\u00f3 gerente e em ambos vamos ter esse resultado: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Estranho n\u00e9, se o swarm cria essa conex\u00e3o com todos, porque o cont\u00eainer que subimos n\u00e3o foi refletido nos demais? \u00c9 porque rodamos uma imagem local, e n\u00e3o no modo swarm. Certo, o modo swarm precisa de um comando mais espec\u00edfico para roda a imagem que \u00e9 o docker service create . Antes de executar, vamos remover a imagem local, v\u00e1 ao terminal do n\u00f3 que a imagem est\u00e1 rodando: docker rm 56e3abd7f344 --force E agora sim, vamos criar nosso servi\u00e7o: Execute Saida docker service create -p 5000 :5000 sposigor/app-flask-teste:1 A diferen\u00e7a \u00e9 somente o comando inicial, service create , que vai permitir que todos os n\u00f3s se comuniquem no modo swarm. Error response from daemon: This node is not a swarm manager. Worker nodes can ' t be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager. Se voc\u00ea executou o comando em um n\u00f3 trabalhador, vai receber o erro do qual os trabalhadores n\u00e3o podem iniciar um servi\u00e7o, apesar do docker run executar, para subir uma imagem no modo swarm \u00e9 preciso fazer atrav\u00e9s do gerente. V\u00e1 ao n\u00f3 gerente e execute o comando. Execute Saida docker service create -p 5000 :5000 sposigor/app-flask-teste:1 9q3asggjagwfw89vrojc1urew overall progress: 1 out of 1 tasks 1 /1: running [================================================== > ] verify: Service converged Ent\u00e3o ele est\u00e1 informando que o Service converged , mas o que isso significa? Basicamente que ocorreu bem o processo de cria\u00e7\u00e3o de um servi\u00e7o e com isso ele criou uma tarefa: overall progress: 1 out of 1 tasks . Vamos por partes, execute o docker servie ls . Execute Saida docker service ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME MODE REPLICAS IMAGE PORTS 9q3asggjagwf epic_almeida replicated 1 /1 sposigor/app-flask-teste:1 *:5000->5000/tcp Analisar o cabe\u00e7alho para entender o que significa cada coluna: ID : \u00c9 o ID do servi\u00e7o NAME : \u00c9 o nome do servi\u00e7o MODE : \u00c9 o modo do servi\u00e7o REPLICAS : \u00c9 a quantidade de replicas IMAGE : \u00c9 a imagem que o servi\u00e7o est\u00e1 rodando PORTS : \u00c9 a porta que est\u00e1 disponivel Para saber qual maquina est\u00e1 executando o container, use o docker service ps , e passe o ID do servi\u00e7o. Execute Saida docker service ps 9q3asggjagwf Lembrando s\u00f3 o n\u00f3 gerente pode executar Se quiser como \u00e9 possivel passar s\u00f3 alguns caracteres docker service ps 9q3 , desde que n\u00e3o tenha outro servi\u00e7o com o mesmo inicio. ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3qpfhbbmigm4 epic_almeida.1 sposigor/app-flask-teste:1 node2 Running Running 28 minutes ago Analisar o cabe\u00e7alho para entender o que significa cada coluna: ID : \u00c9 o ID do servi\u00e7o. NAME : \u00c9 o nome do servi\u00e7o. IMAGE : \u00c9 a imagem que o servi\u00e7o est\u00e1 rodando. NODE : \u00c9 o n\u00f3 que est\u00e1 executando a imagem naquele momento. DESIRED STATE : \u00c9 o estado desejado da tarefa, pode est\u00e1 em (running, shutdown, or accepted). CURRENT STATE : \u00c9 o estado atual da tarefa. ERROR : Se houver algum erro pe informado PORTS : \u00c9 a porta que est\u00e1 dispon\u00edvel. Bem, agora que o servi\u00e7o est\u00e1 dispon\u00edvel como swarm, vamos testa a aplica\u00e7\u00e3o em outras m\u00e1quinas, v\u00e1 em um n\u00f3 trabalhador e libere a porta 5000. Por mais que apenas uma das m\u00e1quinas esteja rodando a aplica\u00e7\u00e3o, ela est\u00e1 dispon\u00edvel na porta 5000 de todas as maquinas. Quem \u00e9 que o cara que est\u00e1 ajudando a resolver esse problema \u00e9 o Routing Mesh atrav\u00e9s do Load balancing. Routing Mesh \u00b6 O modo swarm do Docker Engine facilita a publica\u00e7\u00e3o de portas para servi\u00e7os para disponibiliz\u00e1-los para recursos fora do swarm. Todos os n\u00f3s, ficam dispon\u00edveis em uma malha de roteamento de entrada. A malha de roteamento permite que cada n\u00f3 no enxame aceite conex\u00f5es em portas publicadas para qualquer servi\u00e7o em execu\u00e7\u00e3o no enxame, mesmo que n\u00e3o haja nenhuma tarefa em execu\u00e7\u00e3o no n\u00f3. A malha de roteamento roteia todas as solicita\u00e7\u00f5es de entrada para portas publicadas em n\u00f3s dispon\u00edveis para um cont\u00eainer ativo. Testar na pr\u00e1tica, vamos subir a mesma imagem na mesma porta para ver o que ocorre: Execute Saida docker service create -p 5000 :5000 sposigor/app-flask-teste:1 Lembrando s\u00f3 o n\u00f3 gerente pode executar Error response from daemon: rpc error: code = InvalidArgument desc = port '5000' is already in use by service 'epic_almeida' ( 9q3asggjagwfw89vrojc1urew ) as an ingress port E assim ele nos d\u00e1 um erro, para verificar melhor vamos remover o cont\u00eainer que no meu caso est\u00e1 rodando no node2: Execute docker rm 6f21 --force Voltei para o n\u00f3 gerente e execute o docker service ls : Execute Saida docker service ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME MODE REPLICAS IMAGE PORTS 9q3asggjagwf epic_almeida replicated 1 /1 sposigor/app-flask-teste:1 *:5000->5000/tcp Est\u00e1 dizendo que o servi\u00e7o ainda est\u00e1 rodando, vamos usar o docker service ps e passa o ID do servi\u00e7o: Execute Saida docker service ps 9q3asggjagwf Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS f91ba1oajs39 epic_almeida.1 sposigor/app-flask-teste:1 node3 Running Running 2 minutes ago 3qpfhbbmigm4 \\_ epic_almeida.1 sposigor/app-flask-teste:1 node2 Shutdown Failed 3 minutes ago \"task: non-zero exit (137)\" E o swarm se encarregou de subir a imagem automaticamente em outro n\u00f3, no caso do node2 para o node3, ent\u00e3o se fomos l\u00e1 na porta 5000 novamente, a aplica\u00e7\u00e3o vai estar rodando normalmente. Gerente \u00b6 Imagine que sem querer voc\u00ea desligue o computador que est\u00e1 o gerente? E agora ser\u00e1 que deu ruim? Sim, se voc\u00ea possui somente um gerente no seu swarm, o swarm vai ser desconfigurado, sendo necess\u00e1rio, reconfigurar o swarm. Mas os servi\u00e7os param? N\u00e3o, eles v\u00e3o continuar, contudo, n\u00e3o existe mais ningu\u00e9m para garantir a estabilidade do servi\u00e7o. Como podemos resolver essa categoria de situa\u00e7\u00e3o? Basta fazer o backup da pasta do swarm. Isso vai ser feito de forma manual, porque o docker n\u00e3o disponibiliza ainda nenhuma ferramenta para backup nesse sentido. Fa\u00e7a uma c\u00f3pia da sua pasta que cont\u00e9m as informa\u00e7\u00f5es do swarm, no Linux ela fica em /var/lib/docker/swarm/ No Linux basta usar o comando cp -r /var/lib/docker/swarm/ backup Agora com seu backup feito, para subir um swarm do zero com as configura\u00e7\u00f5es nesse backup: Vamos fazer o caminho inverso, cp -r backup/* /var/lib/docker/swarm/ Durante a cria\u00e7\u00e3o do swarm vamos passa uma nova flag --force-new-cluster docker swarm init --force-new-cluster --advertise-addr 192.168.0.13 Essa \u00e9 a f\u00f3rmula mais usada para recupera\u00e7\u00e3o de swarm, independente do que tenha acontecido. Numa aplica\u00e7\u00e3o pequena, n\u00e3o faz sentido ter que criar esses backups, por\u00e9m quando os servi\u00e7os s\u00e3o muitos e a infraestrutura \u00e9 maior ainda, \u00e9 necess\u00e1rio ter garantias para recuperar rapidamente o estado anterior ao problema. Criando Gerentes \u00b6 Talvez, ter mais gerentes no seu swarm possa ajudar a resolve esse problema, afinal por mais que o backup seja uma solu\u00e7\u00e3o, para determinadas situa\u00e7\u00f5es, podemos simplesmente adicionar novos Gerentes. Adicione mais uma inst\u00e2ncia no docker play e vamos passar para o token do comando docker swarm join-token e passe o manager: Execute Saida docker swarm join-token manager Lembrando s\u00f3 o n\u00f3 gerente pode executar To add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-1waduqjabmpmygpy6xicwtt1srwbchelwnu8fsv3fj8qk4jdvi-8tke9gy4e6crw0euq2tki1m0y 192 .168.0.13:2377 V\u00e1 \u00e0 inst\u00e2ncia criada e execute o comando passado. Execute Saida docker swarm join --token SWMTKN-1-1waduqjabmpmygpy6xicwtt1srwbchelwnu8fsv3fj8qk4jdvi-8tke9gy4e6crw0euq2tki1m0y 192 .168.0.13:2377 Lembrando s\u00f3 o n\u00f3 gerente pode executar This node joined a swarm as a manager. Se por acaso estive com a configura\u00e7\u00e3o de 1 gerente e 4 trabalhadores, basta derruba um dos trabalhadores e remover do swarm, e em seguida suba novamente como gerente. Em um dos gerentes: Execute Saida docker node ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION fhiexwgtupwto0yzz5i4nbve7 node1 Ready Active Leader 20 .10.0 x3ghs9kh5nftbnnd7otoyg4hu node2 Ready Active 20 .10.0 nufnx3dlv0orjgb0dm5li4aah node3 Ready Active 20 .10.0 jvxn6dt0zob66po3d5rdrbaru node4 Ready Active 20 .10.0 1bk98ofc3qasq9n6v298x4jjv * node5 Ready Active Reachable 20 .10.0 E pronto o seu novo gerente j\u00e1 est\u00e1 rodando normalmente. Raft na pr\u00e1tica \u00b6 Vamos reiniciar nossa aplica\u00e7\u00e3o no docker play feche com o close session e de outro start, lembrando dependendo o servi\u00e7o pode estar cheio, mas basta continuar tentando at\u00e9 conseguir. Click na chave de boca e selecione a op\u00e7\u00e3o: Ap\u00f3s carregar a configura\u00e7\u00e3o, execute docker node ls em qualquer um dos gerentes: Execute Saida docker node ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 3g4vyx694h4pd3dxlj442x88z manager1 Ready Active Reachable 20 .10.0 lheo3at6gjkka6uoehk3lr0ow * manager2 Ready Active Leader 20 .10.0 ptvwj6n2ums5o8b0lsfabhxgk manager3 Ready Active Reachable 20 .10.0 n2230i73ivqhyqqg0ty2hw3nd worker1 Ready Active 20 .10.0 tdiufxyidqv47atj937ffthb1 worker2 Ready Active 20 .10.0 Agora temos 3 gerentes, sendo um deles o lider. Vamos derrubar o lider com docker swarm leave --force no terminal do lider. Assim que executar v\u00e1 em outro gerente: Execute Saida docker node ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 3g4vyx694h4pd3dxlj442x88z * manager1 Ready Active Leader 20 .10.0 lheo3at6gjkka6uoehk3lr0ow manager2 Unknown Active Unreachable 20 .10.0 ptvwj6n2ums5o8b0lsfabhxgk manager3 Ready Active Reachable 20 .10.0 n2230i73ivqhyqqg0ty2hw3nd worker1 Ready Active 20 .10.0 tdiufxyidqv47atj937ffthb1 worker2 Ready Active 20 .10.0 Temos um novo l\u00edder agora, que pe maneger1, enquanto o manager3 est\u00e1 com o MANAGER STATUS de Unreachable , ou seja, est\u00e1 fora do swarm por algum motivo. O que ocorreu? O Raft, foi acionado para gera uma elei\u00e7\u00e3o, e o vencedor da elei\u00e7\u00e3o foi o maneger1. Recomenda\u00e7\u00e3o do docker sobre os gerentes, eles recomendam que tenhamos n\u00fameros impares, 3, 5 ou 7. Removendo Gerentes \u00b6 Vamos remover o nosso gerente que est\u00e1 Unreachable . Em qualquer gerente execute: Execute Saida docker node rm manager2 Lembrando s\u00f3 o n\u00f3 gerente pode executar Error response from daemon: rpc error: code = FailedPrecondition desc = node lheo3at6gjkka6uoehk3lr0ow is a cluster manager and is a member of the raft cluster. It must be demoted to worker before removal E ele deu um erro, ele est\u00e1 dizendo que precisamos rebaixar o menager2 para trabalhador antes de remover efetivamente do swarm, usando o docker node demote : Execute Saida docker node demote manager2 Lembrando s\u00f3 o n\u00f3 gerente pode executar Manager manager2 demoted in the swarm. Agora \u00e9 poss\u00edvel remover com o docker node rm , fa\u00e7a e vamos adicionar o gerente no swarm com o docker swarm join-token manager pegue o comando que ele retorna e cola no gerente que voc\u00ea removeu. Restri\u00e7\u00e3o de Servi\u00e7os \u00b6 Por padr\u00e3o o docker permite que o gerente rode servi\u00e7os, ou seja, \u00e9 poss\u00edvel que a maquina que vai efetivamente roda alguma imagem seja um gerente e n\u00e3o um trabalhador. Mas vamos supor que por algum motivo voc\u00ea n\u00e3o quer permitir que o gerente rode qualquer servi\u00e7o, ou s\u00f3 quer determinado servi\u00e7o rode no gerente. Ent\u00e3o podemos resolver da seguinte forma: 1 2 3 4 Suba a imagem docker service create -p 5000 :5000 sposigor/app-flask-teste:1 Lembrando s\u00f3 o n\u00f3 gerente pode executar docker node ls E a sa\u00edda: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 3g4vyx694h4pd3dxlj442x88z manager1 Ready Active Leader 20 .10.0 3xtgnz4d2098nnhjrq5ilv7l6 * manager2 Ready Active Reachable 20 .10.0 ptvwj6n2ums5o8b0lsfabhxgk manager3 Ready Active Reachable 20 .10.0 n2230i73ivqhyqqg0ty2hw3nd worker1 Ready Active 20 .10.0 tdiufxyidqv47atj937ffthb1 worker2 Ready Active 20 .10.0 Aqui vamos alterar o estado da AVAILABILITY , para conseguimos fazer essa restri\u00e7\u00e3o do n\u00f3. Enquanto estiver como Active significa que est\u00e1 dispon\u00edvel para roda servi\u00e7os. Para alterar vamos usar o docker node update Vamos passa a flag --availability que tem 3 estados (\"active\"|\"pause\"|\"drain\"). No nosso caso queremos usar o drain para que o gerente n\u00e3o execute nenhum servi\u00e7o. Por fim basta passa qual maquina voc\u00ea quer alterar o estado. docker node update --availability drain manager2 Rode novamente o docker node ls docker node ls Saida: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 3g4vyx694h4pd3dxlj442x88z manager1 Ready Active Leader 20 .10.0 3xtgnz4d2098nnhjrq5ilv7l6 * manager2 Ready Drain Reachable 20 .10.0 ptvwj6n2ums5o8b0lsfabhxgk manager3 Ready Active Reachable 20 .10.0 n2230i73ivqhyqqg0ty2hw3nd worker1 Ready Active 20 .10.0 tdiufxyidqv47atj937ffthb1 worker2 Ready Active 20 .10.0 Assim conseguimos alterar a AVAILABILITY do manager2 para a Drain e com isso ele se torna inacessivel para executar servi\u00e7os, mas continuar a executar a atividade de gerente normalmente. Mas vamos supor que voc\u00ea queira fazer isso em todos os gerentes de uma vez, vamos ter que alterar 1 por 1? N\u00e3o, vamos usar um comando mais generalista que vai definir aonde queremos que o servi\u00e7o rode. Usando o docker service update e passando a flag --constraint-add que vai restrigir determido servi\u00e7o para um grupo especifico. Execute Saida Vamos passa para o comando a flag --constraint-add Nesse caso queremos especificar um tipo de n\u00f3, ent\u00e3o vamos usar o node.role==worker E por ultimo vamos passa o ID do servi\u00e7o para saber qual servi\u00e7o vamos aplicar essa restri\u00e7\u00e3o docker service update --constraint-add node.role == worker gnj34gs43ebi gnj34gs43ebi overall progress: 1 out of 1 tasks 1 /1: running [================================================== > ] verify: Service converged Alguns cuidados durante a restri\u00e7\u00e3o desses servi\u00e7os, vamos supor que voc\u00ea fa\u00e7a essa restri\u00e7\u00e3o usando um n\u00f3 que n\u00e3o existe, ele vai rodar o comando e todas as suas aplica\u00e7\u00f5es se tornaram indisponiveis, porque a restri\u00e7\u00e3o que houve, restringiu para um n\u00f3 inexistente. Quando restringir o servi\u00e7os nos gerente? Queremos rodar servi\u00e7os no manager, que s\u00e3o servi\u00e7os de monitoramento, servi\u00e7os de seguran\u00e7a, ou seja, servi\u00e7os que s\u00e3o cr\u00edticos, que dependemos na nossa aplica\u00e7\u00e3o. \u00c9 importante pondera essa quest\u00e3o, para entender quais s\u00e3o os pontos criticos da aplica\u00e7\u00e3o. Replicas \u00b6 J\u00e1 vimos essa palavra aqui: Execute Saida docker service ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME MODE REPLICAS IMAGE PORTS gnj34gs43ebi dazzling_lamarr replicated 1 /1 sposigor/app-flask-teste:1 *:5000->5000/tcp Certo o que significa isso, bem para explicar a gente precisa descobrir aonde nosso servi\u00e7o est\u00e1 rodando: Execute Saida docker service ps gnj34gs43ebi Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS w105x64ylhne dazzling_lamarr.1 sposigor/app-flask-teste:1 worker2 Running Running 37 minutes ago O servi\u00e7o est\u00e1 rodando no worker2 , para explicar vamos repassar pelo conceito de Load Balancing. Quando a porta 5000 \u00e9 acessada independete da maquina que estiver no swarm, a aplica\u00e7\u00e3o \u00e9 diponibilizada, por\u00e9m se houver intera\u00e7\u00e3o, vamos supor que haja um cadastro de um usuario, o que ocorre na pratica \u00e9 que essa altera\u00e7\u00e3o vai ser redirecionada para a maquina que est\u00e1 rodando essa imagem. E a Replica serve para isso, para que possamos usar mais de uma maquina nesse processo de processamento dessa demanda. Ent\u00e3o vamos replicar essa imagem, que para outras maquinas com o docker service update . Execute Saida docker service update --replicas 8 gnj34gs43ebi Lembrando s\u00f3 o n\u00f3 gerente pode executar gnj34gs43ebi overall progress: 4 out of 4 tasks 1 /4: running [================================================== > ] 2 /4: running [================================================== > ] 3 /4: running [================================================== > ] 4 /4: running [================================================== > ] verify: Service converged Vamos executar o docker service ls para verificar como est\u00e1 as r\u00e9plicas: docker service ls docker service ps Removendo --constraint-rm 8 Replicas docker service ls Lembrando s\u00f3 o n\u00f3 gerente pode executar Saida: ID NAME MODE REPLICAS IMAGE PORTS gnj34gs43ebi dazzling_lamarr replicated 4 /4 sposigor/app-flask-teste:1 *:5000->5000/tcp Temos agora, 4 replicas Para saber quais maquinas est\u00e3o rodando o servi\u00e7o: docker service ps gnj34gs43ebi Saida: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS w105x64ylhne dazzling_lamarr.1 sposigor/app-flask-teste:1 worker2 Running Running 52 minutes ago v9bmopdi3nr3 dazzling_lamarr.2 sposigor/app-flask-teste:1 worker1 Running Running 3 minutes ago sp81t2z56n67 dazzling_lamarr.3 sposigor/app-flask-teste:1 worker1 Running Running 4 minutes ago oxdvxulccegb dazzling_lamarr.4 sposigor/app-flask-teste:1 worker2 Running Running 5 minutes ago Lembrando que fizemo a restri\u00e7\u00e3o para somente workes executarem as tarefas. Vamos remover a restri\u00e7\u00e3o e tentar novamente com 8 replicas Vamos passar os mesmos par\u00e2metros que usamos para restringir os servi\u00e7os docker service update --constraint-rm node.role == worker gnj34gs43ebi J\u00e1 vamos passar 8 no par\u00e2metro. docker service update --replicas 8 gnj34gs43ebi Em seguida: docker service ps gnj34gs43ebi E a sa\u00edda: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS w105x64ylhne dazzling_lamarr.1 sposigor/app-flask-teste:1 worker2 Running Running about an hour ago v9bmopdi3nr3 dazzling_lamarr.2 sposigor/app-flask-teste:1 worker1 Running Running 17 minutes ago sp81t2z56n67 dazzling_lamarr.3 sposigor/app-flask-teste:1 worker1 Running Running 17 minutes ago oxdvxulccegb dazzling_lamarr.4 sposigor/app-flask-teste:1 worker2 Running Running 18 minutes ago gs8m3brh4sw6 dazzling_lamarr.5 sposigor/app-flask-teste:1 manager3 Running Running 3 minutes ago sr1u0kr19hut dazzling_lamarr.6 sposigor/app-flask-teste:1 manager3 Running Running 3 minutes ago z80bqt30sscq dazzling_lamarr.7 sposigor/app-flask-teste:1 manager3 Running Running 3 minutes ago 2yjmab0lkl7q dazzling_lamarr.8 sposigor/app-flask-teste:1 manager1 Running Running 3 minutes ago Com isso agora temos diversas maquinas rodando nosso servi\u00e7o, ou seja, o poder de processamento aumentou. Porque assim que ocorrer uma demanda de processamento, o load balancing por tr\u00e1s vai estar resolvendo isso. Tamb\u00e9m \u00e9 poss\u00edvel usar o docker service scale , vamos passar o ID do servi\u00e7o e a quantidade de r\u00e9plicas para escalar docker service scale gnj34gs43ebi = 6 Temos tamb\u00e9m a op\u00e7\u00e3o de passar diretamente na cria\u00e7\u00e3o do servi\u00e7o, basta passa a flag --mode e o par\u00e2metro global docker service create -p 5000 :5000 --mode global sposigor/app-flask-teste:1 \u00c9 importante lembrar que n\u00e3o \u00e9 recomendado passar todos os servi\u00e7os com o par\u00e2metro global, afinal isso vai exigir um consumo que poderia ser poupado da aplica\u00e7\u00e3o no processador individual. Seja sempre critico com essa necessidade.","title":"Servi\u00e7o"},{"location":"docker%20swarm/servi%C3%A7os/#servico","text":"Antes de subimos nossa primeira imagem, vamos realizar um teste, para entender melhor o fluxo que devemos seguir quando usamos o swarm: Escolha um dos n\u00f3s trabalhadores Execute o comando docker run -d -p 5000:5000 sposigor/app-flask-teste:1 Libere a porta em OPEN PORT e digite 5000 : Ele vai abrir uma nova aba com a aplica\u00e7\u00e3o. Se lembra de como funciona o swarm? Ent\u00e3o, escolha outro n\u00f3 trabalhador e execute o docker ps , fa\u00e7a o mesmo para o n\u00f3 gerente e em ambos vamos ter esse resultado: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Estranho n\u00e9, se o swarm cria essa conex\u00e3o com todos, porque o cont\u00eainer que subimos n\u00e3o foi refletido nos demais? \u00c9 porque rodamos uma imagem local, e n\u00e3o no modo swarm. Certo, o modo swarm precisa de um comando mais espec\u00edfico para roda a imagem que \u00e9 o docker service create . Antes de executar, vamos remover a imagem local, v\u00e1 ao terminal do n\u00f3 que a imagem est\u00e1 rodando: docker rm 56e3abd7f344 --force E agora sim, vamos criar nosso servi\u00e7o: Execute Saida docker service create -p 5000 :5000 sposigor/app-flask-teste:1 A diferen\u00e7a \u00e9 somente o comando inicial, service create , que vai permitir que todos os n\u00f3s se comuniquem no modo swarm. Error response from daemon: This node is not a swarm manager. Worker nodes can ' t be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager. Se voc\u00ea executou o comando em um n\u00f3 trabalhador, vai receber o erro do qual os trabalhadores n\u00e3o podem iniciar um servi\u00e7o, apesar do docker run executar, para subir uma imagem no modo swarm \u00e9 preciso fazer atrav\u00e9s do gerente. V\u00e1 ao n\u00f3 gerente e execute o comando. Execute Saida docker service create -p 5000 :5000 sposigor/app-flask-teste:1 9q3asggjagwfw89vrojc1urew overall progress: 1 out of 1 tasks 1 /1: running [================================================== > ] verify: Service converged Ent\u00e3o ele est\u00e1 informando que o Service converged , mas o que isso significa? Basicamente que ocorreu bem o processo de cria\u00e7\u00e3o de um servi\u00e7o e com isso ele criou uma tarefa: overall progress: 1 out of 1 tasks . Vamos por partes, execute o docker servie ls . Execute Saida docker service ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME MODE REPLICAS IMAGE PORTS 9q3asggjagwf epic_almeida replicated 1 /1 sposigor/app-flask-teste:1 *:5000->5000/tcp Analisar o cabe\u00e7alho para entender o que significa cada coluna: ID : \u00c9 o ID do servi\u00e7o NAME : \u00c9 o nome do servi\u00e7o MODE : \u00c9 o modo do servi\u00e7o REPLICAS : \u00c9 a quantidade de replicas IMAGE : \u00c9 a imagem que o servi\u00e7o est\u00e1 rodando PORTS : \u00c9 a porta que est\u00e1 disponivel Para saber qual maquina est\u00e1 executando o container, use o docker service ps , e passe o ID do servi\u00e7o. Execute Saida docker service ps 9q3asggjagwf Lembrando s\u00f3 o n\u00f3 gerente pode executar Se quiser como \u00e9 possivel passar s\u00f3 alguns caracteres docker service ps 9q3 , desde que n\u00e3o tenha outro servi\u00e7o com o mesmo inicio. ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3qpfhbbmigm4 epic_almeida.1 sposigor/app-flask-teste:1 node2 Running Running 28 minutes ago Analisar o cabe\u00e7alho para entender o que significa cada coluna: ID : \u00c9 o ID do servi\u00e7o. NAME : \u00c9 o nome do servi\u00e7o. IMAGE : \u00c9 a imagem que o servi\u00e7o est\u00e1 rodando. NODE : \u00c9 o n\u00f3 que est\u00e1 executando a imagem naquele momento. DESIRED STATE : \u00c9 o estado desejado da tarefa, pode est\u00e1 em (running, shutdown, or accepted). CURRENT STATE : \u00c9 o estado atual da tarefa. ERROR : Se houver algum erro pe informado PORTS : \u00c9 a porta que est\u00e1 dispon\u00edvel. Bem, agora que o servi\u00e7o est\u00e1 dispon\u00edvel como swarm, vamos testa a aplica\u00e7\u00e3o em outras m\u00e1quinas, v\u00e1 em um n\u00f3 trabalhador e libere a porta 5000. Por mais que apenas uma das m\u00e1quinas esteja rodando a aplica\u00e7\u00e3o, ela est\u00e1 dispon\u00edvel na porta 5000 de todas as maquinas. Quem \u00e9 que o cara que est\u00e1 ajudando a resolver esse problema \u00e9 o Routing Mesh atrav\u00e9s do Load balancing.","title":"Servi\u00e7o"},{"location":"docker%20swarm/servi%C3%A7os/#routing-mesh","text":"O modo swarm do Docker Engine facilita a publica\u00e7\u00e3o de portas para servi\u00e7os para disponibiliz\u00e1-los para recursos fora do swarm. Todos os n\u00f3s, ficam dispon\u00edveis em uma malha de roteamento de entrada. A malha de roteamento permite que cada n\u00f3 no enxame aceite conex\u00f5es em portas publicadas para qualquer servi\u00e7o em execu\u00e7\u00e3o no enxame, mesmo que n\u00e3o haja nenhuma tarefa em execu\u00e7\u00e3o no n\u00f3. A malha de roteamento roteia todas as solicita\u00e7\u00f5es de entrada para portas publicadas em n\u00f3s dispon\u00edveis para um cont\u00eainer ativo. Testar na pr\u00e1tica, vamos subir a mesma imagem na mesma porta para ver o que ocorre: Execute Saida docker service create -p 5000 :5000 sposigor/app-flask-teste:1 Lembrando s\u00f3 o n\u00f3 gerente pode executar Error response from daemon: rpc error: code = InvalidArgument desc = port '5000' is already in use by service 'epic_almeida' ( 9q3asggjagwfw89vrojc1urew ) as an ingress port E assim ele nos d\u00e1 um erro, para verificar melhor vamos remover o cont\u00eainer que no meu caso est\u00e1 rodando no node2: Execute docker rm 6f21 --force Voltei para o n\u00f3 gerente e execute o docker service ls : Execute Saida docker service ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME MODE REPLICAS IMAGE PORTS 9q3asggjagwf epic_almeida replicated 1 /1 sposigor/app-flask-teste:1 *:5000->5000/tcp Est\u00e1 dizendo que o servi\u00e7o ainda est\u00e1 rodando, vamos usar o docker service ps e passa o ID do servi\u00e7o: Execute Saida docker service ps 9q3asggjagwf Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS f91ba1oajs39 epic_almeida.1 sposigor/app-flask-teste:1 node3 Running Running 2 minutes ago 3qpfhbbmigm4 \\_ epic_almeida.1 sposigor/app-flask-teste:1 node2 Shutdown Failed 3 minutes ago \"task: non-zero exit (137)\" E o swarm se encarregou de subir a imagem automaticamente em outro n\u00f3, no caso do node2 para o node3, ent\u00e3o se fomos l\u00e1 na porta 5000 novamente, a aplica\u00e7\u00e3o vai estar rodando normalmente.","title":"Routing Mesh"},{"location":"docker%20swarm/servi%C3%A7os/#gerente","text":"Imagine que sem querer voc\u00ea desligue o computador que est\u00e1 o gerente? E agora ser\u00e1 que deu ruim? Sim, se voc\u00ea possui somente um gerente no seu swarm, o swarm vai ser desconfigurado, sendo necess\u00e1rio, reconfigurar o swarm. Mas os servi\u00e7os param? N\u00e3o, eles v\u00e3o continuar, contudo, n\u00e3o existe mais ningu\u00e9m para garantir a estabilidade do servi\u00e7o. Como podemos resolver essa categoria de situa\u00e7\u00e3o? Basta fazer o backup da pasta do swarm. Isso vai ser feito de forma manual, porque o docker n\u00e3o disponibiliza ainda nenhuma ferramenta para backup nesse sentido. Fa\u00e7a uma c\u00f3pia da sua pasta que cont\u00e9m as informa\u00e7\u00f5es do swarm, no Linux ela fica em /var/lib/docker/swarm/ No Linux basta usar o comando cp -r /var/lib/docker/swarm/ backup Agora com seu backup feito, para subir um swarm do zero com as configura\u00e7\u00f5es nesse backup: Vamos fazer o caminho inverso, cp -r backup/* /var/lib/docker/swarm/ Durante a cria\u00e7\u00e3o do swarm vamos passa uma nova flag --force-new-cluster docker swarm init --force-new-cluster --advertise-addr 192.168.0.13 Essa \u00e9 a f\u00f3rmula mais usada para recupera\u00e7\u00e3o de swarm, independente do que tenha acontecido. Numa aplica\u00e7\u00e3o pequena, n\u00e3o faz sentido ter que criar esses backups, por\u00e9m quando os servi\u00e7os s\u00e3o muitos e a infraestrutura \u00e9 maior ainda, \u00e9 necess\u00e1rio ter garantias para recuperar rapidamente o estado anterior ao problema.","title":"Gerente"},{"location":"docker%20swarm/servi%C3%A7os/#criando-gerentes","text":"Talvez, ter mais gerentes no seu swarm possa ajudar a resolve esse problema, afinal por mais que o backup seja uma solu\u00e7\u00e3o, para determinadas situa\u00e7\u00f5es, podemos simplesmente adicionar novos Gerentes. Adicione mais uma inst\u00e2ncia no docker play e vamos passar para o token do comando docker swarm join-token e passe o manager: Execute Saida docker swarm join-token manager Lembrando s\u00f3 o n\u00f3 gerente pode executar To add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-1waduqjabmpmygpy6xicwtt1srwbchelwnu8fsv3fj8qk4jdvi-8tke9gy4e6crw0euq2tki1m0y 192 .168.0.13:2377 V\u00e1 \u00e0 inst\u00e2ncia criada e execute o comando passado. Execute Saida docker swarm join --token SWMTKN-1-1waduqjabmpmygpy6xicwtt1srwbchelwnu8fsv3fj8qk4jdvi-8tke9gy4e6crw0euq2tki1m0y 192 .168.0.13:2377 Lembrando s\u00f3 o n\u00f3 gerente pode executar This node joined a swarm as a manager. Se por acaso estive com a configura\u00e7\u00e3o de 1 gerente e 4 trabalhadores, basta derruba um dos trabalhadores e remover do swarm, e em seguida suba novamente como gerente. Em um dos gerentes: Execute Saida docker node ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION fhiexwgtupwto0yzz5i4nbve7 node1 Ready Active Leader 20 .10.0 x3ghs9kh5nftbnnd7otoyg4hu node2 Ready Active 20 .10.0 nufnx3dlv0orjgb0dm5li4aah node3 Ready Active 20 .10.0 jvxn6dt0zob66po3d5rdrbaru node4 Ready Active 20 .10.0 1bk98ofc3qasq9n6v298x4jjv * node5 Ready Active Reachable 20 .10.0 E pronto o seu novo gerente j\u00e1 est\u00e1 rodando normalmente.","title":"Criando Gerentes"},{"location":"docker%20swarm/servi%C3%A7os/#raft-na-pratica","text":"Vamos reiniciar nossa aplica\u00e7\u00e3o no docker play feche com o close session e de outro start, lembrando dependendo o servi\u00e7o pode estar cheio, mas basta continuar tentando at\u00e9 conseguir. Click na chave de boca e selecione a op\u00e7\u00e3o: Ap\u00f3s carregar a configura\u00e7\u00e3o, execute docker node ls em qualquer um dos gerentes: Execute Saida docker node ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 3g4vyx694h4pd3dxlj442x88z manager1 Ready Active Reachable 20 .10.0 lheo3at6gjkka6uoehk3lr0ow * manager2 Ready Active Leader 20 .10.0 ptvwj6n2ums5o8b0lsfabhxgk manager3 Ready Active Reachable 20 .10.0 n2230i73ivqhyqqg0ty2hw3nd worker1 Ready Active 20 .10.0 tdiufxyidqv47atj937ffthb1 worker2 Ready Active 20 .10.0 Agora temos 3 gerentes, sendo um deles o lider. Vamos derrubar o lider com docker swarm leave --force no terminal do lider. Assim que executar v\u00e1 em outro gerente: Execute Saida docker node ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 3g4vyx694h4pd3dxlj442x88z * manager1 Ready Active Leader 20 .10.0 lheo3at6gjkka6uoehk3lr0ow manager2 Unknown Active Unreachable 20 .10.0 ptvwj6n2ums5o8b0lsfabhxgk manager3 Ready Active Reachable 20 .10.0 n2230i73ivqhyqqg0ty2hw3nd worker1 Ready Active 20 .10.0 tdiufxyidqv47atj937ffthb1 worker2 Ready Active 20 .10.0 Temos um novo l\u00edder agora, que pe maneger1, enquanto o manager3 est\u00e1 com o MANAGER STATUS de Unreachable , ou seja, est\u00e1 fora do swarm por algum motivo. O que ocorreu? O Raft, foi acionado para gera uma elei\u00e7\u00e3o, e o vencedor da elei\u00e7\u00e3o foi o maneger1. Recomenda\u00e7\u00e3o do docker sobre os gerentes, eles recomendam que tenhamos n\u00fameros impares, 3, 5 ou 7.","title":"Raft na pr\u00e1tica"},{"location":"docker%20swarm/servi%C3%A7os/#removendo-gerentes","text":"Vamos remover o nosso gerente que est\u00e1 Unreachable . Em qualquer gerente execute: Execute Saida docker node rm manager2 Lembrando s\u00f3 o n\u00f3 gerente pode executar Error response from daemon: rpc error: code = FailedPrecondition desc = node lheo3at6gjkka6uoehk3lr0ow is a cluster manager and is a member of the raft cluster. It must be demoted to worker before removal E ele deu um erro, ele est\u00e1 dizendo que precisamos rebaixar o menager2 para trabalhador antes de remover efetivamente do swarm, usando o docker node demote : Execute Saida docker node demote manager2 Lembrando s\u00f3 o n\u00f3 gerente pode executar Manager manager2 demoted in the swarm. Agora \u00e9 poss\u00edvel remover com o docker node rm , fa\u00e7a e vamos adicionar o gerente no swarm com o docker swarm join-token manager pegue o comando que ele retorna e cola no gerente que voc\u00ea removeu.","title":"Removendo Gerentes"},{"location":"docker%20swarm/servi%C3%A7os/#restricao-de-servicos","text":"Por padr\u00e3o o docker permite que o gerente rode servi\u00e7os, ou seja, \u00e9 poss\u00edvel que a maquina que vai efetivamente roda alguma imagem seja um gerente e n\u00e3o um trabalhador. Mas vamos supor que por algum motivo voc\u00ea n\u00e3o quer permitir que o gerente rode qualquer servi\u00e7o, ou s\u00f3 quer determinado servi\u00e7o rode no gerente. Ent\u00e3o podemos resolver da seguinte forma: 1 2 3 4 Suba a imagem docker service create -p 5000 :5000 sposigor/app-flask-teste:1 Lembrando s\u00f3 o n\u00f3 gerente pode executar docker node ls E a sa\u00edda: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 3g4vyx694h4pd3dxlj442x88z manager1 Ready Active Leader 20 .10.0 3xtgnz4d2098nnhjrq5ilv7l6 * manager2 Ready Active Reachable 20 .10.0 ptvwj6n2ums5o8b0lsfabhxgk manager3 Ready Active Reachable 20 .10.0 n2230i73ivqhyqqg0ty2hw3nd worker1 Ready Active 20 .10.0 tdiufxyidqv47atj937ffthb1 worker2 Ready Active 20 .10.0 Aqui vamos alterar o estado da AVAILABILITY , para conseguimos fazer essa restri\u00e7\u00e3o do n\u00f3. Enquanto estiver como Active significa que est\u00e1 dispon\u00edvel para roda servi\u00e7os. Para alterar vamos usar o docker node update Vamos passa a flag --availability que tem 3 estados (\"active\"|\"pause\"|\"drain\"). No nosso caso queremos usar o drain para que o gerente n\u00e3o execute nenhum servi\u00e7o. Por fim basta passa qual maquina voc\u00ea quer alterar o estado. docker node update --availability drain manager2 Rode novamente o docker node ls docker node ls Saida: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 3g4vyx694h4pd3dxlj442x88z manager1 Ready Active Leader 20 .10.0 3xtgnz4d2098nnhjrq5ilv7l6 * manager2 Ready Drain Reachable 20 .10.0 ptvwj6n2ums5o8b0lsfabhxgk manager3 Ready Active Reachable 20 .10.0 n2230i73ivqhyqqg0ty2hw3nd worker1 Ready Active 20 .10.0 tdiufxyidqv47atj937ffthb1 worker2 Ready Active 20 .10.0 Assim conseguimos alterar a AVAILABILITY do manager2 para a Drain e com isso ele se torna inacessivel para executar servi\u00e7os, mas continuar a executar a atividade de gerente normalmente. Mas vamos supor que voc\u00ea queira fazer isso em todos os gerentes de uma vez, vamos ter que alterar 1 por 1? N\u00e3o, vamos usar um comando mais generalista que vai definir aonde queremos que o servi\u00e7o rode. Usando o docker service update e passando a flag --constraint-add que vai restrigir determido servi\u00e7o para um grupo especifico. Execute Saida Vamos passa para o comando a flag --constraint-add Nesse caso queremos especificar um tipo de n\u00f3, ent\u00e3o vamos usar o node.role==worker E por ultimo vamos passa o ID do servi\u00e7o para saber qual servi\u00e7o vamos aplicar essa restri\u00e7\u00e3o docker service update --constraint-add node.role == worker gnj34gs43ebi gnj34gs43ebi overall progress: 1 out of 1 tasks 1 /1: running [================================================== > ] verify: Service converged Alguns cuidados durante a restri\u00e7\u00e3o desses servi\u00e7os, vamos supor que voc\u00ea fa\u00e7a essa restri\u00e7\u00e3o usando um n\u00f3 que n\u00e3o existe, ele vai rodar o comando e todas as suas aplica\u00e7\u00f5es se tornaram indisponiveis, porque a restri\u00e7\u00e3o que houve, restringiu para um n\u00f3 inexistente. Quando restringir o servi\u00e7os nos gerente? Queremos rodar servi\u00e7os no manager, que s\u00e3o servi\u00e7os de monitoramento, servi\u00e7os de seguran\u00e7a, ou seja, servi\u00e7os que s\u00e3o cr\u00edticos, que dependemos na nossa aplica\u00e7\u00e3o. \u00c9 importante pondera essa quest\u00e3o, para entender quais s\u00e3o os pontos criticos da aplica\u00e7\u00e3o.","title":"Restri\u00e7\u00e3o de Servi\u00e7os"},{"location":"docker%20swarm/servi%C3%A7os/#replicas","text":"J\u00e1 vimos essa palavra aqui: Execute Saida docker service ls Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME MODE REPLICAS IMAGE PORTS gnj34gs43ebi dazzling_lamarr replicated 1 /1 sposigor/app-flask-teste:1 *:5000->5000/tcp Certo o que significa isso, bem para explicar a gente precisa descobrir aonde nosso servi\u00e7o est\u00e1 rodando: Execute Saida docker service ps gnj34gs43ebi Lembrando s\u00f3 o n\u00f3 gerente pode executar ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS w105x64ylhne dazzling_lamarr.1 sposigor/app-flask-teste:1 worker2 Running Running 37 minutes ago O servi\u00e7o est\u00e1 rodando no worker2 , para explicar vamos repassar pelo conceito de Load Balancing. Quando a porta 5000 \u00e9 acessada independete da maquina que estiver no swarm, a aplica\u00e7\u00e3o \u00e9 diponibilizada, por\u00e9m se houver intera\u00e7\u00e3o, vamos supor que haja um cadastro de um usuario, o que ocorre na pratica \u00e9 que essa altera\u00e7\u00e3o vai ser redirecionada para a maquina que est\u00e1 rodando essa imagem. E a Replica serve para isso, para que possamos usar mais de uma maquina nesse processo de processamento dessa demanda. Ent\u00e3o vamos replicar essa imagem, que para outras maquinas com o docker service update . Execute Saida docker service update --replicas 8 gnj34gs43ebi Lembrando s\u00f3 o n\u00f3 gerente pode executar gnj34gs43ebi overall progress: 4 out of 4 tasks 1 /4: running [================================================== > ] 2 /4: running [================================================== > ] 3 /4: running [================================================== > ] 4 /4: running [================================================== > ] verify: Service converged Vamos executar o docker service ls para verificar como est\u00e1 as r\u00e9plicas: docker service ls docker service ps Removendo --constraint-rm 8 Replicas docker service ls Lembrando s\u00f3 o n\u00f3 gerente pode executar Saida: ID NAME MODE REPLICAS IMAGE PORTS gnj34gs43ebi dazzling_lamarr replicated 4 /4 sposigor/app-flask-teste:1 *:5000->5000/tcp Temos agora, 4 replicas Para saber quais maquinas est\u00e3o rodando o servi\u00e7o: docker service ps gnj34gs43ebi Saida: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS w105x64ylhne dazzling_lamarr.1 sposigor/app-flask-teste:1 worker2 Running Running 52 minutes ago v9bmopdi3nr3 dazzling_lamarr.2 sposigor/app-flask-teste:1 worker1 Running Running 3 minutes ago sp81t2z56n67 dazzling_lamarr.3 sposigor/app-flask-teste:1 worker1 Running Running 4 minutes ago oxdvxulccegb dazzling_lamarr.4 sposigor/app-flask-teste:1 worker2 Running Running 5 minutes ago Lembrando que fizemo a restri\u00e7\u00e3o para somente workes executarem as tarefas. Vamos remover a restri\u00e7\u00e3o e tentar novamente com 8 replicas Vamos passar os mesmos par\u00e2metros que usamos para restringir os servi\u00e7os docker service update --constraint-rm node.role == worker gnj34gs43ebi J\u00e1 vamos passar 8 no par\u00e2metro. docker service update --replicas 8 gnj34gs43ebi Em seguida: docker service ps gnj34gs43ebi E a sa\u00edda: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS w105x64ylhne dazzling_lamarr.1 sposigor/app-flask-teste:1 worker2 Running Running about an hour ago v9bmopdi3nr3 dazzling_lamarr.2 sposigor/app-flask-teste:1 worker1 Running Running 17 minutes ago sp81t2z56n67 dazzling_lamarr.3 sposigor/app-flask-teste:1 worker1 Running Running 17 minutes ago oxdvxulccegb dazzling_lamarr.4 sposigor/app-flask-teste:1 worker2 Running Running 18 minutes ago gs8m3brh4sw6 dazzling_lamarr.5 sposigor/app-flask-teste:1 manager3 Running Running 3 minutes ago sr1u0kr19hut dazzling_lamarr.6 sposigor/app-flask-teste:1 manager3 Running Running 3 minutes ago z80bqt30sscq dazzling_lamarr.7 sposigor/app-flask-teste:1 manager3 Running Running 3 minutes ago 2yjmab0lkl7q dazzling_lamarr.8 sposigor/app-flask-teste:1 manager1 Running Running 3 minutes ago Com isso agora temos diversas maquinas rodando nosso servi\u00e7o, ou seja, o poder de processamento aumentou. Porque assim que ocorrer uma demanda de processamento, o load balancing por tr\u00e1s vai estar resolvendo isso. Tamb\u00e9m \u00e9 poss\u00edvel usar o docker service scale , vamos passar o ID do servi\u00e7o e a quantidade de r\u00e9plicas para escalar docker service scale gnj34gs43ebi = 6 Temos tamb\u00e9m a op\u00e7\u00e3o de passar diretamente na cria\u00e7\u00e3o do servi\u00e7o, basta passa a flag --mode e o par\u00e2metro global docker service create -p 5000 :5000 --mode global sposigor/app-flask-teste:1 \u00c9 importante lembrar que n\u00e3o \u00e9 recomendado passar todos os servi\u00e7os com o par\u00e2metro global, afinal isso vai exigir um consumo que poderia ser poupado da aplica\u00e7\u00e3o no processador individual. Seja sempre critico com essa necessidade.","title":"Replicas"}]}